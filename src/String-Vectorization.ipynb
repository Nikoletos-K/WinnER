{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4fe61c8-bf0a-4448-b478-96cf89f0ba1a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div align=\"center\">\n",
    "<font size=\"8\">\n",
    "  <b>String Vectorization</b> \n",
    "</font><br>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98974a70-5b7a-4e33-bbb1-0b92c5b5b919",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prototype selection\n",
    "\n",
    "Available methods for choosing prototypes:\n",
    "- K-Means\n",
    "- K-Medoids\n",
    "- CLARA\n",
    "- CLARANS\n",
    "\n",
    "Available distances between strings:\n",
    "- Edit distance (without processing)\n",
    "- Jaccard (tokenization)\n",
    "- Euclid-Jaccard (tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29f9ce53-0b8e-4b99-80d2-d0e848f0bde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization initialized with.. \n",
      "- Metric:  jaccard\n",
      "- Q-gramms:  2\n",
      "- Char-Tokenization:  False\n",
      "- Text cleanning process:  None\n",
      "\n",
      "Processing strarts.. \n",
      "- Data size:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing..: 100%|██████████| 4/4 [00:00<00:00, 2777.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([{('abcd',)}, {('bcda',)}, {('cdab',)}, {('dabc',)}], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    metric: function with the above format\n",
    "    \n",
    "        def metric(str1,str2):\n",
    "                ...\n",
    "            return distance\n",
    "'''\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from logging import info as info\n",
    "from logging import warning as warning\n",
    "from logging import exception as exception\n",
    "from logging import error as error\n",
    "\n",
    "\n",
    "logging.basicConfig(filename='tokenization.log', level=logging.INFO)\n",
    "info = print\n",
    "\n",
    "class Tokenizer:\n",
    "     \n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        metric = None, \n",
    "        qgrams = None, \n",
    "        is_char_tokenization = None, \n",
    "        clean = None\n",
    "    ):\n",
    "        \n",
    "        self.metric = metric\n",
    "        self.qgrams = qgrams\n",
    "        self.is_char_tokenization = is_char_tokenization\n",
    "        self.clean = clean\n",
    "        \n",
    "        info(\"Tokenization initialized with.. \")\n",
    "        info(\"- Metric: \", self.metric)\n",
    "        info(\"- Q-gramms: \", self.qgrams)\n",
    "        info(\"- Char-Tokenization: \", self.is_char_tokenization)\n",
    "        info(\"- Text cleanning process: \", self.clean)\n",
    "        \n",
    "    def process(self, data):\n",
    "        \n",
    "        # if isinstance(data, list):\n",
    "        # elif isinstance(data, pd.DataFrame):\n",
    "        # elif isinstance(data, np.array):\n",
    "            \n",
    "        self.data_size = len(data)\n",
    "        self.data = np.array(data, dtype = object)\n",
    "        self.tokenized_data = np.empty([self.data_size], dtype = object)\n",
    "        \n",
    "        info(\"\\nProcessing strarts.. \")\n",
    "        info(\"- Data size: \", self.data_size)\n",
    "        \n",
    "        # self.data_mapping = np.array(input_strings, dtype=object)\n",
    "        \n",
    "        for i in tqdm(range(0, self.data_size), desc=\"Processing..\"):\n",
    "            if self.clean is not None:\n",
    "                string = self.clean(self.data[i])\n",
    "            else:\n",
    "                string = self.data[i]\n",
    "            # info(string)\n",
    "            if self.is_char_tokenization:\n",
    "                self.tokenized_data[i] = set(nltk.ngrams(string, n = self.qgrams))\n",
    "            else:\n",
    "                if len(nltk.word_tokenize(string)) > self.qgrams:\n",
    "                    self.tokenized_data[i] = set(nltk.ngrams(nltk.word_tokenize(string), n = self.qgrams))\n",
    "                else:\n",
    "                    self.tokenized_data[i] = set(nltk.ngrams(nltk.word_tokenize(string), n = len(nltk.word_tokenize(string))))\n",
    "            # info(self.tokenized_data[i])\n",
    "\n",
    "        return self.tokenized_data\n",
    "        \n",
    "dataset = [\"abcd\", \"bcda\", \"cdab\", \"dabc\"]\n",
    "tok = Tokenizer('jaccard', 2, False)\n",
    "tok.process(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81bf4fc5-692e-4c2b-875d-e4843e1be667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcd', 'bcda', 'cdab', 'dabc']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9690a1f2-6e95-43b6-a712-6e42ce4cb2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379d5d5-c07f-4e63-b4a7-deacca81b086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
