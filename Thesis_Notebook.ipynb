{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    " <img src=\"http://www.di.uoa.gr/themes/corporate_lite/logo_en.png\" title=\"Department of Informatics and Telecommunications - University of Athens\"/> </p>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<h3 align=\"center\" > \n",
    "  Bachelor Thesis\n",
    "</h3>\n",
    "\n",
    "<h1 align=\"center\" > \n",
    "  Entity Resolution in Dissimilarity Spaces\n",
    "</h1>\n",
    "\n",
    "---\n",
    "\n",
    "<h3 align=\"center\"> \n",
    " <b>Konstantinos Nikoletos</b>\n",
    "</h3>\n",
    "\n",
    "<h4 align=\"center\"> \n",
    " <b>Supervisor: Dr. Alex Delis</b>,  Professor NKUA\n",
    "</h4>\n",
    "<br>\n",
    "<h4 align=\"center\"> \n",
    "Athens\n",
    "</h4>\n",
    "<h4 align=\"center\"> \n",
    "January-September 2021\n",
    "</h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "| Table of Contents |\n",
    "| :--   |\n",
    "|**1. [Abstract](#Abstract)** |\n",
    "|**2. [Introduction](#Introduction)** <br />    **2.1. [   Entity resolution](#Entity-resolution)** <br />   **2.2. [   Dissimilatiry space](#Dissimilatiry-Space)**|\n",
    "|**3. [ A dissimilarity-based space embedding methodology](#A-dissimilarity-based-space-embedding-methodology)**|\n",
    "\n",
    "-->\n",
    "\n",
    "# Contents\n",
    "---\n",
    "**1. [Abstract](#Abstract)** \\\n",
    "**2. [Introduction](#Introduction)**  \\\n",
    "&nbsp;&nbsp;&nbsp;**2.1. [   Entity resolution](#Entity-resolution)** \\\n",
    "&nbsp;&nbsp;&nbsp;**2.2. [   Dissimilatiry space](#Dissimilatiry-Space)** \\\n",
    "**3. [ A dissimilarity-based space embedding methodology](#A-dissimilarity-based-space-embedding-methodology)** \\\n",
    "&nbsp;&nbsp;&nbsp;**3.1 [String Clustering and Prototype Selection](#3.1-String-Clustering-and-Prototype-Selection)** \\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3.1.1. [Edit distance metric](#Edit-distance-metric)** \\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3.1.2. [String clustering algorithm](#String-clustering-algorithm)** \\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3.1.3. [Algorithm complexity](#Algorithm-complexity)** \\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3.1.4. [Prototype selection](#Prototype-selection)** \\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3.1.5. [Algorithm-1: The String Clustering and Prototype Selection Algorithm](#Algorithm-1:-The-String-Clustering-and-Prototype-Selection-Algorithm)** \\\n",
    "&nbsp;&nbsp;&nbsp;**3.2 [The Vantage Space Embedding and the Chorus of Prototypes Transform Similarity Coefficient](#3.2-The-Vantage-Space-Embedding-and-the-Chorus-of-Prototypes-Transform-Similarity-Coefficient)** \\\n",
    "&nbsp;&nbsp;&nbsp;**3.3 [A Top-k List Approach for Similarity Searching in the Vantage Space](#3.3-A-Top-k-List-Approach-for-Similarity-Searching-in-the-Vantage-Space)** \\\n",
    "&nbsp;&nbsp;&nbsp;**3.4 [Hashing of Partially Ranked Data for Efficient Similarity Search](#3.4-Hashing-of-Partially-Ranked-Data-for-Efficient-Similarity-Search)** \\\n",
    "**4. [ Evaluation](#Evaluation)** \\\n",
    "**5. [References](#References)** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook it will be presented a dissimilarity-based entity resolution\n",
    "framework that introduces a new efficient object representation\n",
    "scheme. This framework consists of four parts. First part is the string clustering and prototype selection, in which clusters will be made that afterwords will be used for the string embedding. The second part in this methology is the string embedding into an N-dimensional Vantage space which has been generated by the prototype selection. Next, in the third part, it will be presented a distance measure that relies on Kendall tau\n",
    "correlation coefficient and generalizes the similarity measures and\n",
    "distances presented so far. Finally, in the fourth part, a sparse embedding scheme on this metric is added in order to minimize the computational cost of this methodology. \n",
    "\n",
    "This system will be evaluated in three databases. Its performance will be compared with some other famous Entity reslution systems in metrics Recall and Precision and also in computational time. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    " ### Entity resolution\n",
    " \n",
    "__Entity resolution (ER)__ or Deduplication are among the research themes that have recently received escalated interest. ER is the process of creating systematic linkage between disparate data records that represent the same thing in reality, in the absence of a join key. For example, as a previous project that I made, say you have a dataset of camera records from multiple websites (Amazon, AliBaba, etc) and you want to find which of these records refer to the same real object. Records may have slightly different names, somewhat different descriptions, maybe similar prices, and totally different unique identifiers. This may heard no big deal, but taking into serious the volume of some datasets and databases, gets you to understand how challenging, in prospects of  accuracy and computability this is. ER applications are now used for multiple reasons, not only for avoiding duplicates in databases, but also for reasons like finding \"similar\" accounts in social media or email, that are connected to  criminal actions.     \n",
    "\n",
    "The goal of this project, is to make an Entity resolution system that performs both well in Precision, Recall and execution time.  In this work we embrace an embedding approach by selecting a number of pivot objects to act as prototypes for transforming a dissimilarity space of proximities into a reduced set of distances of objects from these prototypes. It is now important to make clear what a dissimilarity space is. This definition comes from the fields of Statistis and theoritical Machine Learning. \n",
    "\n",
    "### Dissimilatiry Space\n",
    "\n",
    "Dissimilarities [[1]]() have been used in pattern recognition for a long time. In the first approach the dissimilarity matrix is considered as a set of row vectors, one for every object.  They represent the objects in a vector space constructed by the dissimilarities to the other objects.  Usually, this vector space is treated as a Euclidean space and equipped with the standard inner product definition. Let $ \\textit{X} = \\{ x_1, . . . , x_n \\} $ be a training set. Given a dissimilarity function and/or dissimilarity data, we define a data-dependent mapping $ D(·, R) : X → R $ from  $ X $ to the so-called __dissimilarity space (DS)__ . The $k-element$ set $R$ consists of\n",
    "objects that are representative for the problem. This set is called the representation or __prototype set__ and it may be a subset of X . In the dissimilarity space each dimension $ D(·, p_i) $ describes a dissimilarity to a prototype $ p_i $ from R. In this paper, we initially choose $ R := X $ . As a result, every object is described by an n-dimensional dissimilarity vector $ D(x, X ) = [d(x, x_1) . . . d(x, x_n)]^T $. The resulting vector space is endowed with the traditional inner product and the Euclidean metric.\n",
    "Any dissimilarity measure ρ can be defined in the DS. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "___This project builds on four pillars:___\n",
    "\n",
    "1. Object partitioning and embedding. More specifically the embedding technique that is mainly used is called __Vantage Embedding__  and the __Chorus of Prototypes scheme__ .\n",
    "2. Machine Learning techniques that build __nearest neighbors classification__ models on the selection of prototypes\n",
    "3. Various correlation coefficient and distance metrics that are applied on ranked data as well as a generalization of the well known __Hausdorff distance metric for partially ranked data__.\n",
    "4. __Locality Sensitive Hashing (LSH)__ techniques specifically tuned for handling ranking data to render the similarity search process very efficient.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A dissimilarity-based space embedding methodology\n",
    "---\n",
    "\n",
    "Central theme in this methodology is the transformation of the input data in a representation form that can easily and accurately circumvent the inherent lack of features of objects and handle a variety of different data types in a unified way. \n",
    "\n",
    "\n",
    "The first step is to read and process the input data (strings in this particular work). Section 3.1 consists of the idea and the algorithm of string clustering in order to create the embeddings. But, firstly, it is highly important to define what's the Vantage Space and the Chorus of Prototypes scheme. These approaches are used in order to to efficiently and effectively capture the similarity of high dimensional data. \n",
    "\n",
    "\n",
    "After a brief definition of the above embedding techniques, we need to create a set of string prototypes and according to these prototypes, create the embeddings into an N-dimensional space. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 String Clustering and Prototype Selection\n",
    "\n",
    "The first step in this methodology is to cluster the input strings in order to identify the cluster representatives that will be used as prototypes in the embedding process. It is vey important to mention that every cluster needs two representative strings that will selected from the clustering algorithm.\n",
    "\n",
    "__Why do we need 2 representatives?__\n",
    "\n",
    "This way we can compute the inner product space where one string serves the origin and the other the endpoint of a vector.\n",
    "\n",
    "After the formation of the clusters, the prototype selection phase follows, in which one of the members in each cluster (not necessarily one of the two cluster representatives), will become its unique prototype and will be used\n",
    "as the pivot object for the embedding.\n",
    "\n",
    "Note that based on the assumed dissimilarity representation of the input objects, the considered distance metric for the input strings in this work is the __Edit Distance metric__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit distance metric\n",
    "Edit distance is a way of quantifying how dissimilar two strings (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other. Βased on the assumed dissimilarity representation of the input objects, the considered distance metric for the input strings in this work is the Edit Distance metric.\n",
    "\n",
    "Theres already an implementation for this metric in library editdistance\n",
    "\n",
    "Downloading the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: editdistance in c:\\users\\nikol\\anaconda3\\lib\\site-packages (0.5.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit distance: 2\n"
     ]
    }
   ],
   "source": [
    "import editdistance\n",
    "\n",
    "# def EditDistance(str1,str2):\n",
    "#     str1 = str1 if str1 not None else ''\n",
    "#     str2 = str2 if str2 not None else ''\n",
    "    \n",
    "#     return editdistance.eval(str1,str2)\n",
    "\n",
    "def EditDistance(str1,str2,verbose=None):\n",
    "    if verbose:\n",
    "        if str1 == None:\n",
    "            print(\"1\")\n",
    "        elif str2 == None:\n",
    "            print(\"2\")\n",
    "        print(str1+\" | \"+str2+\" : \"+str(editdistance.eval(str1,str2)))\n",
    "    \n",
    "    return editdistance.eval(str1,str2)\n",
    "\n",
    "\n",
    "print(\"Edit distance: \"+str(editdistance.eval('banana', 'bahama')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can either implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit distance: 2\n"
     ]
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/edit-distance-dp-5/\n",
    "# A Naive recursive Python program to fin minimum number\n",
    "# operations to convert str1 to str2\n",
    " \n",
    "def EditDistance(str1, str2, m, n):\n",
    " \n",
    "    # If first string is empty, the only option is to\n",
    "    # insert all characters of second string into first\n",
    "    if m == 0:\n",
    "        return n\n",
    "\n",
    "    # If second string is empty, the only option is to\n",
    "    # remove all characters of first string\n",
    "    if n == 0:\n",
    "        return m\n",
    "\n",
    "    # If last characters of two strings are same, nothing\n",
    "    # much to do. Ignore last characters and get count for\n",
    "    # remaining strings.\n",
    "    if str1[m-1] == str2[n-1]:\n",
    "        return EditDistance(str1, str2, m-1, n-1)\n",
    "\n",
    "    # If last characters are not same, consider all three\n",
    "    # operations on last character of first string, recursively\n",
    "    # compute minimum cost for all three operations and take\n",
    "    # minimum of three values.\n",
    "    return 1 + min(EditDistance(str1, str2, m, n-1),    # Insert\n",
    "               EditDistance(str1, str2, m-1, n),    # Remove\n",
    "               EditDistance(str1, str2, m-1, n-1))    # Replace\n",
    "                \n",
    "str1 = \"banana\"\n",
    "str2 = \"bahama\"\n",
    "print(\"Edit distance: \"+str(EditDistance(str1, str2, len(str1), len(str2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String clustering algorithm\n",
    "\n",
    "The string clustering algorithm produces as its output two vectors that contain for each discovered cluster, two representatives, as well as the assignment of individual strings to the closest cluster.\n",
    "\n",
    "***Functionality:***\n",
    "The string clustering algorithm iterates through the list of the input strings, and for each input string loops over the list of the currently discovered clusters. The algorithm starts processing the of the first discovered cluster1. The second string will be checked against the first representative of the first cluster, and if it is the case that the distance between these two strings is less than the distance threshold 𝑑 given as input to the algorithm, the second string will join the first cluster and it will become the second representative of this cluster. If the distance between these two strings is greater\n",
    "than 𝑑, then the algorithm will go on with the next cluster. Given there is only one cluster which has been created so far, and by assuming that the distance between the second string and the first representative is greater than the threshold, the second string will be automatically allocated to the second cluster (which is empty at this moment), in which case it will become its first representative.\n",
    "\n",
    "\n",
    "### Cluster  membership condition\n",
    "\n",
    "When a new string is checked for membership to a cluster, the condition that must be satisfied by this newly coming string in order to join this cluster is that the sum of its distances from the two representatives must be less than the distance threshold.\n",
    "\n",
    "$$\n",
    "\\textbf{String_Sum_Of_Distances < Distance_Threshold}\n",
    "$$\n",
    "\n",
    "By comparing the newly arrived string against the two cluster representatives and by ensuring that the cluster\n",
    "membership condition is satisfied, we have the right to provide distance guarantees for all the pairs of strings that will eventually join the same cluster, meaning that no string in the cluster is more than 𝑑 distance away from any other string in the same cluster.\n",
    "\n",
    "![](img/fig_1.png)\n",
    "\n",
    "$$\n",
    "\\textbf{Figure 1: Properties of distances of strings from cluster representatives}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Triangle inequality\n",
    "\n",
    "Figure 1 is a visualize of the membership condition that must be fullfilled in each cluster. \n",
    "In this graph:\n",
    "\n",
    "- Nodes __A,B,C,D,E,F__ represent the strings\n",
    "- Edges represent the Edit distances among the strings\n",
    "\n",
    "__Figure Scenario:__\n",
    "\n",
    "1. String __A__ joins first the cluster\n",
    "2. String __B__ joins first the cluster $\\Rightarrow$ __A__,__B__ become the 2 representatives\n",
    "3. __A__,__B__ distance must be less than d $\\Rightarrow$ $\\textbf{distance(A,B) < d}$\n",
    "4. String __C__ is checker for entering the cluster. Condition checked: $\\sum_{n=A,B}\\textbf{distance(C,n) < d}$. If condition is valid string __C__ enters the cluster.\n",
    "5. Assume that strings __D__ and __E__ are checked for membership to the cluster in a similar fashion,which is done by questioning for each one of them whether their sum of the distances from the two representatives is less than or equal to the distance threshold. Assuming also that these conditions fulfilled and they join the clusters. It can be proofed from the triangle inequality that the distances of the newly joined __C__,__D__,__E__ from each other are below d.\n",
    "\n",
    "(PROOF)\n",
    "\n",
    "\n",
    "### Algorithm complexity\n",
    "For $n$ strings and $k$ prototypes: $\\textit{O(nk)}$\n",
    "\n",
    "\n",
    "### Prototype selection\n",
    "From among the strings that were allocated to a certain cluster by the string clustering algorithm, we need to specify one string that will play the role of the cluster prototype. This method benefits the algorithm complexity, as it is only needed to compare the incoming new string with the 2 representatives and not with all the cluster. Even though either one of the two representatives of each cluster which were derived during the clustering process can assume the cluster prototype role, there are still other choices that are way more precise and can be computed in a computationally efficient manner. To accomplish this, we rely on the notion of the set median string which has appeared as a concept in a number of studies before. \n",
    "\n",
    "\n",
    "\n",
    "#### Median string\n",
    "The string 𝑚 that belongs to a set of strings 𝑆 and satisfies the following property\n",
    "$$\n",
    "m = \\textit{argmax}_{y\\in S} \\sum_{\\forall x \\in S} d(x,y)\n",
    "$$\n",
    "\n",
    "Briefly, this condition means that it is the string in S whose sum of the distances from all the othe strings in S is minimum.\n",
    "It is obvious that this procedure has an expencive computation as it is needed to traverse the set of strings multiple times. For this reason in this framework it will be used another way of finding the cluster prototypes. \n",
    "\n",
    "\n",
    "![](img/fig_2.png)\n",
    "\n",
    "$$\n",
    "\\textbf{Figure 2: Projections of distances of strings from cluster representative A}\n",
    "$$\n",
    "\n",
    "Figure 2 illustrates the idea of the string prototype selection for a random cluster of strings. It will be demontrated an efficient alforithm for selecting prototypes in the following steps:\n",
    "\n",
    "1. Calculating the distances of the projections of the strings from the leftmost representative string __A__ onto the line that extends from points __A__ and __B__. Example is the computation of distance $d_{CA}$ \n",
    "    - Assuming that __A__ and __B__ are already projected,which means that we do not have to do anything about these two strings.\n",
    "    - Distance $d_{CA}$ :\n",
    "        \\begin{align*}\n",
    "            CA^2 &= CC'^2 + d_{CA}^2 \\Leftrightarrow   &&  \\textit{(Pythagorian Theorem - PT in C'CA trianlge)} \\\\\n",
    "            CA^2 &= CB^2 - (d_{CA}+AB)^2 + d_{CA}^2 \\Leftrightarrow   &&  (\\textit{C'CB trianlge: } CC'^2 + C'B^2 = CB^2)   \\\\\n",
    "            CA^2 &= CB^2 - d_{CA}^2 - AB^2 - 2 \\cdot AB^2 \\cdot  d_{CA} + d_{CA}^2 \\Leftrightarrow && (\\textit{Algebraic identity})\\\\\n",
    "            CA^2 &= CB^2 - AB^2 - 2 \\cdot AB^2 \\cdot  d_{CA} \\Leftrightarrow && \\\\\n",
    "            2 \\cdot AB^2 \\cdot  d_{CA} &= CB^2 - AB^2 - CA^2 \\Leftrightarrow &&  \\\\  \n",
    "            d_{CA} &= \\frac{CB^2 - AB^2 - CA^2}{2 \\cdot AB^2} &&  \\\\  \n",
    "        \\end{align*}\n",
    "        The distances __AB__,__CA__,__CB__ are already known from the clustering phase, so there's no need to be computed again. This way we can now easily use the formula to compute all the wanted distances\n",
    "2. __Sorting phase__:   After we have estimated all the projected distances, we sort these distances algebraically from the smallest to the largest. In this implementation it will be used quicksort algorithm from numpy library.\n",
    "3. We choose the __median distance__ among those and set it as the prototype of this cluster. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Algorithm 1:*** The String Clustering and Prototype Selection Algorithm\n",
    "\n",
    "\n",
    "***The algorithm takes as input:***\n",
    " - __𝑆__: the input strings in vector, \n",
    " - __𝑘__: the maximum number of clusters to be generated, \n",
    " - __d__: the maximum allowable distance of a string to join a cluster\n",
    "\n",
    "it produces in the first phase two arrays,\n",
    " - an array variable __𝐶__ that contains the assignment of individual strings to cluster identities, and \n",
    " - the 2D array variable __r__ that maintains the assignment of representatives to clusters. \n",
    "\n",
    "In the second phase, it produces the assignment of prototypes to clusters in the array variable __Prototype__.\n",
    "\n",
    "***Returns:***\n",
    "- __Prototype__: Assignment of prototypes to clusters as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CLUSTERING_PROTOTYPES(S,k,d,r,C) \n",
    "The String Clustering and Prototype Selection Algorithm\n",
    "is the main clustering method, that takes as input the intial strings S, \n",
    "the max number of clusters to be generated in k,\n",
    "the maximum allowable distance of a string to join a cluster in var d\n",
    "and returns the prototype for each cluster in array Prototype\n",
    "'''\n",
    "def CLUSTERING_PROTOTYPES(S,k,d,r,C):\n",
    "    \n",
    "    # ----------------- Initialization phase ----------------- #\n",
    "    i = 0\n",
    "    j = 0\n",
    "        \n",
    "    while i < S.size:     # String-clustering phase, for all strings\n",
    "#         print(S.size)\n",
    "        while j < k :       # iteration through clusters, for all clusters\n",
    "            if r[0][j] == None:      # case empty first representative for cluster j\n",
    "                r[0][j] = S[i]   # init cluster representative with string i\n",
    "                C[i] = j         # store in C that i-string belongs to cluster j\n",
    "                break\n",
    "            elif r[1][j] == None and (EditDistance(S[i],r[0][j]) <= d):  # case empty second representative \n",
    "                r[1][j] = S[i]                                             # and ED of representative 1  smaller than i-th string \n",
    "                C[i] = j\n",
    "                break\n",
    "            elif (EditDistance(S[i],r[0][j]) + EditDistance(S[i],r[1][j])) <= d:\n",
    "                C[i] = j\n",
    "                break\n",
    "            else:\n",
    "                j += 1\n",
    "        i += 1\n",
    "    \n",
    "    # ----------------- Prototype selection phase ----------------- #\n",
    "        \n",
    "#     Projections = np.array([1,2])\n",
    "    Prototype = np.empty([k],dtype=object)\n",
    "    \n",
    "    while j < k:\n",
    "        Prototype[j] = r[0][j]  # First representative becomes the Prototype\n",
    "#         Projections[j] = Approximated_Projection_Distances_ofCluster()\n",
    "#         sortedProjections[j] = np.sort(np.array(Projections[j]),kind = 'quicksort' ) \n",
    "#         Prototype[j] = median(sortedProjections[j])\n",
    "        j += 1\n",
    "    \n",
    "    return Prototype\n",
    "\n",
    "def Approximated_Projection_Distances_ofCluster():\n",
    "    \n",
    "    \n",
    "    return distances\n",
    "\n",
    "def median(distances):\n",
    "    \n",
    "    return median_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_strings = [\"abcd\",\"efgh\",\"h\",\"i\",\"1\",\"2\",\"4\",\"jshsshsjaleebsj\"]\n",
    "max_number_of_clusters = 3\n",
    "k = max_number_of_clusters\n",
    "d = 5\n",
    "S = np.array(input_strings,dtype=object)\n",
    "C = np.empty([S.size], dtype=int)\n",
    "r = np.empty([2,k],dtype=object)\n",
    "\n",
    "# print(r)\n",
    "# print(str(None))\n",
    "Prototypes = CLUSTERING_PROTOTYPES(S,k,d,r,C)\n",
    "# print(C)\n",
    "# print(r)\n",
    "# print(S)\n",
    "# print(Prototypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 The Vantage Space Embedding and the Chorus of Prototypes Transform Similarity Coefficient\n",
    "\n",
    "The second step in the methodology is to embed the input strings into the 𝑁-dimensional vantage space which has been generated by the prototype selection algorithm and to demonstrate the significance of using a correlation metric proposed in the context of the Chorus Transform for estimating the similarity of the embedded pairs of strings in the Vantage space.\n",
    "\n",
    "\n",
    "### Vantage Space\n",
    "\n",
    "The *Vantage Objects* approach maps pairwise distances of input objects into an 𝑁-dimensional space of pivot objects which is known as __Vantage Space__ in such a way that points that lie close to each other in this space correspond to similar objects in the original dissimilarity space. The __Chorus of Prototypes Transform (CT)__ on the other hand proposes the use of a rank correlation coefficient defined on the data induced by the distances of the input objects from the pivot objects.\n",
    "\n",
    "\n",
    "__Note:__\n",
    "\n",
    "- __k__ denotes the max number of expected clusters but\n",
    "- __N__ is the actual number of produced clusters\n",
    "\n",
    "#### Definition of a Finite metric space (S,d)\n",
    "\n",
    "A metric space is an ordered pair __(S,d)__ where:\n",
    "- __S__ is a set (our object set - strings) and \n",
    "- __d__ is a metric on S, i.e., a function $ d:S x S \\rightarrow \\Re   $\n",
    "\n",
    "such that for any random $o_1,o_2,o_3 \\in S$, the following holds:\n",
    "\n",
    "\\begin{align*}\n",
    "    d(o_1,o_2) & \\ge 0,   d(o_1,o_2) = 0 \\textit{ iff } o_1=o_2   &&  \\textit{(non-negativity)} \\\\   \n",
    "    d(o_1,o_2) &= d(o_2,o_1)   &&  \\textit{(symmetry)} \\\\\n",
    "    d(o_1,o_3) & \\le d(o_1,o_2) + d(o_2,o_3)   &&  \\textit{(triangle inequality)}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "#### Vantage embedding\n",
    "\n",
    "In this work it will be used a method called vantage embedding, which utilizes a set of 𝑁 objects from S such that\n",
    "\\begin{align*}\n",
    "    A^* &= \\{ A^*_1,A^*_2,...,A^*_n \\}    &&   \\textbf{Vantage Objects}\n",
    "\\end{align*}\n",
    "\n",
    "Vantage objects generate $N$ __different one dimensional mappings__ which are used collectively to generate the embedding for each string.\n",
    "\n",
    "\n",
    "The distance $x_j$ from each object $A_i \\in S$ to every ___Vantage Object___ in $A^*$ \n",
    "$$\n",
    "x_j = d(A_i,A_j^*)\n",
    "$$\n",
    "\n",
    "is computed creating in this way an __N-dimensional__ point in the Vantage Space for the object $A_i$:\n",
    "$$\n",
    "p_i = \\{ x_1,x_2,...,x_N \\}\n",
    "$$\n",
    "\n",
    "The mapping for an object $A_i$ in the Vantage Space is denoted by $F(A_i)$. Now it is pretty useful to define a distance metric $δ$ for this space we created, with the above formula:\n",
    "$$\n",
    "    δ(A_i,A_j) = \\max_{A_υ \\in Α*} | d(A_i,A_υ) - d(A_j,A_υ) | \n",
    "$$\n",
    "\n",
    "#### Kendall’s tau rank coefficient definition\n",
    "\n",
    "Let $(x_1,y_1), ... , (x_n,y_n)$ be a set of observations of the joint random variables X and Y, such that all the values of $(x_i)$ and $(y_i) $ are unique. Any pair of observations $(x_i,y_i)$ and $(x_j,y_j)$, where $i<j$ are said to be __concordant__ values in the sort order of $(x_i,x_j)$ and $(y_i,y_j)$ agrees: that is , if either both $x_i>x_j$ and $y_i>y_j$ holds or both $x_i<x_j$ and $y_i<y_j$ \n",
    "\n",
    "Otherwise they are said to be __discordant__.\n",
    "\n",
    "The Kendall τ coefficient is defined as:\n",
    "\\begin{align*}\n",
    "    τ &= \\frac{\\textbf{(number_of_concordant_pairs)} - \\textbf{(number_of_discordant_pairs)} }{ {n\\choose 2} }  && \\\\\n",
    "\\end{align*}\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/Concordant_Points_Kendall_Correlation.svg/330px-Concordant_Points_Kendall_Correlation.svg.png)\n",
    "$$\n",
    "\\textbf{Figure 3: Example of concordant and discordant points}\n",
    "$$\n",
    "\n",
    "In this example all points in the gray area are concordant and all points in the white area are discordant with respect to point $(X_{1},Y_{1})$. With $n=30$ points, there are a total of ${30 \\choose 2} = 435$ possible point pairs. In this example there are 395 concordant point pairs and 40 discordant point pairs, leading to a Kendall rank correlation coefficient of 0.816.\n",
    "\n",
    "Linking this definition to our work:\n",
    "\\begin{align*}\n",
    "    τ &= \\frac{|C| - |D|}{ {n \\choose 2} }  && \\\\\n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "- $|C|$: concordant pairs of components\n",
    "- $|D|$: discordant pairs of components\n",
    "- $  {n\\choose 2} = \\frac{n(n-1)}{2} $\n",
    "\n",
    "Equivalent way to compute Kendall's tau correlation coefficient is by computing the product of the signs of the differences between their $j$ and $i$ components. This method is shown in the following equation:\n",
    "\n",
    "$$\n",
    "τ = \\frac{2}{n(n-1)}\\sum_i\\sum_{i<j} \\textit{sign}(s_1[i] - s_1[j]) \\cdot \\textit{sign}(s_1[i] - s_1[j]) \n",
    "$$\n",
    "\n",
    "\n",
    "__SciPy implementation__\n",
    "\n",
    "Kendall's tau rank is implemented in the python library __SciPy__ for the stats projects and it will be used in this implementation. More info [scipy.stats.kendalltau](https://web.archive.org/web/20181008171919/https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html)\n",
    "\n",
    "````\n",
    "scipy.stats.kendalltau(x, y, initial_lexsort=None, nan_policy='propagate')\n",
    "````\n",
    "\n",
    "Use example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4714045207910316 0.2827454599327748\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "x1 = [12, 2, 1, 12, 2]\n",
    "x2 = [1, 4, 7, 1, 0]\n",
    "\n",
    "tau, p_value = stats.kendalltau(x1, x2)\n",
    "print(tau,p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method described in above must no be implemented and linked with 3.1 Clustering phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 A Top-k List Approach for Similarity Searching in the Vantage Space\n",
    "\n",
    "\n",
    "\n",
    "### Abstract Algebra definitions\n",
    "\n",
    "Lets get started with groups, by taking as example the group G:\n",
    "- A group 𝐺 is a set of elements based on which a compositional law is defined that obeys certain properties.\n",
    "- The group 𝐺 is:\n",
    "    - __closed under composition__, which is associative, and \n",
    "    - it contains an __identity element__, as well as \n",
    "    - __inverses__ of all the elements in the group.\n",
    "        \n",
    "Let S(A) be the set of all permutations on $𝐴 = \\{1, 2, 3, · · · , 𝑁\\}$ which is a group with respect to the composition of mappings. \n",
    "\n",
    "- __Finite group__: If a group 𝐺 has a finite number of elements \n",
    "- __|𝐺|__: the number of elements in a group\n",
    "- $S_N$: The group __𝑆(𝐴)__ is known as the symmetric group of 𝑁 elements\n",
    "- __Subgroup of 𝐺__: A subset 𝐻 of a group 𝐺 is called a subgroup of 𝐺 if 𝐻 forms a group with respect to the same operation that is defined on 𝐺.  If 𝐻 is a subgroup of 𝐺 and 𝑎 ∈ 𝐺 then:\n",
    "    - $𝑎𝐻 = {𝑥 ∈ 𝐺|𝑥 = 𝑎ℎ \\textit{ for some }ℎ ∈ 𝐻}$ is called a __left coset__ of 𝐻 in 𝐺 while 𝐻𝑎 is called a __right coset__ of 𝐻 in 𝐺\n",
    "    - The distinct left or right cosets of a subgroup H in a group 𝐺 form a partition of G.\n",
    "\n",
    "### Hausdorff metric\n",
    "The Hausdorff distance, or Hausdorff metric, also called Pompeiu–Hausdorff distance, measures how far two subsets of a metric space are from each other. It turns the set of non-empty compact subsets of a metric space into a metric space in its own right. Informally, two sets are close in the Hausdorff distance if every point of either set is close to some point of the other set. The Hausdorff distance is the longest distance you can be forced to travel by an adversary who chooses a point in one of the two sets, from where you then must travel to the other set. In other words, it is the greatest of all the distances from a point in one set to the closest point in the other set.\n",
    "\n",
    "\n",
    "Let $𝑆_{𝑁 −𝑘}$ be the subgroup of $𝑆_𝑁$ consisting of all permutations that leave the first 𝑘 integers fixed\n",
    "$𝑆_{𝑁−𝑘} = {𝜋 ∈ 𝑆_𝑁 |𝜋(𝑖) = 𝑖 \\textit{  for all } 𝑖 = 1, · · · , 𝑘}$.  A partition of the group $𝑆_𝑁$ can be generated then based on the subgroup $𝑆_{𝑁−𝑘}$ and the right cosets of this subgroup in $𝑆_𝑁$ . If we consider an arbitrary group 𝐺, and any subgroup 𝐾 of this group, as well as a metric 𝑑 which is defined on 𝐺, then the metric 𝑑 induces a metric $𝑑^∗$ on its coset space 𝐺/𝐾 which is called the Hausdorff metric induced by 𝑑 that measures the distance between any two right cosets $𝐾_𝜋, 𝐾_𝜎 ∈ 𝐺/𝐾$ by the formula shown in equation.\n",
    "\n",
    "$$\n",
    "𝑑^∗(𝐾𝜋, 𝐾𝜎) = max\\{ \\max_{𝛽 ∈𝐾𝜎} \\min_{𝛼 ∈ 𝐾𝜋} 𝑑(𝛼, 𝛽), \\max_{𝛼 ∈ 𝐾𝜋} \\min_{𝛽 ∈𝐾𝜎} 𝑑(𝛼, 𝛽)\\}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Hashing of Partially Ranked Data for Efficient Similarity Search\n",
    "\n",
    "After having demonstrated the extension of the Kendall tau distance\n",
    "metric to partially ranked data by using the induced Hausdorff\n",
    "distance, we proceed in the last step of our methodology, which\n",
    "is to incorporate a sparse embedding scheme on this metric. The\n",
    "adopted embedding scheme transforms the feature space of the\n",
    "ranked distances for its induced distance metric into integer codes\n",
    "upon which the hashing of the embedded strings will take place.\n",
    "This transformation has as a result that the computation of the\n",
    "similar pairs of strings will become as efficient computationally as\n",
    "possible, without sacrificing in return any of the recall and precision\n",
    "metrics along the way.\n",
    "\n",
    "### Winner Take All (WTA) hash\n",
    "A collection of binary coded vectors, each\n",
    "one of which corresponds to the position within a sized 𝑚 subset\n",
    "of the original ranked data vector, where the maximum rank value\n",
    "is located. More specifically, the authors propose an embedding\n",
    "to be applied to the embedded string space that is not sensitive\n",
    "to the rank values of the features themselves, but rather on the\n",
    "relative ranking of the values of these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# References\n",
    "\n",
    "[1]   [The dissimilarity representation for pattern recognition, a tutorial\n",
    "Robert P.W. Duin and Elzbieta Pekalska Delft University of Technology, The Netherlands School of Computer Science, University of Manchester, United Kingdom](http://homepage.tudelft.nl/a9p19/presentations/DisRep_Tutorial_doc.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
