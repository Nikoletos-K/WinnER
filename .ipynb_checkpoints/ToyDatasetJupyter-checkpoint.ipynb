{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    " <img src=\"http://www.di.uoa.gr/themes/corporate_lite/logo_en.png\" title=\"Department of Informatics and Telecommunications - University of Athens\"/> </p>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<h3 align=\"center\" > \n",
    "  Bachelor Thesis\n",
    "</h3>\n",
    "\n",
    "<h1 align=\"center\" > \n",
    "  Entity Resolution in Dissimilarity Spaces <br>\n",
    "  Implementation notebook\n",
    "</h1>\n",
    "\n",
    "---\n",
    "\n",
    "<h3 align=\"center\"> \n",
    " <b>Konstantinos Nikoletos</b>\n",
    "</h3>\n",
    "\n",
    "<h4 align=\"center\"> \n",
    " <b>Supervisors:<br> Dr. Alex Delis</b>,  Professor NKUA <br> <b> Dr. Vassilis Verikios</b>, Professor Hellenic Open University\n",
    "\n",
    "</h4>\n",
    "<br>\n",
    "<h4 align=\"center\"> \n",
    "Athens\n",
    "</h4>\n",
    "<h4 align=\"center\"> \n",
    "January 2021 - Ongoing\n",
    "</h4>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Implementation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __0.0 Install components__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: editdistance in c:\\users\\nikol\\anaconda3\\lib\\site-packages (0.5.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\nikol\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas_read_xml in c:\\users\\nikol\\anaconda3\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: zipfile36 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas_read_xml) (0.1.3)\n",
      "Collecting urllib3>=1.26.3\n",
      "  Using cached urllib3-1.26.5-py2.py3-none-any.whl (138 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas_read_xml) (1.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas_read_xml) (2.22.0)\n",
      "Requirement already satisfied: distlib in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas_read_xml) (0.3.2)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas_read_xml) (4.0.1)\n",
      "Requirement already satisfied: xmltodict in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas_read_xml) (0.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas->pandas_read_xml) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas->pandas_read_xml) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas->pandas_read_xml) (2019.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from requests->pandas_read_xml) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from requests->pandas_read_xml) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from requests->pandas_read_xml) (2.8)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas->pandas_read_xml) (1.14.0)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "Successfully installed urllib3-1.26.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: requests 2.22.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.5 which is incompatible.\n",
      "ERROR: botocore 1.12.189 has requirement urllib3<1.26,>=1.20, but you'll have urllib3 1.26.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas_read_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\nikol\\anaconda3\\lib\\site-packages (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from requests) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from requests) (2.8)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.5\n",
      "    Uninstalling urllib3-1.26.5:\n",
      "      Successfully uninstalled urllib3-1.26.5\n",
      "Successfully installed urllib3-1.25.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pandas-read-xml 0.3.1 has requirement urllib3>=1.26.3, but you'll have urllib3 1.25.11 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __0.1 Import libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import editdistance\n",
    "import string\n",
    "import sklearn\n",
    "import pandas_read_xml as pdx\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from scipy.spatial.distance import directed_hausdorff,hamming\n",
    "from scipy.stats._stats import _kendall_dis\n",
    "from scipy.stats import spearmanr,kendalltau,pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_score,accuracy_score,auc,f1_score,recall_score,precision_score,classification_report\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import stats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Final model__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankedWTAHash:\n",
    "\n",
    "  def __init__(self, max_numberOf_clusters, max_editDistance, windowSize, number_of_permutations=1, min_numOfNodes = 2, metric = 'kendal', similarityVectors='ranked', similarityThreshold=None, maxOnly=None ):\n",
    "    '''\n",
    "      Constructor\n",
    "    '''\n",
    "    self.max_numberOf_clusters = max_numberOf_clusters\n",
    "    self.pairDictionary = dict()\n",
    "    self.max_editDistance = max_editDistance\n",
    "    self.windowSize = windowSize\n",
    "    self.S_set = None \n",
    "    self.S_index = None \n",
    "    self.similarityThreshold = similarityThreshold\n",
    "    self.maxOnly = maxOnly\n",
    "    self.metric = metric\n",
    "    self.min_numOfNodes = min_numOfNodes\n",
    "    self.similarityVectors = similarityVectors\n",
    "    self.number_of_permutations = number_of_permutations\n",
    "  \n",
    "  def fit(self, X):\n",
    "    \"\"\"\n",
    "      Fit the classifier from the training dataset.\n",
    "      Parameters\n",
    "      ----------\n",
    "      X : Training data.\n",
    "      Returns\n",
    "      -------\n",
    "      self : The fitted classifier.\n",
    "    \"\"\"\n",
    "    print(\"\\n#####################################################################\\n#     .~ RankedWTAHash with Vantage embeddings starts training ~.   #\\n#####################################################################\\n\")\n",
    "\n",
    "    if isinstance(X, list):\n",
    "      input_strings = X\n",
    "    else:\n",
    "      input_strings = list(X)\n",
    "\n",
    "    # print(input_strings)\n",
    "    self.S_set = np.array(input_strings,dtype=object)\n",
    "    # print(self.S_set)\n",
    "    self.S_index = np.arange(0,len(input_strings),1)\n",
    "\n",
    "    # print(\"\\n\\nString positions are:\")\n",
    "    # print(self.S_index)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    print(\"###########################################################\\n# > 1. Prototype selection phase                          #\\n###########################################################\\n\")\n",
    "    print(\"\\n-> Finding prototypes and representatives of each cluster:\")\n",
    "    prototypes_time = time.time()\n",
    "    self.prototypeArray,self.selected_numOfPrototypes = self.Clustering_Prototypes(self.S_index,self.max_numberOf_clusters, self.max_editDistance, self.pairDictionary)\n",
    "    print(\"\\n- Prototypes selected\")\n",
    "    self.embeddingDim = self.prototypeArray.size\n",
    "    print(self.prototypeArray)\n",
    "    for pr in self.prototypeArray:\n",
    "        print(pr,\" -> \",self.S_set[pr])\n",
    "    print(\"\\n- Final number of prototypes: \",self.selected_numOfPrototypes )\n",
    "    prototypes_time = time.time() - prototypes_time\n",
    "    print(\"\\n# Finished in %.6s secs\" % (prototypes_time))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"###########################################################\\n# > 2. Embeddings based on the Vantage objects            #\\n###########################################################\\n\")\n",
    "    print(\"\\n-> Creating Embeddings:\")\n",
    "    embeddings_time = time.time()\n",
    "    self.Embeddings = self.CreateVantageEmbeddings(self.S_index,self.prototypeArray, self.pairDictionary)\n",
    "    print(\"- Embeddings created\")\n",
    "    print(self.Embeddings)\n",
    "    embeddings_time = time.time() - embeddings_time\n",
    "    print(\"\\n# Finished in %.6s secs\" % (embeddings_time))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    print(\"###########################################################\\n# > 3. WTA Hashing                                        #\\n###########################################################\\n\")\n",
    "    print(\"\\n-> Creating WTA Buckets:\")\n",
    "    wta_time = time.time()\n",
    "    self.HashedClusters,self.buckets,self.rankedVectors = self.WTA(self.Embeddings,self.windowSize,self.embeddingDim, self.number_of_permutations)\n",
    "    print(\"- WTA Buckets created\")\n",
    "    print(self.HashedClusters)\n",
    "    print(\"\\n- WTA number of buckets: \", len(np.unique(self.HashedClusters)))\n",
    "    print(\"\\n- WTA RankedVectors after permutation:\")\n",
    "    print(self.rankedVectors)\n",
    "    wta_time = time.time() - wta_time\n",
    "    print(\"\\n# Finished in %.6s secs\" % (wta_time))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"###########################################################\\n# > 4. Similarity checking                                #\\n###########################################################\\n\")\n",
    "    print(\"\\n-> Similarity checking:\")\n",
    "    similarity_time = time.time()\n",
    "    if self.similarityVectors == 'ranked':\n",
    "      self.mapping,self.mapping_matrix = self.SimilarityEvaluation(self.buckets,self.rankedVectors,self.similarityThreshold,maxOnly=self.maxOnly, metric=self.metric)\n",
    "    elif self.similarityVectors == 'initial':\n",
    "      self.mapping,self.mapping_matrix = self.SimilarityEvaluation(self.buckets,self.Embeddings,self.similarityThreshold,maxOnly=self.maxOnly, metric=self.metric)      \n",
    "    print(\"- Similarity mapping in a dictionary\")\n",
    "    print(self.mapping)\n",
    "    print(\"- Similarity mapping in a matrix\")\n",
    "    print(self.mapping_matrix)\n",
    "    similarity_time = time.time() - similarity_time\n",
    "    print(\"\\n# Finished in %.6s secs\" % (similarity_time))\n",
    "    print(\"\\n#####################################################################\\n#                    .~ End of training ~.                          #\\n#####################################################################\\n\")\n",
    "\n",
    "    return self\n",
    "\n",
    "  def EditDistance(self, str1,str2,verbose=False):\n",
    "      if verbose:\n",
    "        if str1 == None:\n",
    "            print(\"1\")\n",
    "        elif str2 == None:\n",
    "            print(\"2\")\n",
    "        print(\"-> \"+str(str1))\n",
    "        print(\"--> \"+str(str2))\n",
    "        print(str(editdistance.eval(self.S_set[str1],self.S_set[str2])))\n",
    "      \n",
    "      \n",
    "      # NOTE: Duplicates inside the dictionary     \n",
    "\n",
    "      if ((str1,str2) or (str2,str1))  in self.pairDictionary.keys():\n",
    "        return self.pairDictionary[(str1,str2)]\n",
    "      else:\n",
    "        # if verbose:\n",
    "        # print(\"++++++++++\")\n",
    "        # print(str1,str2)\n",
    "        # print(self.S_set[str1],self.S_set[str2])\n",
    "        # print(\"++++++++++\")\n",
    "        distance = editdistance.eval(self.S_set[str1],self.S_set[str2])\n",
    "        self.pairDictionary[(str2,str1)] = self.pairDictionary[(str1,str2)] = distance\n",
    "        return distance\n",
    "\n",
    "  #####################################################################\n",
    "  # 1. Prototype selection algorithm                                  #\n",
    "  #####################################################################\n",
    "\n",
    "  '''\n",
    "  Clustering_Prototypes(S,k,d,r,C) \n",
    "  The String Clustering and Prototype Selection Algorithm\n",
    "  is the main clustering method, that takes as input the intial strings S, \n",
    "  the max number of clusters to be generated in k,\n",
    "  the maximum allowable distance of a string to join a cluster in var d\n",
    "  and returns the prototype for each cluster in array Prototype\n",
    "  '''\n",
    "  def Clustering_Prototypes(self,S,k,d,pairDictionary,verbose=False):\n",
    "      \n",
    "      # ----------------- Initialization phase ----------------- #\n",
    "      i = 0\n",
    "      j = 0\n",
    "      C = np.empty([S.size], dtype=int)\n",
    "      r = np.empty([2,k],dtype=object)\n",
    "\n",
    "      Clusters = [ [] for l in range(0,k)]\n",
    "\n",
    "      for i in tqdm(range(0,S.size,1)):     # String-clustering phase, for all strings\n",
    "          while j < k :       # iteration through clusters, for all clusters\n",
    "              if r[0][j] == None:      # case empty first representative for cluster j\n",
    "                  r[0][j] = S[i]   # init cluster representative with string i\n",
    "                  C[i] = j         # store in C that i-string belongs to cluster j\n",
    "                  Clusters[j].append(S[i])\n",
    "                  break\n",
    "              elif r[1][j] == None and (self.EditDistance(S[i],r[0][j]) <= d):  # case empty second representative \n",
    "                  r[1][j] = S[i]                                             # and ED of representative 1  smaller than i-th string \n",
    "                  C[i] = j\n",
    "                  Clusters[j].append(S[i])\n",
    "                  break\n",
    "              elif (r[0][j] != None and r[1][j] != None) and (self.EditDistance(S[i],r[0][j]) + self.EditDistance(S[i],r[1][j])) <= d:\n",
    "                  C[i] = j\n",
    "                  Clusters[j].append(S[i])\n",
    "                  break\n",
    "              else:\n",
    "                  j += 1\n",
    "          i += 1\n",
    "\n",
    "      # ----------------- Prototype selection phase ----------------- #\n",
    "          \n",
    "      Projections = np.empty([k],dtype=object)\n",
    "      Prototypes = np.empty([k],dtype=int)\n",
    "      sortedProjections = np.empty([k],dtype=object)\n",
    "\n",
    "      Projections = []\n",
    "      Prototypes = []\n",
    "      sortedProjections = []\n",
    "\n",
    "      if verbose:\n",
    "          print(\"- - - - - - - - -\")\n",
    "          print(\"Cluster array:\")\n",
    "          print(C)\n",
    "          print(\"- - - - - - - - -\")\n",
    "          print(\"Represantatives array:\")\n",
    "          print(r)\n",
    "          print(\"- - - - - - - - -\")  \n",
    "          print(\"Clusters:\")\n",
    "          print(Clusters)\n",
    "          print(\"- - - - - - - - -\")  \n",
    "\n",
    "      new_numofClusters = k\n",
    "\n",
    "      # print(\"\\n\\n\\n****** Prototype selection phase *********\") \n",
    "      prototype_index = 0\n",
    "      for j in range(0,k,1):\n",
    "          \n",
    "          # IF small cluster\n",
    "          # print(\"Len \",len(Clusters[j]))\n",
    "          if len(Clusters[j]) < self.min_numOfNodes or r[1][j] == None or r[0][j]==None:\n",
    "            new_numofClusters-=1\n",
    "            continue\n",
    "\n",
    "          Projections.append(self.Approximated_Projection_Distances_ofCluster(r[1][j], r[0][j], j, Clusters[j],pairDictionary))         \n",
    "          # print(Projections[prototype_index])\n",
    "          sortedProjections.append({new_numofClusters: v for new_numofClusters, v in sorted(Projections[prototype_index].items(), key=lambda item: item[1])})\n",
    "          \n",
    "          \n",
    "          Prototypes.append(self.Median(sortedProjections[prototype_index]))\n",
    "          # print(Prototypes[prototype_index])\n",
    "\n",
    "          prototype_index += 1\n",
    "\n",
    "      # print(\"\\n****** END *********\\n\")\n",
    "\n",
    "      return np.array(Prototypes),new_numofClusters\n",
    "\n",
    "\n",
    "  def Approximated_Projection_Distances_ofCluster(self, right_rep, left_rep, cluster_id, clusterSet, pairDictionary):\n",
    "      # print(\"here\")\n",
    "      # print(clusterSet)\n",
    "      # print(right_rep, left_rep)\n",
    "\n",
    "      distances_vector = dict()\n",
    "\n",
    "      if len(clusterSet) > 2:\n",
    "        rep_distance     = self.EditDistance(right_rep,left_rep)\n",
    "                 \n",
    "        for str_inCluster in range(0,len(clusterSet)): \n",
    "          if clusterSet[str_inCluster] != right_rep and clusterSet[str_inCluster] != left_rep:\n",
    "            # print(clusterSet[str_inCluster],right_rep,left_rep)\n",
    "            right_rep_distance = self.EditDistance(right_rep,clusterSet[str_inCluster])\n",
    "            left_rep_distance  = self.EditDistance(left_rep,clusterSet[str_inCluster])\n",
    "            \n",
    "            if rep_distance == 0: \n",
    "              distances_vector[clusterSet[str_inCluster]] = 0\n",
    "            else:\n",
    "              distance = (right_rep_distance**2-rep_distance**2-left_rep_distance**2 ) / (2*rep_distance)\n",
    "              distances_vector[clusterSet[str_inCluster]] = distance\n",
    "      \n",
    "      else:\n",
    "        print(\"set: \",clusterSet)\n",
    "        print(\"left: \",left_rep)\n",
    "        print(\"right: \",right_rep)\n",
    "        if left_rep != None and right_rep == None:\n",
    "          distances_vector[left_rep] = left_rep\n",
    "          # print(\"l\")\n",
    "        elif right_rep != None and left_rep == None:\n",
    "          distances_vector[right_rep] = right_rep\n",
    "          # print(\"r\")\n",
    "        elif left_rep == None and right_rep == None:\n",
    "          return None\n",
    "        elif left_rep != None and right_rep != None:\n",
    "          distances_vector[right_rep] = right_rep\n",
    "          distances_vector[left_rep]  = left_rep\n",
    "      # print(distances_vector)\n",
    "      return distances_vector\n",
    "\n",
    "  def Median(self, distances):    \n",
    "      '''\n",
    "      Returns the median value of a vector\n",
    "      '''\n",
    "      keys = list(distances.keys())\n",
    "      if keys == 1:\n",
    "        return keys[0]\n",
    "\n",
    "      # print(distances)\n",
    "      keys = list(distances.keys())\n",
    "      # print(keys)\n",
    "      median_position = int(len(keys)/2)\n",
    "      # print(median_position)\n",
    "      median_value = keys[median_position]\n",
    "\n",
    "      return median_value\n",
    "  \n",
    "\n",
    "  \n",
    "  #####################################################################\n",
    "  #       2. Embeddings based on the Vantage objects                  #\n",
    "  #####################################################################\n",
    "\n",
    "  '''\n",
    "  CreateVantageEmbeddings(S,VantageObjects): Main function for creating the string embeddings based on the Vantage Objects\n",
    "  '''\n",
    "  def CreateVantageEmbeddings(self, S, VantageObjects, pairDictionary):\n",
    "      \n",
    "      # ------- Distance computing ------- #     \n",
    "      vectors = []\n",
    "      for s in tqdm(range(0,S.size)):\n",
    "          string_embedding = []\n",
    "          for p in range(0,VantageObjects.size): \n",
    "              if VantageObjects[p] != None:\n",
    "                  string_embedding.append(self.DistanceMetric(s,p,S,VantageObjects, pairDictionary))\n",
    "              \n",
    "          # --- Ranking representation ---- #\n",
    "          ranked_string_embedding = stats.rankdata(string_embedding, method='dense')\n",
    "          \n",
    "          # ------- Vectors dataset ------- #\n",
    "          vectors.append(ranked_string_embedding)\n",
    "      \n",
    "      return np.array(vectors)\n",
    "      \n",
    "\n",
    "  '''\n",
    "  DistanceMetric(s,p,S,Prototypes): Implementation of equation (5)\n",
    "  '''\n",
    "  def DistanceMetric(self, s, p, S, VantageObjects, pairDictionary):\n",
    "      \n",
    "      max_distance = None\n",
    "      \n",
    "      for pp in range(0,VantageObjects.size):\n",
    "          if VantageObjects[pp] != None:\n",
    "              string_distance = self.EditDistance(S[s],VantageObjects[pp])    # Edit distance String-i -> Vantage Object\n",
    "              VO_distance     = self.EditDistance(VantageObjects[p],VantageObjects[pp])    # Edit distance Vantage Object-j -> Vantage Object-i\n",
    "\n",
    "              abs_diff = abs(string_distance-VO_distance)\n",
    "\n",
    "              # --- Max distance diff --- #        \n",
    "              if max_distance == None:\n",
    "                  max_distance = abs_diff\n",
    "              elif abs_diff > max_distance:\n",
    "                  max_distance = abs_diff\n",
    "              \n",
    "      return max_distance\n",
    "\n",
    "  def dropNone(array):\n",
    "      array = list(filter(None, list(array)))\n",
    "      return np.array(array)\n",
    "\n",
    "  def topKPrototypes():\n",
    "      return\n",
    "\n",
    "  #####################################################################\n",
    "  #                 3. Similarity checking                            # \n",
    "  #####################################################################\n",
    "\n",
    "  def SimilarityEvaluation(self, buckets,vectors,threshold,maxOnly=None,metric=None):\n",
    "    \n",
    "    print(buckets)\n",
    "    print(vectors)\n",
    "    numOfVectors = vectors.shape[0]\n",
    "    vectorDim    = vectors.shape[1]\n",
    "    mapping_matrix = np.zeros([numOfVectors,numOfVectors],dtype=np.int8)\n",
    "    mapping = {}\n",
    "\n",
    "    # Loop for every bucket\n",
    "    for bucketid in tqdm(buckets.keys()):\n",
    "      bucket_vectors = buckets[bucketid]\n",
    "      numOfVectors = len(bucket_vectors)\n",
    "\n",
    "      # For every vector inside the bucket\n",
    "      for v_index in range(0,numOfVectors,1):\n",
    "        v_vector_id = bucket_vectors[v_index]\n",
    "\n",
    "        # Loop to all the other\n",
    "        for i_index in range(v_index+1,numOfVectors,1):\n",
    "          i_vector_id = bucket_vectors[i_index]\n",
    "\n",
    "          # print(vectors[v_vector_id])\n",
    "          # print(vectors[i_vector_id])\n",
    "          if vectorDim == 1:\n",
    "            warnings.warn(\"Vector dim equal to 1-Setting metric to kendalltau\")\n",
    "            metric = 'kendal'\n",
    "\n",
    "          if metric == None or metric == 'kendal':  # Simple Kendal tau metric\n",
    "            similarity_prob, p_value = kendalltau(vectors[v_vector_id], vectors[i_vector_id])\n",
    "          elif metric == 'customKendal':  # Custom Kendal tau\n",
    "            numOf_discordant_pairs = _kendall_dis(vectors[v_vector_id], vectors[i_vector_id])\n",
    "            similarity_prob = (2*numOf_discordant_pairs) / (vectorDim*(vectorDim-1))\n",
    "          elif metric == 'jaccard':\n",
    "            similarity_prob = jaccard_score(vectors[v_vector_id], vectors[i_vector_id], average='micro')\n",
    "          elif metric == 'cosine':\n",
    "            similarity_prob = cosine_similarity(np.array(vectors[v_vector_id]).reshape(1, -1), np.array(vectors[i_vector_id]).reshape(1, -1))\n",
    "          elif metric == 'pearson':\n",
    "            similarity_prob, _ = pearsonr(vectors[v_vector_id], vectors[i_vector_id])\n",
    "          elif metric == 'spearman':\n",
    "            similarity_prob, _ = spearmanr(vectors[v_vector_id], vectors[i_vector_id])\n",
    "          else:\n",
    "            print(\"Please choose metric\")\n",
    "            \n",
    "\n",
    "          # if v_vector_id == 0:\n",
    "          #   print(v_vector_id, i_vector_id,\" : \",similarity_prob )        \n",
    "          if similarity_prob > threshold or maxOnly:\n",
    "            if not maxOnly:\n",
    "              if v_vector_id not in mapping.keys():\n",
    "                mapping[v_vector_id] = []\n",
    "              mapping[v_vector_id].append(i_vector_id)  # insert into mapping\n",
    "              mapping_matrix[v_vector_id][i_vector_id] = 1  # inform prediction matrix\n",
    "              mapping_matrix[i_vector_id][v_vector_id] = 1\n",
    "            else:\n",
    "              if v_vector_id not in mapping.keys():  \n",
    "                mapping[v_vector_id] = (i_vector_id,similarity_prob)\n",
    "                mapping_matrix[v_vector_id][i_vector_id] = 1\n",
    "                mapping_matrix[i_vector_id][v_vector_id] = 1\n",
    "              else:\n",
    "                if mapping[v_vector_id][1] < similarity_prob:\n",
    "                  mapping[v_vector_id] = (i_vector_id,similarity_prob)\n",
    "                  mapping_matrix[v_vector_id][i_vector_id] = 1\n",
    "                  mapping_matrix[i_vector_id][v_vector_id] = 1\n",
    "    \n",
    "    return mapping, mapping_matrix\n",
    "\n",
    "  #####################################################################\n",
    "  #                        4. WTA Hashing                             # \n",
    "  #####################################################################\n",
    "\n",
    "  def WTA(self,vectors,K,inputDim, number_of_permutations):\n",
    "    '''\n",
    "      Winner Take All hash - Yagnik\n",
    "      .............................\n",
    "\n",
    "      K: window size\n",
    "    '''\n",
    "    newVectors = []\n",
    "    buckets = dict()\n",
    "\n",
    "    numOfVectors = vectors.shape[0]\n",
    "    vectorDim    = vectors.shape[1]\n",
    "\n",
    "    if vectorDim < K:\n",
    "      K = vectorDim\n",
    "      warnings.warn(\"Window size greater than vector dimension\")\n",
    "      \n",
    "    C = np.zeros([numOfVectors,number_of_permutations], dtype=int)\n",
    "    \n",
    "#     X_new = np.array(vectors)\n",
    "    \n",
    "    permutation_dimension = inputDim\n",
    "    for permutation_index in range(0,number_of_permutations,1):\n",
    "        theta = np.random.permutation(permutation_dimension)\n",
    "        i=0;j=0;\n",
    "#         print(newVectors)\n",
    "        for v_index in range(0,numOfVectors,1):\n",
    "#           print(v_index)\n",
    "          \n",
    "          if permutation_index == 0:\n",
    "#             print(\"Before: \",vectors[v_index])\n",
    "            X_new = self.permuted(vectors[v_index],theta)\n",
    "            newVectors.append(X_new[:K])\n",
    "          else:\n",
    "#             print(\"Before: \",newVectors[v_index])\n",
    "#             print(theta[:K])\n",
    "            X_new = self.permuted(newVectors[v_index],theta)\n",
    "            newVectors[v_index] = X_new[:K]\n",
    "          \n",
    "              \n",
    "          X_new = X_new[:K]\n",
    "#           print(\"After: \",X_new)\n",
    "#           print(\"X_new: \",X_new)\n",
    "          index_max = max(range(len(X_new)), key=X_new.__getitem__)\n",
    "#           print(\"- \",index_max)\n",
    "          c_i = index_max\n",
    "\n",
    "          for j in range(0,K,1):\n",
    "            if X_new[j] > X_new[c_i]:\n",
    "              c_i = j\n",
    "\n",
    "#           print(\"-> \",c_i)\n",
    "          C[i][permutation_index] = c_i\n",
    "#           print(C)\n",
    "          \n",
    "          i+=1\n",
    "        permutation_dimension = K\n",
    "\n",
    "    for c,i in zip(C,range(0,numOfVectors,1)):\n",
    "        buckets = self.bucketInsert(buckets,str(c),i)\n",
    "    print(C)\n",
    "    print(buckets)\n",
    "    return C,buckets,np.array(newVectors)\n",
    "\n",
    "  def permuted(self,vector,permutation):\n",
    "    permuted_vector = [vector[x] for x in permutation]\n",
    "    return permuted_vector \n",
    "\n",
    "  def bucketInsert(self,buckets,bucket_id,item):\n",
    "    if bucket_id not in buckets.keys():\n",
    "      buckets[bucket_id] = []\n",
    "    buckets[bucket_id].append(item)\n",
    "\n",
    "    return buckets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function\n",
    "\n",
    "Returns:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#                          Evaluation                               # \n",
    "#####################################################################\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "def evaluate_cora(predicted_matrix, true_matrix, with_classification_report=False ):\n",
    "\n",
    "  print(\"#####################################################################\\n#                          Evaluation                               #\\n#####################################################################\\n\")\n",
    "  true_matrix = csr_matrix(true_matrix)\n",
    "  # print(true_matrix)\n",
    "  predicted_matrix =  csr_matrix(predicted_matrix)\n",
    "  # print(predicted_matrix)\n",
    "\n",
    "  acc = 100*accuracy_score(true_matrix, predicted_matrix)\n",
    "  f1 =  100*f1_score(true_matrix, predicted_matrix, average='micro')\n",
    "  recall = 100*recall_score(true_matrix, predicted_matrix, average='micro')\n",
    "  precision = 100*precision_score(true_matrix, predicted_matrix, average='micro')\n",
    "\n",
    "  print(\"Accuracy:  %3.2f %%\" % (acc))\n",
    "  print(\"F1-Score:  %3.2f %%\" % (f1))\n",
    "  print(\"Recall:    %3.2f %%\" % (recall))\n",
    "  print(\"Precision: %3.2f %%\" % (precision))\n",
    "\n",
    "  # results_dataframe = pd.DataFrame(columns=['Accuracy','Precision','Recall','F1'])\n",
    "  # results_dataframe.loc[len(results_dataframe)+1] = [acc,precision,recall,f1]\n",
    "\n",
    "  if with_classification_report:\n",
    "    print(classification_report(true_matrix, predicted_matrix))\n",
    "\n",
    "  print('\\n\\n')\n",
    "  return acc,f1,precision,recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-8-d41362c9976d>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-d41362c9976d>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    for n2 in tqdm(max_editDistance):\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def GridSearch_cora(data,true_matrix,max_numberOf_clusters,max_editDistance,similarityThreshold,windowSize,metric,similarityVectors):\n",
    "    results_dataframe = pd.DataFrame(columns=['max_numberOf_clusters','max_editDistance','similarityThreshold','windowSize','metric','similarityVectors','Accuracy','Precision','Recall','F1','Time'])\n",
    "\n",
    "    for n1 in tqdm(max_numberOf_clusters):\n",
    "        for n2 in tqdm(max_editDistance):\n",
    "            for n3 in tqdm(similarityThreshold):\n",
    "                for n4 in tqdm(windowSize):\n",
    "                    for n5 in tqdm(metric):\n",
    "                        for n6 in tqdm(similarityVectors):\n",
    "                      print(\"-------------------------\")\n",
    "                      print('max_numberOf_clusters: ',n1)\n",
    "                      print('max_editDistancez: ',n2)\n",
    "                      print('similarityThreshold: ',n3)\n",
    "                      print('windowSize: ',n4)\n",
    "                      print('metric: ',n5)\n",
    "                      print('similarityVectors: ',n6)\n",
    "                      print(\"-------------------------\")\n",
    "                      start = time.time()\n",
    "                      model = RankedWTAHash(\n",
    "                          max_numberOf_clusters= n1,\n",
    "                          max_editDistance= n2,\n",
    "                          windowSize= n4,\n",
    "                          similarityThreshold= n3,\n",
    "                          maxOnly= False,\n",
    "                          metric=n5,\n",
    "                          similarityVectors=n6\n",
    "                      )\n",
    "                      model = model.fit(data)\n",
    "                      exec_time = time.time() - start\n",
    "                      acc,f1,precision,recall = evaluate_cora(model.mapping_matrix,true_matrix)\n",
    "                      results_dataframe.loc[len(results_dataframe)+1] = [n1,n2,n3,n4,n5,n6,acc,precision,recall,f1,exec_time]\n",
    "\n",
    "    return results_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# __Evaluation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Opening data file\n",
    "# import io\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __CoRA__ - New"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpcora = r\"/content/drive/My Drive/ERinDS/CORA.xml\"\n",
    "# fpcora_gold = r\"/content/drive/My Drive/ERinDS/cora_gold.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fpcora = os.path.abspath(\"CORA.xml\")\n",
    "fpcora_gold = os.path.abspath(\"cora_gold.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cora = pdx.read_xml(fpcora,['CORA', 'NEWREFERENCE'],root_is_rows=False)\n",
    "xml_dataframe = cora\n",
    "xml_dataframe.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import true values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_gold = pd.read_csv(fpcora_gold,sep=';')\n",
    "true_values = cora_gold\n",
    "cora_glod_30 = cora_gold.head(47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_glod_30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(row):\n",
    "\n",
    "    paper_str = \" \".join(row)\n",
    "    paper_str = paper_str.lower()\n",
    "    paper_str = paper_str.replace(\"\\n\", \" \").replace(\"/z\", \" \").replace(\"[\",\"\").replace(\"]\",\"\").replace(\",\", \" \")\n",
    "\n",
    "    return str(paper_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df = xml_dataframe.head(15).sample(frac=1).reset_index(drop=True)\n",
    "# shuffled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset with 30 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cora_createDataset(xml_dataframe, true_values, fields, keepNone = False):\n",
    "\n",
    "    rawStr_col = []\n",
    "    index_to_id_dict = {}\n",
    "    sameEntities_dictionary = {}\n",
    "\n",
    "    i=0\n",
    "    for _, row in tqdm(xml_dataframe.iterrows()):\n",
    "        index_to_id_dict[int(row['@id'])] = i\n",
    "\n",
    "        rawStr = []\n",
    "        for field in fields:    # NAN\n",
    "            if not (keepNone == False and field == None):\n",
    "                rawStr.append(str(row[field]))\n",
    "        i+=1\n",
    "        rawStr_col.append(preprocess(rawStr))\n",
    "\n",
    "    num_of_records = len(xml_dataframe)\n",
    "    trueValues_matrix = np.zeros([num_of_records,num_of_records],dtype=np.int8)\n",
    "\n",
    "    cluster_dict = {0:set()}\n",
    "    cluster_dict[0].add(0)\n",
    "    clusters = []\n",
    "    key = 0\n",
    "\n",
    "    for _, row in tqdm(true_values.head(47).iterrows()):  \n",
    "        # print(index_to_id_dict[row['id1']],index_to_id_dict[row['id2']])\n",
    "        trueValues_matrix[index_to_id_dict[row['id1']]][index_to_id_dict[row['id2']]] = 1\n",
    "        trueValues_matrix[index_to_id_dict[row['id2']]][index_to_id_dict[row['id1']]] = 1\n",
    "\n",
    "\n",
    "        if index_to_id_dict[row['id1']] in cluster_dict[key] or index_to_id_dict[row['id2']] in cluster_dict[key]:\n",
    "            cluster_dict[key].add(index_to_id_dict[row['id1']])\n",
    "            cluster_dict[key].add(index_to_id_dict[row['id2']])\n",
    "        elif index_to_id_dict[row['id1']] in cluster_dict[key] and index_to_id_dict[row['id2']] not in cluster_dict[key]: \n",
    "            cluster_dict[key].add(index_to_id_dict[row['id2']])\n",
    "        elif index_to_id_dict[row['id2']] in cluster_dict[key] and index_to_id_dict[row['id1']] not in cluster_dict[key]: \n",
    "            cluster_dict[key].add(index_to_id_dict[row['id1']])\n",
    "        elif index_to_id_dict[row['id2']] not in cluster_dict[key] and index_to_id_dict[row['id1']] not in cluster_dict[key]: \n",
    "            key+=1\n",
    "            cluster_dict[key] = set()\n",
    "            cluster_dict[key].add(index_to_id_dict[row['id1']])\n",
    "            cluster_dict[key].add(index_to_id_dict[row['id2']])    \n",
    "            print('here')\n",
    "        clusters.append(key)\n",
    "\n",
    "        if index_to_id_dict[row['id1']] not in sameEntities_dictionary.keys():\n",
    "             sameEntities_dictionary[index_to_id_dict[row['id1']]] = []\n",
    "        sameEntities_dictionary[ index_to_id_dict[row['id1']]].append( index_to_id_dict[row['id2']])\n",
    "\n",
    "    print(cluster_dict)\n",
    "    print(clusters)\n",
    "    return rawStr_col,sameEntities_dictionary, trueValues_matrix, clusters\n",
    "\n",
    "\n",
    "# fields = ['author', 'title', 'journal', 'volume', 'pages', 'date', '#text',\n",
    "#        'publisher', 'address', 'note', 'booktitle', 'editor', 'booktile',\n",
    "#        'tech', 'institution', 'Pages', 'year', 'type', 'month']\n",
    "\n",
    "fields = ['author', 'title', 'journal']\n",
    "\n",
    "data, true_labels, true_matrix, clusters = cora_createDataset(xml_dataframe.head(30), true_values, fields)\n",
    "\n",
    "cora_glod_30['cluster_id'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = [ len(x) for x in data ]\n",
    "print(\"Average length: %d\" % (np.mean(data_length)))\n",
    "print(\"Min length: %d\" % (min(data_length)))\n",
    "print(\"Max length: %d\" % (max(data_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(range(0,len(data_length),1),data_length)\n",
    "plt.xlabel(\"Record index\")\n",
    "plt.ylabel(\"String length\")\n",
    "plt.title(\"StrLength\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension reduction for visualization purposes - MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_matrix = np.zeros((len(data),len(data)), dtype=np.int)\n",
    "for i in range(0,len(data),1):\n",
    "    for j in range(0,len(data),1):\n",
    "        ed_matrix[i][j] = editdistance.eval(data[i],data[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(ed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(components[0],components[1],'o',markersize=10, alpha=0.8)\n",
    "plt.xlabel(\"C-0\")\n",
    "plt.ylabel(\"C-1\")\n",
    "plt.title(\"Dimension reduction\")\n",
    "# plt.legend(loc='center left', labelspacing=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = RankedWTAHash(\n",
    "    max_numberOf_clusters= 10,\n",
    "    max_editDistance= 112,\n",
    "    windowSize= 5,\n",
    "    similarityThreshold= 0.7,\n",
    "    metric='jaccard',\n",
    "    similarityVectors='ranked',\n",
    "    number_of_permutations = 5\n",
    ")\n",
    "model = model.fit(data)\n",
    "evaluate_cora(model.mapping_matrix,true_matrix, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# References\n",
    "\n",
    "1.   [The dissimilarity representation for pattern recognition, a tutorial\n",
    "Robert P.W. Duin and Elzbieta Pekalska Delft University of Technology, The Netherlands School of Computer Science, University of Manchester, United Kingdom](http://homepage.tudelft.nl/a9p19/presentations/DisRep_Tutorial_doc.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
