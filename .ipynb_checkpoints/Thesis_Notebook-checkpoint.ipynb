{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    " <img src=\"http://www.di.uoa.gr/themes/corporate_lite/logo_en.png\" title=\"Department of Informatics and Telecommunications - University of Athens\"/> </p>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<h3 align=\"center\" > \n",
    "  Bachelor Thesis\n",
    "</h3>\n",
    "\n",
    "<h1 align=\"center\" > \n",
    "  Entity Resolution in Dissimilarity Spaces\n",
    "</h1>\n",
    "\n",
    "---\n",
    "\n",
    "<h3 align=\"center\"> \n",
    " <b>Konstantinos Nikoletos</b>\n",
    "</h3>\n",
    "\n",
    "<h4 align=\"center\"> \n",
    " <b>Supervisor: Dr. Alex Delis</b>,  Professor NKUA\n",
    "</h4>\n",
    "<br>\n",
    "<h4 align=\"center\"> \n",
    "Athens\n",
    "</h4>\n",
    "<h4 align=\"center\"> \n",
    "January-September 2021\n",
    "</h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook it will be presented a dissimilarity-based entity resolution\n",
    "framework that introduces a new efficient object representation\n",
    "scheme. This framework consists of four parts. First part is the string clustering and prototype selection, in which clusters will be made that afterwords will be used for the string embedding. The second part in this methology is the string embedding into an N-dimensional Vantage space which has been generated by the prototype selection. Next, in the third part, it will be presented a distance measure that relies on Kendall tau\n",
    "correlation coefficient and generalizes the similarity measures and\n",
    "distances presented so far. Finally, in the fourth part, a sparse embedding scheme on this metric is added in order to minimize the computational cost of this methodology. \n",
    "\n",
    "This system will be evaluated in three databases. Its performance will be compared with some other famous Entity reslution systems in metrics Recall and Precision and also in computational time. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "---\n",
    "\n",
    "Every technique and methodology used in this work, that is out of the ordinary, will be briefly introduced and explained. Starting with Entity resolution. \n",
    "\n",
    "\n",
    " ### Entity resolution\n",
    " \n",
    "__Entity resolution (ER)__ or Deduplication are among the research themes that have recently received escalated interest. ER is the process of creating systematic linkage between disparate data records that represent the same thing in reality, in the absence of a join key. For example, as a previous project that I made, say you have a dataset of camera records from multiple websites (Amazon, AliBaba, etc) and you want to find which of these records refer to the same real object. Records may have slightly different names, somewhat different descriptions, maybe similar prices, and totally different unique identifiers. This may heard no big deal, but taking into serious the volume of some datasets and databases, gets you to understand how challenging, in prospects of  accuracy and computability this is. ER applications are now used for multiple reasons, not only for avoiding duplicates in databases, but also for reasons like finding \"similar\" accounts in social media or email, that are connected to  criminal actions.     \n",
    "\n",
    "The goal of this project, is to make an Entity resolution system that performs both well in Precision, Recall and execution time.  In this work we embrace an embedding approach by selecting a number of pivot objects to act as prototypes for transforming a dissimilarity space of proximities into a reduced set of distances of objects from these prototypes. It is now important to make clear what a dissimilarity space is. This definition comes from the fields of Statistis and theoritical Machine Learning. \n",
    "\n",
    "### Dissimilatiry Space\n",
    "\n",
    "Dissimilarities [1] have been used in pattern recognition for a long time. In the first approach the dissimilarity matrix is considered as a set of row vectors, one for every object.  They represent the objects in a vector space constructed by the dissimilarities to the other objects.  Usually, this vector space is treated as a Euclidean space and equipped with the standard inner product definition. Let $ \\textit{X} = \\{ x_1, . . . , x_n \\} $ be a training set. Given a dissimilarity function and/or dissimilarity data, we define a data-dependent mapping $ D(¬∑, R) : X ‚Üí R $ from  $ X $ to the so-called __dissimilarity space (DS)__ . The $k-element$ set $R$ consists of\n",
    "objects that are representative for the problem. This set is called the representation or __prototype set__ and it may be a subset of X . In the dissimilarity space each dimension $ D(¬∑, p_i) $ describes a dissimilarity to a prototype $ p_i $ from R. In this paper, we initially choose $ R := X $ . As a result, every object is described by an n-dimensional dissimilarity vector $ D(x, X ) = [d(x, x_1) . . . d(x, x_n)]^T $. The resulting vector space is endowed with the traditional inner product and the Euclidean metric.\n",
    "Any dissimilarity measure œÅ can be defined in the DS. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "___This project builds on four pillars:___\n",
    "\n",
    "1. Object partitioning and embedding. More specifically the embedding technique that is mainly used is called __Vantage Embedding__  and the __Chorus of Prototypes scheme__ .\n",
    "2. Machine Learning techniques that build __nearest neighbors classification__ models on the selection of prototypes\n",
    "3. Various correlation coefficient and distance metrics that are applied on ranked data as well as a generalization of the well known __Hausdorff distance metric for partially ranked data__.\n",
    "4. __Locality Sensitive Hashing (LSH)__ techniques specifically tuned for handling ranking data to render the similarity search process very efficient.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A dissimilarity-based space embedding methodology\n",
    "---\n",
    "\n",
    "Central theme in this methodology is the transformation of the input data in a representation form that can easily and accurately circumvent the inherent lack of features of objects and handle a variety of different data types in a unified way. \n",
    "\n",
    "\n",
    "The first step is to read and process the input data (strings in this particular work). Section 3.1 consists of the idea and the algorithm of string clustering in order to create the embeddings. But, firstly, it is highly important to define what's the Vantage Space and the Chorus of Prototypes scheme. These approaches are used in order to to efficiently and effectively capture the similarity of high dimensional data. \n",
    "\n",
    "### Vantage Space\n",
    "\n",
    "The *Vantage Objects* approach maps pairwise distances of input objects into an ùëÅ-dimensional space of pivot objects which is known as __Vantage Space__ in such a way that points that lie close to each other in this space correspond to similar objects in the original dissimilarity space. The __Chorus of Prototypes Transform (CT)__ on the other hand proposes the use of a rank correlation coefficient defined on the data induced by the distances of the input objects from the pivot objects.\n",
    "\n",
    "After a brief definition of the above embedding techniques, we need to create a set of string prototypes and according to these prototypes, create the embeddings into an N-dimensional space. \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 String Clustering and Prototype Selection\n",
    "\n",
    "The first step in this methodology is to cluster the input strings in order to identify the cluster representatives that will be used as prototypes in the embedding process. It is vey important to mention that every cluster needs two representative strings that will selected from the clustering algorithm.\n",
    "\n",
    "__Why do we need 2 representatives?__\n",
    "\n",
    "This way we can compute the inner product space where one string serves the origin and the other the endpoint of a vector.\n",
    "\n",
    "After the formation of the clusters, the prototype selection phase follows, in which one of the members in each cluster (not necessarily one of the two cluster representatives), will become its unique prototype and will be used\n",
    "as the pivot object for the embedding.\n",
    "\n",
    "Note that based on the assumed dissimilarity representation of the input objects, the considered distance metric for the input strings in this work is the __Edit Distance metric__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit distance metric\n",
    "Edit distance is a way of quantifying how dissimilar two strings (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other. Œíased on the assumed dissimilarity representation of the input objects, the considered distance metric for the input strings in this work is the Edit Distance metric.\n",
    "\n",
    "Theres already an implementation for this metric in library editdistance\n",
    "\n",
    "Downloading the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting editdistance\n",
      "  Downloading editdistance-0.5.3-cp37-cp37m-win_amd64.whl (23 kB)\n",
      "Installing collected packages: editdistance\n",
      "Successfully installed editdistance-0.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit distance: 2\n"
     ]
    }
   ],
   "source": [
    "import editdistance\n",
    "\n",
    "print(\"Edit distance: \"+str(editdistance.eval('banana', 'bahama')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can either implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit distance: 2\n"
     ]
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/edit-distance-dp-5/\n",
    "# A Naive recursive Python program to fin minimum number\n",
    "# operations to convert str1 to str2\n",
    " \n",
    "def EditDistance(str1, str2, m, n):\n",
    " \n",
    "    # If first string is empty, the only option is to\n",
    "    # insert all characters of second string into first\n",
    "    if m == 0:\n",
    "        return n\n",
    "\n",
    "    # If second string is empty, the only option is to\n",
    "    # remove all characters of first string\n",
    "    if n == 0:\n",
    "        return m\n",
    "\n",
    "    # If last characters of two strings are same, nothing\n",
    "    # much to do. Ignore last characters and get count for\n",
    "    # remaining strings.\n",
    "    if str1[m-1] == str2[n-1]:\n",
    "        return EditDistance(str1, str2, m-1, n-1)\n",
    "\n",
    "    # If last characters are not same, consider all three\n",
    "    # operations on last character of first string, recursively\n",
    "    # compute minimum cost for all three operations and take\n",
    "    # minimum of three values.\n",
    "    return 1 + min(EditDistance(str1, str2, m, n-1),    # Insert\n",
    "               EditDistance(str1, str2, m-1, n),    # Remove\n",
    "               EditDistance(str1, str2, m-1, n-1))    # Replace\n",
    "                \n",
    "str1 = \"banana\"\n",
    "str2 = \"bahama\"\n",
    "print(\"Edit distance: \"+str(EditDistance(str1, str2, len(str1), len(str2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String clustering algorithm\n",
    "\n",
    "The string clustering algorithm produces as its output two vectors that contain for each discovered cluster, two representatives, as well as the assignment of individual strings to the closest cluster.\n",
    "\n",
    "***Functionality:***\n",
    "The string clustering algorithm iterates through the list of the input strings, and for each input string loops over the list of the currently discovered clusters. The algorithm starts processing the of the first discovered cluster1. The second string will be checked against the first representative of the first cluster, and if it is the case that the distance between these two strings is less than the distance threshold ùëë given as input to the algorithm, the second string will join the first cluster and it will become the second representative of this cluster. If the distance between these two strings is greater\n",
    "than ùëë, then the algorithm will go on with the next cluster. Given there is only one cluster which has been created so far, and by assuming that the distance between the second string and the first representative is greater than the threshold, the second string will be automatically allocated to the second cluster (which is empty at this moment), in which case it will become its first representative.\n",
    "\n",
    "\n",
    "### Cluster  membership condition\n",
    "\n",
    "When a new string is checked for membership to a cluster, the condition that must be satisfied by this newly coming string in order to join this cluster is that the sum of its distances from the two representatives must be less than the distance threshold.\n",
    "\n",
    "$$\n",
    "\\textbf{String_Sum_Of_Distances < Distance_Threshold}\n",
    "$$\n",
    "\n",
    "By comparing the newly arrived string against the two cluster representatives and by ensuring that the cluster\n",
    "membership condition is satisfied, we have the right to provide distance guarantees for all the pairs of strings that will eventually join the same cluster, meaning that no string in the cluster is more than ùëë distance away from any other string in the same cluster.\n",
    "\n",
    "![](img/fig_1.png)\n",
    "\n",
    "$$\n",
    "\\textbf{Figure 1: Properties of distances of strings from cluster representatives}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Triangle inequality\n",
    "\n",
    "Figure 1 is a visualize of the membership condition that must be fullfilled in each cluster. \n",
    "In this graph:\n",
    "\n",
    "- Nodes __A,B,C,D,E,F__ represent the strings\n",
    "- Edges represent the Edit distances among the strings\n",
    "\n",
    "__Figure Scenario:__\n",
    "\n",
    "1. String __A__ joins first the cluster\n",
    "2. String __B__ joins first the cluster $\\Rightarrow$ __A__,__B__ become the 2 representatives\n",
    "3. __A__,__B__ distance must be less than d $\\Rightarrow$ $\\textbf{distance(A,B) < d}$\n",
    "4. String __C__ is checker for entering the cluster. Condition checked: $\\sum_{n=A,B}\\textbf{distance(C,n) < d}$. If condition is valid string __C__ enters the cluster.\n",
    "5. Assume that strings __D__ and __E__ are checked for membership to the cluster in a similar fashion,which is done by questioning for each one of them whether their sum of the distances from the two representatives is less than or equal to the distance threshold. Assuming also that these conditions fulfilled and they join the clusters. It can be proofed from the triangle inequality that the distances of the newly joined __C__,__D__,__E__ from each other are below d.\n",
    "\n",
    "(PROOF)\n",
    "\n",
    "\n",
    "### Algorithm complexity\n",
    "For $n$ strings and $k$ prototypes: $\\textit{O(nk)}$\n",
    "\n",
    "\n",
    "### Prototype selection\n",
    "From among the strings that were allocated to a certain cluster by the string clustering algorithm, we need to specify one string that will play the role of the cluster prototype. This method benefits the algorithm complexity, as it is only needed to compare the incoming new string with the 2 representatives and not with all the cluster. Even though either one of the two representatives of each cluster which were derived during the clustering process can assume the cluster prototype role, there are still other choices that are way more precise and can be computed in a computationally efficient manner. To accomplish this, we rely on the notion of the set median string which has appeared as a concept in a number of studies before. \n",
    "\n",
    "\n",
    "\n",
    "#### Median string\n",
    "The string ùëö that belongs to a set of strings ùëÜ and satisfies the following property\n",
    "$$\n",
    "m = \\textit{argmax}_{y\\in S} \\sum_{\\forall x \\in S} d(x,y)\n",
    "$$\n",
    "\n",
    "Briefly, this condition means that it is the string in S whose sum of the distances from all the othe strings in S is minimum.\n",
    "It is obvious that this procedure has an expencive computation as it is needed to traverse the set of strings multiple times. For this reason in this framework it will be used another way of finding the cluster prototypes. \n",
    "\n",
    "\n",
    "![](img/fig_2.png)\n",
    "\n",
    "$$\n",
    "\\textbf{Figure 2: Projections of distances of strings from cluster representative A}\n",
    "$$\n",
    "\n",
    "Figure 2 illustrates the idea of the string prototype selection for a random cluster of strings. It will be demontrated an efficient alforithm for selecting prototypes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The algorithm takes as input:***\n",
    " - __ùëÜ__: the input strings in vector, \n",
    " - __ùëò__: the maximum number of clusters to be generated, \n",
    " - __d__: the maximum allowable distance of a string to join a cluster\n",
    "\n",
    "it produces in the first phase two arrays,\n",
    " - an array variable __ùê∂__ that contains the assignment of individual strings to cluster identities, and \n",
    " - the 2D array variable __r__ that maintains the assignment of representatives to clusters. \n",
    "\n",
    "In the second phase, it produces the assignment of prototypes to clusters in the array variable __Prototype__.\n",
    "\n",
    "***Returns:***\n",
    "- __Prototype__: array\n",
    "- __ùê∂__: array\n",
    "- __r__: 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLUSTERING_PROTOTYPES(S,k,d,r,C):\n",
    "\n",
    "    i = 1\n",
    "    j = 1\n",
    "        \n",
    "    while i <=   :     # String-clustering phase\n",
    "        while j <= k :\n",
    "            if r[j]:\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_strings = [\"abcd\",\"efgh\",\"h\",\"i\"]\n",
    "max_number_of_clusters = 2\n",
    "msx_distance = 1\n",
    "C = []\n",
    "r = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 The Vantage Space Embedding and the Chorus of Prototypes Transform Similarity Coefficient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 A Top-k List Approach for Similarity Searching in the Vantage Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Hashing of Partially Ranked Data for Efficient Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1]   [The dissimilarity representation for pattern recognition, a tutorial\n",
    "Robert P.W. Duin and Elzbieta Pekalska Delft University of Technology, The Netherlands School of Computer Science, University of Manchester, United Kingdom](http://homepage.tudelft.nl/a9p19/presentations/DisRep_Tutorial_doc.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
