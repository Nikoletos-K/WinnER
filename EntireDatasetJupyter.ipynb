{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    " <img src=\"http://www.di.uoa.gr/themes/corporate_lite/logo_en.png\" title=\"Department of Informatics and Telecommunications - University of Athens\"/> </p>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<h3 align=\"center\" > \n",
    "  Bachelor Thesis\n",
    "</h3>\n",
    "\n",
    "<h1 align=\"center\" > \n",
    "  Entity Resolution in Dissimilarity Spaces <br>\n",
    "  Implementation notebook\n",
    "</h1>\n",
    "\n",
    "---\n",
    "\n",
    "<h3 align=\"center\"> \n",
    " <b>Konstantinos Nikoletos</b>\n",
    "</h3>\n",
    "\n",
    "<h4 align=\"center\"> \n",
    " <b>Supervisors:<br> Dr. Alex Delis</b>,  Professor NKUA <br> <b> Dr. Vassilis Verikios</b>, Professor Hellenic Open University\n",
    "\n",
    "</h4>\n",
    "<br>\n",
    "<h4 align=\"center\"> \n",
    "Athens\n",
    "</h4>\n",
    "<h4 align=\"center\"> \n",
    "January 2021 - Ongoing\n",
    "</h4>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Implementation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __0.0 Install components__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: editdistance in c:\\users\\nikol\\anaconda3\\lib\\site-packages (0.5.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\nikol\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas_read_xml in c:\\users\\nikol\\anaconda3\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: zipfile36 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas_read_xml) (0.1.3)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas_read_xml) (4.0.1)\n",
      "Requirement already satisfied: distlib in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas_read_xml) (0.3.2)\n",
      "Collecting urllib3>=1.26.3\n",
      "  Using cached urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas_read_xml) (2.22.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas_read_xml) (1.0.1)\n",
      "Requirement already satisfied: xmltodict in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas_read_xml) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pyarrow->pandas_read_xml) (1.18.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from requests->pandas_read_xml) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from requests->pandas_read_xml) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from requests->pandas_read_xml) (2019.11.28)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas->pandas_read_xml) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from pandas->pandas_read_xml) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas->pandas_read_xml) (1.14.0)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "Successfully installed urllib3-1.26.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: requests 2.22.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.6 which is incompatible.\n",
      "ERROR: botocore 1.12.189 has requirement urllib3<1.26,>=1.20, but you'll have urllib3 1.26.6 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas_read_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\nikol\\anaconda3\\lib\\site-packages (2.22.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from requests) (2.8)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from requests) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\nikol\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.6\n",
      "    Uninstalling urllib3-1.26.6:\n",
      "      Successfully uninstalled urllib3-1.26.6\n",
      "Successfully installed urllib3-1.25.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pandas-read-xml 0.3.1 has requirement urllib3>=1.26.3, but you'll have urllib3 1.25.11 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __0.1 Import libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import editdistance\n",
    "import string\n",
    "import sklearn\n",
    "import pandas_read_xml as pdx\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from scipy.spatial.distance import directed_hausdorff,hamming\n",
    "from scipy.stats._stats import _kendall_dis\n",
    "from scipy.stats import spearmanr,kendalltau,pearsonr,kruskal,mannwhitneyu\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import jaccard_distance\n",
    "from nltk.metrics.distance import jaro_similarity,jaro_winkler_similarity\n",
    "from sklearn.metrics import jaccard_score,accuracy_score,auc,f1_score,recall_score,precision_score,classification_report\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "from scipy import stats \n",
    "from scipy.spatial.distance import euclidean,hamming\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "plt.style.use('seaborn-whitegrid') # plot style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Final model__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankedWTAHash:\n",
    "\n",
    "  def __init__(self, max_numberOf_clusters, max_editDistance, windowSize, number_of_permutations=1, min_numOfNodes = 2,jaccard_withchars =True,distanceMetricEmbedding = 'l_inf', metric = 'kendal', similarityVectors='ranked', distanceMetric = 'edit', ngramms=3, similarityThreshold=None, maxOnly=None ):\n",
    "    '''\n",
    "      Constructor\n",
    "    '''\n",
    "    self.max_numberOf_clusters = max_numberOf_clusters\n",
    "    self.pairDictionary = dict()\n",
    "    self.max_editDistance = max_editDistance\n",
    "    self.windowSize = windowSize\n",
    "    self.S_set = None \n",
    "    self.S_index = None \n",
    "    self.similarityThreshold = similarityThreshold\n",
    "    self.maxOnly = maxOnly\n",
    "    self.metric = metric\n",
    "    self.min_numOfNodes = min_numOfNodes\n",
    "    self.similarityVectors = similarityVectors\n",
    "    self.number_of_permutations = number_of_permutations\n",
    "    self.distanceMetric = distanceMetric\n",
    "    self.distanceMetricEmbedding = distanceMetricEmbedding\n",
    "    self.ngramms = ngramms\n",
    "    self.jaccard_withchars =  jaccard_withchars\n",
    "  \n",
    "  def fit(self, X):\n",
    "    \"\"\"\n",
    "      Fit the classifier from the training dataset.\n",
    "      Parameters\n",
    "      ----------\n",
    "      X : Training data.\n",
    "      Returns\n",
    "      -------\n",
    "      self : The fitted classifier.\n",
    "    \"\"\"\n",
    "    print(\"\\n#####################################################################\\n#     .~ RankedWTAHash with Vantage embeddings starts training ~.   #\\n#####################################################################\\n\")\n",
    "\n",
    "    if isinstance(X, list):\n",
    "      input_strings = X\n",
    "    else:\n",
    "      input_strings = list(X)\n",
    "\n",
    "    # print(input_strings)\n",
    "    self.initialS_set = np.array(input_strings,dtype=object)\n",
    "    self.S_set = np.array(input_strings,dtype=object)\n",
    "    if self.distanceMetric == 'jaccard' and self.jaccard_withchars == False:    \n",
    "      for i in range(0,len(input_strings)):\n",
    "        self.S_set[i] = set(nltk.ngrams(nltk.word_tokenize(data[i]), n=self.ngramms))\n",
    "    elif self.distanceMetric == 'jaccard' and self.jaccard_withchars == True:    \n",
    "      for i in range(0,len(input_strings)):\n",
    "        self.S_set[i] = set(nltk.ngrams(data[i], n=self.ngramms))\n",
    "    # print(self.S_set)\n",
    "\n",
    "    self.S_index = np.arange(0,len(input_strings),1)\n",
    "\n",
    "    # print(\"\\n\\nString positions are:\")\n",
    "    # print(self.S_index)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    print(\"###########################################################\\n# > 1. Prototype selection phase                          #\\n###########################################################\\n\")\n",
    "    print(\"\\n-> Finding prototypes and representatives of each cluster:\")\n",
    "    prototypes_time = time.time()\n",
    "    self.prototypeArray,self.selected_numOfPrototypes = self.Clustering_Prototypes(self.S_index,self.max_numberOf_clusters, self.max_editDistance, self.pairDictionary)\n",
    "    print(\"\\n- Prototypes selected\")\n",
    "    self.embeddingDim = self.prototypeArray.size\n",
    "    print(self.prototypeArray)\n",
    "    for pr in self.prototypeArray:\n",
    "        print(pr,\" -> \",self.initialS_set[pr])\n",
    "    print(\"\\n- Final number of prototypes: \",self.selected_numOfPrototypes )\n",
    "    prototypes_time = time.time() - prototypes_time\n",
    "    print(\"\\n# Finished in %.6s secs\" % (prototypes_time))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"###########################################################\\n# > 2. Embeddings based on the Vantage objects            #\\n###########################################################\\n\")\n",
    "    print(\"\\n-> Creating Embeddings:\")\n",
    "    embeddings_time = time.time()\n",
    "    self.Embeddings = self.CreateVantageEmbeddings(self.S_index,self.prototypeArray, self.pairDictionary)\n",
    "    print(\"- Embeddings created\")\n",
    "    print(self.Embeddings)\n",
    "    embeddings_time = time.time() - embeddings_time\n",
    "    print(\"\\n# Finished in %.6s secs\" % (embeddings_time))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    print(\"###########################################################\\n# > 3. WTA Hashing                                        #\\n###########################################################\\n\")\n",
    "    print(\"\\n-> Creating WTA Buckets:\")\n",
    "    wta_time = time.time()\n",
    "    self.HashedClusters,self.buckets,self.rankedVectors = self.WTA(self.Embeddings,self.windowSize,self.embeddingDim, self.number_of_permutations)\n",
    "    print(\"- WTA buckets: \")\n",
    "    for key in self.buckets.keys():\n",
    "      print(key,\" -> \",self.buckets[key])\n",
    "    print(\"\\n- WTA number of buckets: \", len(self.buckets.keys()))\n",
    "    print(\"\\n- WTA RankedVectors after permutation:\")\n",
    "    print(self.rankedVectors)\n",
    "    wta_time = time.time() - wta_time\n",
    "    print(\"\\n# Finished in %.6s secs\" % (wta_time))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"###########################################################\\n# > 4. Similarity checking                                #\\n###########################################################\\n\")\n",
    "    print(\"\\n-> Similarity checking:\")\n",
    "    similarity_time = time.time()\n",
    "\n",
    "    if self.similarityVectors == 'ranked':\n",
    "      self.mapping,self.mapping_matrix = self.SimilarityEvaluation(self.buckets,self.rankedVectors,self.similarityThreshold,maxOnly=self.maxOnly, metric=self.metric)\n",
    "    elif self.similarityVectors == 'initial':\n",
    "      self.mapping,self.mapping_matrix = self.SimilarityEvaluation(self.buckets,self.Embeddings,self.similarityThreshold,maxOnly=self.maxOnly, metric=self.metric)      \n",
    "    else:\n",
    "      warnings.warn(\"similarityVectors: Available options are ranked,initial\")\n",
    "#     print(\"- Similarity matrix (all values compared):\")\n",
    "#     print(self.similarityProb_matrix)\n",
    "    print(\"- Similarity mapping in a matrix\")\n",
    "    print(self.mapping_matrix)\n",
    "    similarity_time = time.time() - similarity_time\n",
    "    print(\"\\n# Finished in %.6s secs\" % (similarity_time))\n",
    "    print(\"\\n#####################################################################\\n#                    .~ End of training ~.                          #\\n#####################################################################\\n\")\n",
    "\n",
    "    return self\n",
    "\n",
    "  def EditDistance(self, str1,str2,verbose=False):\n",
    "      if verbose:\n",
    "        if str1 == None:\n",
    "            print(\"1\")\n",
    "        elif str2 == None:\n",
    "            print(\"2\")\n",
    "        print(\"-> \"+str(str1))\n",
    "        print(\"--> \"+str(str2))\n",
    "        print(str(editdistance.eval(self.S_set[str1],self.S_set[str2])))\n",
    "      \n",
    "      \n",
    "      # NOTE: Duplicates inside the dictionary     \n",
    "\n",
    "      if ((str1,str2) or (str2,str1))  in self.pairDictionary.keys():\n",
    "        return self.pairDictionary[(str1,str2)]\n",
    "      else:\n",
    "        # if verbose:\n",
    "        # print(\"++++++++++\")\n",
    "        # print(str1,str2)\n",
    "        # print(self.S_set[str1],self.S_set[str2])\n",
    "        # print(\"++++++++++\")\n",
    "        if self.distanceMetric == 'edit':\n",
    "            distance = editdistance.eval(self.S_set[str1],self.S_set[str2])\n",
    "        elif self.distanceMetric == 'jaccard':\n",
    "            distance = jaccard_distance(self.S_set[str1],self.S_set[str2])\n",
    "        else:\n",
    "            warnings.warn(\"Available metrics for space creation: edit, jaccard \")\n",
    "        self.pairDictionary[(str2,str1)] = self.pairDictionary[(str1,str2)] = distance\n",
    "        return distance\n",
    "\n",
    "  #####################################################################\n",
    "  # 1. Prototype selection algorithm                                  #\n",
    "  #####################################################################\n",
    "\n",
    "  '''\n",
    "  Clustering_Prototypes(S,k,d,r,C) \n",
    "  The String Clustering and Prototype Selection Algorithm\n",
    "  is the main clustering method, that takes as input the intial strings S, \n",
    "  the max number of clusters to be generated in k,\n",
    "  the maximum allowable distance of a string to join a cluster in var d\n",
    "  and returns the prototype for each cluster in array Prototype\n",
    "  '''\n",
    "  def Clustering_Prototypes(self,S,k,d,pairDictionary,verbose=False):\n",
    "      \n",
    "      # ----------------- Initialization phase ----------------- #\n",
    "      i = 0\n",
    "      j = 0\n",
    "      C = np.empty([S.size], dtype=int)\n",
    "      r = np.empty([2,k],dtype=object)\n",
    "\n",
    "      Clusters = [ [] for l in range(0,k)]\n",
    "\n",
    "      for i in tqdm(range(0,S.size,1)):     # String-clustering phase, for all strings\n",
    "          while j < k :       # iteration through clusters, for all clusters\n",
    "              if r[0][j] == None:      # case empty first representative for cluster j\n",
    "                  r[0][j] = S[i]   # init cluster representative with string i\n",
    "                  C[i] = j         # store in C that i-string belongs to cluster j\n",
    "                  Clusters[j].append(S[i])\n",
    "                  break\n",
    "              elif r[1][j] == None and (self.EditDistance(S[i],r[0][j]) <= d):  # case empty second representative \n",
    "                  r[1][j] = S[i]                                             # and ED of representative 1  smaller than i-th string \n",
    "                  C[i] = j\n",
    "                  Clusters[j].append(S[i])\n",
    "                  break\n",
    "              elif (r[0][j] != None and r[1][j] != None) and (self.EditDistance(S[i],r[0][j]) + self.EditDistance(S[i],r[1][j])) <= d:\n",
    "                  C[i] = j\n",
    "                  Clusters[j].append(S[i])\n",
    "                  break\n",
    "              else:\n",
    "                  j += 1\n",
    "          i += 1\n",
    "\n",
    "      # ----------------- Prototype selection phase ----------------- #\n",
    "          \n",
    "      Projections = np.empty([k],dtype=object)\n",
    "      Prototypes = np.empty([k],dtype=int)\n",
    "      sortedProjections = np.empty([k],dtype=object)\n",
    "\n",
    "      Projections = []\n",
    "      Prototypes = []\n",
    "      sortedProjections = []\n",
    "\n",
    "      if verbose:\n",
    "          print(\"- - - - - - - - -\")\n",
    "          print(\"Cluster array:\")\n",
    "          print(C)\n",
    "          print(\"- - - - - - - - -\")\n",
    "          print(\"Represantatives array:\")\n",
    "          print(r)\n",
    "          print(\"- - - - - - - - -\")  \n",
    "          print(\"Clusters:\")\n",
    "          print(Clusters)\n",
    "          print(\"- - - - - - - - -\")  \n",
    "\n",
    "      new_numofClusters = k\n",
    "\n",
    "      # print(\"\\n\\n\\n****** Prototype selection phase *********\") \n",
    "      prototype_index = 0\n",
    "      for j in range(0,k,1):\n",
    "          \n",
    "          # IF small cluster\n",
    "          # print(\"Len \",len(Clusters[j]))\n",
    "          if len(Clusters[j]) < self.min_numOfNodes or r[1][j] == None or r[0][j]==None:\n",
    "            new_numofClusters-=1\n",
    "            continue\n",
    "\n",
    "          Projections.append(self.Approximated_Projection_Distances_ofCluster(r[1][j], r[0][j], j, Clusters[j],pairDictionary))         \n",
    "          # print(Projections[prototype_index])\n",
    "          sortedProjections.append({new_numofClusters: v for new_numofClusters, v in sorted(Projections[prototype_index].items(), key=lambda item: item[1])})\n",
    "          \n",
    "          \n",
    "          Prototypes.append(self.Median(sortedProjections[prototype_index]))\n",
    "          # print(Prototypes[prototype_index])\n",
    "\n",
    "          prototype_index += 1\n",
    "\n",
    "      # print(\"\\n****** END *********\\n\")\n",
    "\n",
    "      return np.array(Prototypes),new_numofClusters\n",
    "\n",
    "\n",
    "  def Approximated_Projection_Distances_ofCluster(self, right_rep, left_rep, cluster_id, clusterSet, pairDictionary):\n",
    "      # print(\"here\")\n",
    "      # print(clusterSet)\n",
    "      # print(right_rep, left_rep)\n",
    "\n",
    "      distances_vector = dict()\n",
    "\n",
    "      if len(clusterSet) > 2:\n",
    "        rep_distance     = self.EditDistance(right_rep,left_rep)\n",
    "                 \n",
    "        for str_inCluster in range(0,len(clusterSet)): \n",
    "          if clusterSet[str_inCluster] != right_rep and clusterSet[str_inCluster] != left_rep:\n",
    "            # print(clusterSet[str_inCluster],right_rep,left_rep)\n",
    "            right_rep_distance = self.EditDistance(right_rep,clusterSet[str_inCluster])\n",
    "            left_rep_distance  = self.EditDistance(left_rep,clusterSet[str_inCluster])\n",
    "            \n",
    "            if rep_distance == 0: \n",
    "              distances_vector[clusterSet[str_inCluster]] = 0\n",
    "            else:\n",
    "              distance = (right_rep_distance**2-rep_distance**2-left_rep_distance**2 ) / (2*rep_distance)\n",
    "              distances_vector[clusterSet[str_inCluster]] = distance\n",
    "      \n",
    "      else:\n",
    "#         print(\"set: \",clusterSet)\n",
    "#         print(\"left: \",left_rep)\n",
    "#         print(\"right: \",right_rep)\n",
    "        if left_rep != None and right_rep == None:\n",
    "          distances_vector[left_rep] = left_rep\n",
    "          # print(\"l\")\n",
    "        elif right_rep != None and left_rep == None:\n",
    "          distances_vector[right_rep] = right_rep\n",
    "          # print(\"r\")\n",
    "        elif left_rep == None and right_rep == None:\n",
    "          return None\n",
    "        elif left_rep != None and right_rep != None:\n",
    "          distances_vector[right_rep] = right_rep\n",
    "          distances_vector[left_rep]  = left_rep\n",
    "      # print(distances_vector)\n",
    "      return distances_vector\n",
    "\n",
    "  def Median(self, distances):    \n",
    "      '''\n",
    "      Returns the median value of a vector\n",
    "      '''\n",
    "      keys = list(distances.keys())\n",
    "      if keys == 1:\n",
    "        return keys[0]\n",
    "\n",
    "      # print(distances)\n",
    "      keys = list(distances.keys())\n",
    "      # print(keys)\n",
    "      median_position = int(len(keys)/2)\n",
    "      # print(median_position)\n",
    "      median_value = keys[median_position]\n",
    "\n",
    "      return median_value\n",
    "  \n",
    "\n",
    "  \n",
    "  #####################################################################\n",
    "  #       2. Embeddings based on the Vantage objects                  #\n",
    "  #####################################################################\n",
    "\n",
    "  '''\n",
    "  CreateVantageEmbeddings(S,VantageObjects): Main function for creating the string embeddings based on the Vantage Objects\n",
    "  '''\n",
    "  def CreateVantageEmbeddings(self, S, VantageObjects, pairDictionary):\n",
    "      \n",
    "      # ------- Distance computing ------- #     \n",
    "      vectors = []\n",
    "      for s in tqdm(range(0,S.size)):\n",
    "          string_embedding = []\n",
    "          for p in range(0,VantageObjects.size): \n",
    "              if VantageObjects[p] != None:\n",
    "#                   print(\"-\",VantageObjects[p])\n",
    "                  string_embedding.append(self.DistanceMetric(s,p,S,VantageObjects, pairDictionary))\n",
    "              \n",
    "          # --- Ranking representation ---- #\n",
    "          ranked_string_embedding = stats.rankdata(string_embedding, method='min')\n",
    "          \n",
    "          # ------- Vectors dataset ------- #\n",
    "          vectors.append(ranked_string_embedding)\n",
    "      \n",
    "      return np.array(vectors)\n",
    "      \n",
    "\n",
    "  '''\n",
    "  DistanceMetric(s,p,S,Prototypes): Implementation of equation (5)\n",
    "  '''\n",
    "  def DistanceMetric(self, s, p, S, VantageObjects, pairDictionary, distanceMetricEmbedding = 'l_inf'):\n",
    "      \n",
    "      if distanceMetricEmbedding == 'l_inf':\n",
    "          max_distance = None\n",
    "\n",
    "          for pp in range(0,VantageObjects.size):\n",
    "              if VantageObjects[pp] != None:\n",
    "                  string_distance = self.EditDistance(S[s],VantageObjects[pp])    # Edit distance String-i -> Vantage Object\n",
    "                  VO_distance     = self.EditDistance(VantageObjects[p],VantageObjects[pp])    # Edit distance Vantage Object-j -> Vantage Object-i\n",
    "\n",
    "                  abs_diff = abs(string_distance-VO_distance)\n",
    "\n",
    "                  # --- Max distance diff --- #        \n",
    "                  if max_distance == None:\n",
    "                      max_distance = abs_diff\n",
    "                  elif abs_diff > max_distance:\n",
    "                      max_distance = abs_diff\n",
    "          return max_distance\n",
    "    \n",
    "      elif distanceMetricEmbedding == 'edit':\n",
    "          return self.EditDistance(S[s],VantageObjects[p])\n",
    "      elif distanceMetricEmbedding == 'jaccard':\n",
    "          return jaccard_distance(S[s],VantageObjects[p])\n",
    "      elif distanceMetricEmbedding == 'euclidean':\n",
    "          return euclidean(S[s],VantageObjects[p])\n",
    "      else:\n",
    "          warnings.warn(\"Available metrics: edit,jaccard,l_inf,euclidean\")\n",
    "        \n",
    "\n",
    "  def dropNone(array):\n",
    "      array = list(filter(None, list(array)))\n",
    "      return np.array(array)\n",
    "\n",
    "  def topKPrototypes():\n",
    "      return\n",
    "\n",
    "  #####################################################################\n",
    "  #                 3. Similarity checking                            # \n",
    "  #####################################################################\n",
    "\n",
    "  def SimilarityEvaluation(self, buckets,vectors,threshold,maxOnly=None,metric=None):\n",
    "    \n",
    "#     print(buckets)\n",
    "#     print(vectors)\n",
    "    numOfVectors = vectors.shape[0]\n",
    "    vectorDim    = vectors.shape[1]\n",
    "    mapping_matrix = np.zeros([numOfVectors,numOfVectors],dtype=np.int8)\n",
    "    self.similarityProb_matrix = np.empty([numOfVectors,numOfVectors],dtype=np.float)* np.nan\n",
    "    mapping = {}\n",
    "\n",
    "    # Loop for every bucket\n",
    "    for bucketid in tqdm(buckets.keys()):\n",
    "      bucket_vectors = buckets[bucketid]\n",
    "      numOfVectors = len(bucket_vectors)\n",
    "      \n",
    "      print(bucket_vectors)\n",
    "      # For every vector inside the bucket\n",
    "      for v_index in range(0,numOfVectors,1):\n",
    "        v_vector_id = bucket_vectors[v_index]\n",
    "        # Loop to all the other\n",
    "        for i_index in range(v_index+1,numOfVectors,1):\n",
    "          i_vector_id = bucket_vectors[i_index]\n",
    "#           print('v_vector_id: ',v_vector_id,'i_vector_id: ',i_vector_id)\n",
    "#           print(vectors[v_vector_id], \" | \",vectors[i_vector_id])\n",
    "\n",
    "          if vectorDim == 1:\n",
    "            warnings.warn(\"Vector dim equal to 1-Setting metric to kendalltau\")\n",
    "            metric = 'kendal'\n",
    "\n",
    "          if metric == None or metric == 'kendal':  # Simple Kendal tau metric\n",
    "            similarity_prob, p_value = kendalltau(vectors[v_vector_id], vectors[i_vector_id])\n",
    "          elif metric == 'customKendal':  # Custom Kendal tau\n",
    "            numOf_discordant_pairs = _kendall_dis(vectors[v_vector_id].astype('intp'), vectors[i_vector_id].astype('intp'))\n",
    "            similarity_prob = (2*numOf_discordant_pairs) / (vectorDim*(vectorDim-1))\n",
    "          elif metric == 'jaccard':\n",
    "            similarity_prob = jaccard_score(vectors[v_vector_id], vectors[i_vector_id], average='micro')\n",
    "          elif metric == 'cosine':\n",
    "            similarity_prob = cosine_similarity(np.array(vectors[v_vector_id]).reshape(1, -1), np.array(vectors[i_vector_id]).reshape(1, -1))\n",
    "          elif metric == 'pearson':\n",
    "            similarity_prob, _ = pearsonr(vectors[v_vector_id], vectors[i_vector_id])\n",
    "          elif metric == 'spearman':\n",
    "            similarity_prob, _ = spearmanr(vectors[v_vector_id], vectors[i_vector_id])\n",
    "          elif metric == 'spearmanf':\n",
    "            similarity_prob = 1-spearman_footrule_distance(vectors[v_vector_id], vectors[i_vector_id])\n",
    "          elif metric == 'hamming':\n",
    "            similarity_prob, _ = hamming(vectors[v_vector_id].astype('intp'), vectors[i_vector_id].astype('intp'))\n",
    "          elif metric == 'kruskal':\n",
    "            if np.array_equal(vectors[v_vector_id],vectors[i_vector_id]):\n",
    "              similarity_prob=1.0\n",
    "            else:  \n",
    "              _,similarity_prob = kruskal(vectors[v_vector_id], vectors[i_vector_id])\n",
    "          elif metric == 'ndcg_score':\n",
    "            similarity_prob, _ = ndcg_score(vectors[v_vector_id], vectors[i_vector_id])\n",
    "          elif metric == 'mannwhitneyu':\n",
    "            if np.array_equal(vectors[v_vector_id],vectors[i_vector_id]):\n",
    "              similarity_prob=1.0\n",
    "            else:  \n",
    "               _,similarity_prob = mannwhitneyu(vectors[v_vector_id], vectors[i_vector_id])\n",
    "          else:\n",
    "            warnings.warn(\"SimilarityEvaluation: Available similarity metrics: kendal,customKendal,jaccard,ndcg_score,cosine,spearman,pearson\")\n",
    "            \n",
    "          self.similarityProb_matrix[v_vector_id][i_vector_id] = similarity_prob\n",
    "#           print(\"pr: \",self.similarityProb_matrix[v_vector_id][i_vector_id],similarity_prob)\n",
    "          # if v_vector_id == 0:\n",
    "          #   print(v_vector_id, i_vector_id,\" : \",similarity_prob )        \n",
    "          if similarity_prob > threshold:\n",
    "            if v_vector_id not in mapping.keys():\n",
    "              mapping[v_vector_id] = []\n",
    "            mapping[v_vector_id].append(i_vector_id)  # insert into mapping\n",
    "            mapping_matrix[v_vector_id][i_vector_id] = 1  # inform prediction matrix\n",
    "            mapping_matrix[i_vector_id][v_vector_id] = 1  # inform prediction matrix\n",
    "\n",
    "    \n",
    "    return mapping, np.triu(mapping_matrix)\n",
    "\n",
    "  #####################################################################\n",
    "  #                        4. WTA Hashing                             # \n",
    "  #####################################################################\n",
    "\n",
    "  def WTA(self,vectors,K,inputDim, number_of_permutations):\n",
    "    '''\n",
    "      Winner Take All hash - Yagnik\n",
    "      .............................\n",
    "\n",
    "      K: window size\n",
    "    '''\n",
    "    newVectors = []\n",
    "    buckets = dict()\n",
    "\n",
    "    numOfVectors = vectors.shape[0]\n",
    "    vectorDim    = vectors.shape[1]\n",
    "\n",
    "    if vectorDim < K:\n",
    "      K = vectorDim\n",
    "      warnings.warn(\"Window size greater than vector dimension\")\n",
    "      \n",
    "    C = np.zeros([numOfVectors,number_of_permutations], dtype=int)\n",
    "    \n",
    "#     X_new = np.array(vectors)\n",
    "    \n",
    "    permutation_dimension = inputDim\n",
    "    for permutation_index in range(0,number_of_permutations,1):\n",
    "        theta = np.random.permutation(permutation_dimension)\n",
    "        i=0;j=0;\n",
    "#         print(newVectors)\n",
    "        for v_index in range(0,numOfVectors,1):\n",
    "#           print(v_index)\n",
    "          \n",
    "          if permutation_index == 0:\n",
    "#             print(\"Before: \",vectors[v_index])\n",
    "            X_new = self.permuted(vectors[v_index],theta)\n",
    "            newVectors.append(X_new)\n",
    "          else:\n",
    "#             print(\"Before: \",newVectors[v_index])\n",
    "#             print(theta[:K])\n",
    "            X_new = self.permuted(vectors[v_index],theta)\n",
    "            newVectors[v_index] = X_new\n",
    "          \n",
    "              \n",
    "          X_new = X_new[:K]\n",
    "#           print(\"After: \",X_new)\n",
    "#           print(\"X_new: \",X_new)\n",
    "          index_max = max(range(len(X_new)), key=X_new.__getitem__)\n",
    "#           print(\"- \",index_max)\n",
    "          c_i = index_max\n",
    "\n",
    "          for j in range(0,K,1):\n",
    "            if X_new[j] > X_new[c_i]:\n",
    "              c_i = j\n",
    "\n",
    "#           print(\"-> \",c_i)\n",
    "          C[i][permutation_index] = c_i\n",
    "#           print(C)\n",
    "          \n",
    "          i+=1\n",
    "        permutation_dimension = K\n",
    "\n",
    "    for c,i in zip(C,range(0,numOfVectors,1)):\n",
    "        buckets = self.bucketInsert(buckets,str(c),i)\n",
    "#     print(C)\n",
    "#     print(buckets)\n",
    "    return C,buckets,np.array(newVectors,dtype=np.intp)\n",
    "\n",
    "  def permuted(self,vector,permutation):\n",
    "    permuted_vector = [vector[x] for x in permutation]\n",
    "    return permuted_vector \n",
    "\n",
    "  def bucketInsert(self,buckets,bucket_id,item):\n",
    "    if bucket_id not in buckets.keys():\n",
    "      buckets[bucket_id] = []\n",
    "    buckets[bucket_id].append(item)\n",
    "\n",
    "    return buckets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function\n",
    "\n",
    "Returns:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#                          Evaluation                               # \n",
    "#####################################################################\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "def evaluate_cora(predicted_matrix, true_matrix, with_classification_report=False ):\n",
    "\n",
    "  print(\"#####################################################################\\n#                          Evaluation                               #\\n#####################################################################\\n\")\n",
    "  true_matrix = sparse.triu(true_matrix)\n",
    "  # print(true_matrix)\n",
    "  predicted_matrix =  sparse.triu(predicted_matrix)\n",
    "  # print(predicted_matrix)\n",
    "\n",
    "  acc = 100*accuracy_score(true_matrix, predicted_matrix)\n",
    "  f1 =  100*f1_score(true_matrix, predicted_matrix, average='micro')\n",
    "  recall = 100*recall_score(true_matrix, predicted_matrix, average='micro')\n",
    "  precision = 100*precision_score(true_matrix, predicted_matrix, average='micro')\n",
    "\n",
    "  print(\"Accuracy:  %3.2f %%\" % (acc))\n",
    "  print(\"F1-Score:  %3.2f %%\" % (f1))\n",
    "  print(\"Recall:    %3.2f %%\" % (recall))\n",
    "  print(\"Precision: %3.2f %%\" % (precision))\n",
    "\n",
    "  # results_dataframe = pd.DataFrame(columns=['Accuracy','Precision','Recall','F1'])\n",
    "  # results_dataframe.loc[len(results_dataframe)+1] = [acc,precision,recall,f1]\n",
    "\n",
    "  if with_classification_report:\n",
    "    print(classification_report(true_matrix, predicted_matrix))\n",
    "\n",
    "  print('\\n\\n')\n",
    "  return acc,f1,precision,recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch_cora(data,true_matrix,max_numberOf_clusters,max_editDistance,similarityThreshold,windowSize,metric,similarityVectors,distanceMetricEmbedding,distanceMetric,number_of_permutations):\n",
    "    results_dataframe = pd.DataFrame(columns=['max_numberOf_clusters','max_editDistance','similarityThreshold','windowSize','metric','similarityVectors',\"distanceMetricEmbedding\",\"distanceMetric\",\"number_of_permutations\",'Accuracy','Precision','Recall','F1','Time'])\n",
    "\n",
    "    for n1 in tqdm(max_numberOf_clusters):\n",
    "        for n2 in tqdm(max_editDistance):\n",
    "            for n3 in tqdm(similarityThreshold):\n",
    "                for n4 in tqdm(windowSize):\n",
    "                    for n5 in tqdm(metric):\n",
    "                        for n6 in tqdm(similarityVectors):\n",
    "                            for n7 in tqdm(distanceMetricEmbedding):\n",
    "                                for n8 in tqdm(distanceMetric):\n",
    "                                    for n9 in tqdm(number_of_permutations):\n",
    "                                        print(\"-------------------------\")\n",
    "                                        print('max_numberOf_clusters: ',n1)\n",
    "                                        print('max_editDistancez: ',n2)\n",
    "                                        print('similarityThreshold: ',n3)\n",
    "                                        print('windowSize: ',n4)\n",
    "                                        print('metric: ',n5)\n",
    "                                        print('similarityVectors: ',n6)\n",
    "                                        print('distanceMetricEmbedding: ',n7)\n",
    "                                        print('distanceMetric: ',n8)\n",
    "                                        print('number_of_permutations: ',n9)\n",
    "                                        print(\"-------------------------\")\n",
    "                                        start = time.time()\n",
    "                                        model = RankedWTAHash(\n",
    "                                          max_numberOf_clusters= n1,\n",
    "                                          max_editDistance= n2,\n",
    "                                          windowSize= n4,\n",
    "                                          similarityThreshold= n3,\n",
    "                                          maxOnly= False,\n",
    "                                          metric=n5,\n",
    "                                          similarityVectors=n6\n",
    "                                        )\n",
    "                                        model = model.fit(data)\n",
    "                                        exec_time = time.time() - start\n",
    "                                        acc,f1,precision,recall = evaluate_cora(model.mapping_matrix,true_matrix)\n",
    "                                        results_dataframe.loc[len(results_dataframe)+1] = [n1,n2,n3,n4,n5,n6,n7,n8,n9,acc,precision,recall,f1,exec_time]\n",
    "\n",
    "    return results_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_footrule_distance(s,t):\n",
    "    \"\"\"\n",
    "    Computes the Spearman footrule distance between two full lists of ranks:\n",
    "        F(s,t) = sum[ |s(i) - t(i)| ]/S,\n",
    "    the normalized sum over all elements in a set of the absolute difference between\n",
    "    the rank according to s and t.  As defined, 0 <= F(s,t) <= 1.\n",
    "    S is a normalizer which is equal to 0.5*len(s)^2 for even length ranklists and\n",
    "    0.5*(len(s)^2 - 1) for odd length ranklists.\n",
    "    If s,t are *not* full, this function should not be used. s,t should be array-like\n",
    "    (lists are OK).\n",
    "    \"\"\"\n",
    "    # check that size of intersection = size of s,t?\n",
    "    assert len(s) == len(t)\n",
    "    sdist = sum(abs(s - t))\n",
    "    # c will be 1 for odd length lists and 0 for even ones\n",
    "    c = len(s) % 2\n",
    "    normalizer = 0.5*(len(s)**2 - c)\n",
    "    return sdist/normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# __Evaluation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __CoRA__ - New"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Opening data file\n",
    "# import io\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive',force_remount=True)\n",
    "\n",
    "# fpcora = r\"/content/drive/My Drive/ERinDS/CORA.xml\"\n",
    "# fpcora_gold = r\"/content/drive/My Drive/ERinDS/cora_gold.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fpcora = os.path.abspath(\"CORA.xml\")\n",
    "fpcora_gold = os.path.abspath(\"cora_gold.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cora = pdx.read_xml(fpcora,['CORA', 'NEWREFERENCE'],root_is_rows=False)\n",
    "xml_dataframe = cora\n",
    "xml_dataframe['@id'] = pd.to_numeric(xml_dataframe['@id']).subtract(1)\n",
    "# xml_dataframe.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import true values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cora_gold = pd.read_csv(fpcora_gold,sep=';')\n",
    "true_values = cora_gold\n",
    "cora_gold['id1'] = pd.to_numeric(cora_gold['id1']).subtract(1)\n",
    "cora_gold['id2'] = pd.to_numeric(cora_gold['id2']).subtract(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(row):\n",
    "\n",
    "    paper_str = \" \".join(row)\n",
    "    paper_str = paper_str.lower()\n",
    "    paper_str = paper_str.replace(\"\\n\", \" \").replace(\"/z\", \" \").replace(\"[\",\"\").replace(\"]\",\"\").replace(\",\", \" \")\n",
    "\n",
    "    return str(paper_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df = xml_dataframe.sample(frac=1).reset_index(drop=True)\n",
    "# shuffled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset with 30 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a8b1069bce426097ff1436615a01a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428e35dcc62747f4ab073f05c77b0417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def cora_createDataset(xml_dataframe, true_values, fields, keepNone = False):\n",
    "\n",
    "    rawStr_col = []\n",
    "    index_to_id_dict = {}\n",
    "    sameEntities_dictionary = {}\n",
    "\n",
    "    i=0\n",
    "    for _, row in tqdm(xml_dataframe.iterrows()):\n",
    "        index_to_id_dict[int(row['@id'])] = i\n",
    "\n",
    "        rawStr = []\n",
    "        for field in fields:    # NAN\n",
    "            if keepNone == True and field != None:\n",
    "                rawStr.append(str(row[field]))\n",
    "        i+=1\n",
    "        rawStr_col.append(preprocess(rawStr))\n",
    "\n",
    "    num_of_records = len(xml_dataframe)\n",
    "    trueValues_matrix = np.zeros([num_of_records,num_of_records],dtype=np.int8)\n",
    "\n",
    "    cluster_dict = {0:set()}\n",
    "    cluster_dict[0].add(0)\n",
    "    clusters = []\n",
    "    key = 0\n",
    "\n",
    "    for _, row in tqdm(true_values.iterrows()):  \n",
    "        # print(index_to_id_dict[row['id1']],index_to_id_dict[row['id2']])\n",
    "        trueValues_matrix[index_to_id_dict[row['id1']]][index_to_id_dict[row['id2']]] = 1\n",
    "        trueValues_matrix[index_to_id_dict[row['id2']]][index_to_id_dict[row['id1']]] = 1\n",
    "\n",
    "\n",
    "        if index_to_id_dict[row['id1']] in cluster_dict[key] or index_to_id_dict[row['id2']] in cluster_dict[key]:\n",
    "            cluster_dict[key].add(index_to_id_dict[row['id1']])\n",
    "            cluster_dict[key].add(index_to_id_dict[row['id2']])\n",
    "        elif index_to_id_dict[row['id1']] in cluster_dict[key] and index_to_id_dict[row['id2']] not in cluster_dict[key]: \n",
    "            cluster_dict[key].add(index_to_id_dict[row['id2']])\n",
    "        elif index_to_id_dict[row['id2']] in cluster_dict[key] and index_to_id_dict[row['id1']] not in cluster_dict[key]: \n",
    "            cluster_dict[key].add(index_to_id_dict[row['id1']])\n",
    "        elif index_to_id_dict[row['id2']] not in cluster_dict[key] and index_to_id_dict[row['id1']] not in cluster_dict[key]: \n",
    "            key+=1\n",
    "            cluster_dict[key] = set()\n",
    "            cluster_dict[key].add(index_to_id_dict[row['id1']])\n",
    "            cluster_dict[key].add(index_to_id_dict[row['id2']])    \n",
    "#             print('here')\n",
    "        clusters.append(key)\n",
    "\n",
    "        if index_to_id_dict[row['id1']] not in sameEntities_dictionary.keys():\n",
    "             sameEntities_dictionary[index_to_id_dict[row['id1']]] = []\n",
    "        sameEntities_dictionary[ index_to_id_dict[row['id1']]].append( index_to_id_dict[row['id2']])\n",
    "\n",
    "#     print(cluster_dict)\n",
    "#     print(clusters)\n",
    "    return rawStr_col,sameEntities_dictionary, np.triu(trueValues_matrix), clusters\n",
    "\n",
    "\n",
    "# fields = ['author', 'title', 'journal', 'volume', 'pages', 'date', '#text',\n",
    "#        'publisher', 'address', 'note', 'booktitle', 'editor', 'booktile',\n",
    "#        'tech', 'institution', 'Pages', 'year', 'type', 'month']\n",
    "\n",
    "fields = ['author', 'title', 'journal']\n",
    "\n",
    "data, true_labels, true_matrix, clusters = cora_createDataset(xml_dataframe, cora_gold, fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length: 0\n",
      "Min length: 0\n",
      "Max length: 0\n",
      "Median length: 0\n"
     ]
    }
   ],
   "source": [
    "data_length = [ len(x) for x in data ]\n",
    "print(\"Average length: %d\" % (np.mean(data_length)))\n",
    "print(\"Min length: %d\" % (min(data_length)))\n",
    "print(\"Max length: %d\" % (max(data_length)))\n",
    "print(\"Median length: %d\" % (np.median(data_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA30AAAFFCAYAAABYPIRQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de1hVdb7H8c9mc9EAS0TTQkotnkMXRtEazdvxGM9Yj2laXI3G0krPkUrLUEtFUbwkWKNJQxfHQ6ai1nTmnKZpLJMkS9vGmA5aec1LeWFMN8l1r/PHPO6JhKiAxe7n+/VPrPXblw98n7V3H9e+OCzLsgQAAAAAMJJfSwcAAAAAADQfSh8AAAAAGIzSBwAAAAAGo/QBAAAAgMEofQAAAABgMEofAAAAABiM0gcAMF5xcbFSU1N1xx13aOjQoRo7dqw+//xzSdL999+v0tLSOq+Xmpqqt956y5aM383xH//xH/r0009tuV8AgPn8WzoAAADNqbKyUg899JBefvllXX/99ZKkN954Qw888IDeeecdFRUVtXDCf/KVHAAA81D6AABGO3funM6ePatvv/3Wu2/YsGEKCQnRU089JUn67W9/q7y8PI0aNUoxMTHas2ePJk2a9IO3++677yo3N1dVVVVq1aqV0tPT1aNHDy1ZskRHjhzRiRMndOTIEV1++eV6+umn1aFDB+3YsUMZGRmqqqpSZGSkjh49qilTpuiPf/xjrRyStGbNGs2cOVOlpaUaPny4Jk6c2Ex/IQCA6Xh5JwDAaJdeeqkmT56ssWPHavDgwZo8ebLWr1+vW265RfPmzZMkrVixQp06dZIkXXvttfrzn/+suLi4em/zwIEDWrx4sfLy8vTHP/5RmZmZSktL8xbLjz/+WM8++6zeeusttW7dWqtXr1Z1dbXS0tL0yCOP6E9/+pNSU1NVUlIiSXXmCAoK0muvvaa1a9fq5Zdf1rFjx5rtbwQAMBtn+gAAxrvvvvsUHx+vbdu2adu2bXrhhRf0wgsvaN26dRdctlevXg3eXlFRkY4fP67Ro0d79zkcDh06dEiSdPPNNyskJESSdN111+mbb77RZ599JkkaOHCgJKl379669tpr672PoUOHSpLat2+v8PBwnTp1ylsIAQD4KSh9AACjuVwuffLJJxo7dqwGDRqkQYMGadKkSRo6dGid76O75JJLGrxNj8ejPn366JlnnvHuO3bsmDp06KC//vWvatWqlXe/w+GQZVlyOp2yLKvW7Tidznrvw9//X0/R528DAICfg5d3AgCMFhYWptzcXH388cfefSdOnJDb7VZUVJScTqeqq6t/0m326dNHRUVF2rt3ryRp06ZNGjZsmMrLy+u9Trdu3RQYGKjCwkJJ0o4dO/TZZ5/J4XBI0s/KAQDAj8GZPgCA0bp06aLnnntOixcv1ldffaWgoCCFhoYqKytLXbt21ZAhQ5SamqolS5bUef0nnnhCU6dO9W6npKRo8uTJmj17tiZNmiTLsuTv76/c3FwFBwfXm8Pf319LlizRzJkzlZOTo6uvvlrh4eHes4IN5QAA4OdyWLxeBAAAWyxYsEBjxoxReHi4jh07puHDh2vDhg1q06ZNS0cDABiMM30AANjkyiuv1OjRo+Xv7y/LsjRnzhwKHwCg2XGmDwAAAAAMxge5AAAAAIDBKH0AAAAAYDAjSp/L5WrpCAAAAADQYn6oExnzQS4UPwAAAAC4kDGlr2fPni0d4QIlJSWKjo5u6Rj4Dmbim5iL72EmvoeZ+B5m4puYi+9hJvb4oZNgRry8EwAAAABQN0ofAAAAABiM0gcAAAAABqP0AQAAAIDBKH0AAAAAYDBKHwAAAAAYjNIHAAAAAAaj9AEAAACAwSh9AAAAAGAwSh8AAAAAGIzSBwAAAAAGo/QBAAAAgMEofQAAAABgMEofAAAAABiM0gcAAAAABqP0AQAAAIDBKH0AAAAAYDBKHwAAAAAYjNIHAAAAAAaj9AEAAACAwSh9AAAAAGAwSh8AAAAAGIzSBwAAAAAGo/QBAAAAgMEofQAAAABgMEofAAAAABiM0gcAAAAABqP0AQAAAIDBKH0AAAAAYDBKHwAAAAAYjNIHAAAAAAaj9AEAAACAwWwrfR6PRzNmzFBiYqJSU1N18ODBWusFBQUaOXKkEhIStHHjxlpr27Zt08CBA+2KCgAAAADG8LfrjjZs2KDKykqtWbNGxcXFmj9/vnJzcyVJJ06cUH5+vtavX6+KigqlpKSob9++CgwM1LFjx/Tyyy+rurrarqgAAAAAYAzbzvS5XC71799fktS9e3ft3LnTu7Zjxw716NFDgYGBCg0NVWRkpHbv3q2KigrNnDlTGRkZdsUEAAAAAKPYdqbP7XYrJCTEu+10OlVdXS1/f3+53W6FhoZ614KDg+V2uzV79mzdf//9uvzyyxu8/ZKSkmbJ3Rjl5eU+metixkx8E3PxPczE9zAT38NMfBNz8T3MpOXZVvpCQkJUVlbm3fZ4PPL3969zraysTAEBAfr444916NAhPffcc/rmm280ceJELV68uM7bj46Obt5f4GcoKSnxyVwXM2bim5iL72EmvoeZ+B5m4puYi+9hJvZwuVz1rtlW+mJjY7Vx40bdfvvtKi4uVlRUlHctJiZGzzzzjCoqKlRZWam9e/cqJiZGf/nLX7yX6du3b72FDwAAAABQN9tKX1xcnIqKipSUlCTLspSVlaXly5crMjJSgwcPVmpqqlJSUmRZliZOnKigoCC7ogEAAACAsWwrfX5+fpo9e3atfd26dfP+nJCQoISEhHqvX1RU1GzZAAAAAMBUfDk7AAAAABiM0gcAAAAABqP0AQAAAIDBKH0AAAAAYDBKHwAAAAAYjNIHAAAAAAaj9AEAAACAwSh9AAAAAGAwSh8AAAAAGIzSBwAAAAAGo/QBAAAAgMEofQAAAABgMEofAAAAABiM0gcAAAAABqP0AQAAAIDBKH0AAAAAYDBKHwAAAAAYjNIHAAAAAAaj9AEAAACAwSh9AAAAAGAwSh8AAAAAGIzSBwAAAAAGo/QBAAAAgMEofQAAAABgMEofAAAAABiM0gcAAAAABqP0AQAAAIDBKH0AAAAAYDBKHwAAAAAYjNIHAAAAAAaj9AEAAACAwSh9AAAAAGAwSh8AAAAAGIzSBwAAAAAGo/QBAAAAgMEofQAAAABgMEofAAAAABiM0gcAAAAABqP0AQAAAIDBKH0AAAAAYDBKHwAAAAAYzN+uO/J4PMrIyNCePXsUGBioOXPm6KqrrvKuFxQUaPXq1fL399f48eM1aNAgHT16VNOmTVNNTY0sy9Ls2bPVtWtXuyIDAAAAwC+ebWf6NmzYoMrKSq1Zs0aPPfaY5s+f7107ceKE8vPztXr1ar300kvKyclRZWWlnn32Wd1zzz3Kz8/XQw89pJycHLviAgAAAIARbDvT53K51L9/f0lS9+7dtXPnTu/ajh071KNHDwUGBiowMFCRkZHavXu30tPTFRoaKkmqqalRUFCQXXEBAAAAwAi2lT63262QkBDvttPpVHV1tfz9/eV2u73lTpKCg4PldrsVFhYmSdq3b58WLFig5557rt7bLykpab7wP1N5eblP5rqYMRPfxFx8DzPxPczE9zAT38RcfA8zaXm2lb6QkBCVlZV5tz0ej/z9/etcKysr85bADz/8ULNmzdLChQt/8P180dHRzZT85yspKfHJXBczZuKbmIvvYSa+h5n4Hmbim5iL72Em9nC5XPWu2faevtjYWBUWFkqSiouLFRUV5V2LiYmRy+VSRUWFzp49q7179yoqKkoffvih5s6dqxdffFE33nijXVEBAAAAwBi2nemLi4tTUVGRkpKSZFmWsrKytHz5ckVGRmrw4MFKTU1VSkqKLMvSxIkTFRQUpKysLFVVVWnKlCmSpC5dumj27Nl2RQYAAACAXzzbSp+fn98Fha1bt27enxMSEpSQkFBr/X/+539syQYAAAAApuLL2QEAAADAYJQ+AAAAADAYpQ8AAAAADEbpAwAAAACDUfoAAAAAwGCUPgAAAAAwGKUPAAAAAAxG6QMAAAAAg1H6AAAAAMBglD4AAAAAMBilDwAAAAAMRukDAAAAAINR+gAAAADAYJQ+AAAAADAYpQ8AAAAADEbpAwAAAACDUfoAAAAAwGCUPgAAAAAwmH9DFygoKNCKFStUXl4uy7LkcDj0zjvv2JENAAAAANBIDZa+1atXKy8vT+3bt7cjDwAAAACgCTVY+tq2basrr7zSjiwAAAAAgCZWb+nLycmRJFVWVmrMmDG67rrr5HA4JEmTJk2yJx0AAAAAoFHqLX1dunSp9d/zzhc/AAAAAIDvq/fTO0eMGKERI0bo008/9f48YsQIffDBB3bmAwAAAAA0Qr1n+lauXKnc3FydPn1ab7/9tiTJsixdc801toUDAAAAADROvaVv1KhRGjVqlJ5//nmNGzfOzkwAAAAAgCbS4Kd3VldXa+nSpd7tgIAAdezYUbfffrsCAgKaNRwAAAAAoHHqfU/feXv27NGBAwcUHh6uI0eOaMuWLdq8ebOmTZtmRz4AAAAAQCM0WPrOnDmjRYsWKSkpSfPmzZOfn5+efvppHT582I58AAAAAIBGaLD0nT17VqWlpZKkf/zjHzp79qyqqqpUXl7e7OEAAAAAAI3T4Hv60tLSlJCQoJCQEH377bd66qmntHz5ct1999125AMAAAAANEKDpW/QoEEaOHCgSktL1a5dOzkcDg0YMMCObAAAAACARmqw9BUVFekPf/iDKioqvPv++7//u1lDAQAAAACaRoOlb968eZo2bZo6duxoRx4AAAAAQBNqsPR16tRJt9xyix1ZAAAAAABNrMHS165dO82YMUPXXXedHA6HJCkxMbHZgwEAAAAAGq/B0hcRESFJOnnyZLOHAQAAAAA0rQZL34QJE/TBBx/o8OHDiomJUZcuXezIBQAAAABoAg2WvpycHH311Vfau3evAgIClJeXp5ycHDuyAQAAAAAaya+hC7hcLi1cuFCXXHKJRowYocOHD9uRCwAAAADQBBosfTU1NaqoqJDD4VBNTY38/Bq8CgAAAADARzT48s7f/va3GjlypEpLSxUfH6/Ro0fbEAsAAAAA0BQaLH233XabbrnlFh08eFAREREKCwv72Xfm8XiUkZGhPXv2KDAwUHPmzNFVV13lXS8oKNDq1avl7++v8ePHa9CgQSotLdXjjz+u8vJydejQQfPmzVPr1q1/dgYAAAAAuJjUW/omTZrk/V6+78vOzv5Zd7ZhwwZVVlZqzZo1Ki4u1vz585WbmytJOnHihPLz87V+/XpVVFQoJSVFffv21bJlyzR06FCNHDlSeXl5WrNmDWcbAQAAAOBHqrf0JSUlNfmduVwu9e/fX5LUvXt37dy507u2Y8cO9ejRQ4GBgQoMDFRkZKR2794tl8ulhx56SJI0YMAA5eTk1Fn6SkpKmjxvY5WXl/tkrosZM/FNzMX3MBPfw0x8DzPxTczF9zCTlldv6bv55pub/M7cbrdCQkK8206nU9XV1fL395fb7VZoaKh3LTg4WG63u9b+4OBgnT17ts7bjo6ObvK8jVVSUuKTuS5mzMQ3MRffw0x8DzPxPczENzEX38NM7OFyuepds/WjOENCQlRWVubd9ng88vf3r3OtrKxMoaGhtfaXlZWpTZs2dkYGAAAAgF80W0tfbGysCgsLJUnFxcWKioryrsXExMjlcqmiokJnz57V3r17FRUVpdjYWG3atEmSVFhYqJ49e9oZGQAAAAB+0Rr89M7U1NRaH+gSEBCgjh07avz48YqIiPhJdxYXF6eioiIlJSXJsixlZWVp+fLlioyM1ODBg5WamqqUlBRZlqWJEycqKChI48ePV3p6ugoKCtS2bduf/SEyAAAAAHAxarD0RUREKDY2Vj179lRxcbE2btyo7t2768knn9SKFSt+0p35+flp9uzZtfZ169bN+3NCQoISEhJqrYeHh+ull176SfcDAAAAAPinBl/eefToUcXHx6tr164aOXKk3G634uPjVVNTY0c+AAAAAEAjNFj6qqqq9P7778vtdquwsFDV1dX68ssvde7cOTvyAQAAAAAaocHSN3/+fK1Zs0bx8fFav369srKyVFxcrKlTp9qRDwAAAADQCA2+py8yMlJLly6tta9z587NFggAAAAA0HQaLH3PP/+8XnzxRbVq1cq7b/Pmzc0aCgAAAADQNBosfX/+85/1/vvvq3Xr1nbkAQAAAAA0oQbf03fllVfWOssHAAAAAPjlaPBMX1VVle644w5FRUVJkhwOB1+QDgAAAAC/EA2WvgceeMCOHAAAAACAZlBv6du4caMGDRqkffv2yeFw1Fq7+eabmz0YAAAAAKDx6i19p0+fliSdPHnStjAAAAAAgKZVb+kbMWKEJGn//v28hw8AAAAAfqEa/PTOqqoq7d69WxUVFaqsrFRlZaUduQAAAAAATaDBD3LZv3+//vM//9O77XA49M477zRrKAAAAABA02iw9M2dO1cxMTHe7Y8++qhZAwEAAAAAmk69pe/jjz/WF198oT/84Q+67777JEkej0crV67U//7v/9oWEAAAAADw89Vb+tq0aaOTJ0+qsrJSJ06ckPTPl3ZOnjzZtnAAAAAAgMapt/RFRUUpKipK8fHxuvzyy3XmzBn5+fkpJCTEznwAAAAAgEao99M7d+3apTvvvFNhYWF6++23NWTIEN11111699137cwHAAAAAGiEekvf4sWLNX/+fAUEBOiZZ57RCy+8oPXr1ysvL8/OfAAAAACARqj35Z2WZenf/u3f9PXXX+vcuXO6/vrrJUl+fg1+tR8AAAAAwEfU2+A8Ho8k6f3331efPn0kSZWVlSorK7MnGQAAAACg0eo909enTx8lJSXpq6++Um5urg4dOqSMjAzdfvvtduYDAAAAADRCvaXvwQcf1ODBgxUWFqa2bdvq0KFDSk5OVlxcnJ35AAAAAACNUG/pk6Ru3bp5f46MjFRkZGSzBwIAAAAANB0+lQUAAAAADEbpAwAAAACDUfoAAAAAwGCUPgAAAAAwGKUPAAAAAAxG6QMAAAAAg1H6AAAAAMBglD4AAAAAMBilDwAAAAAMRukDAAAAAINR+gAAAADAYJQ+AAAAADAYpQ8AAAAADEbpAwAAAACDUfoAAAAAwGCUPgAAAAAwmL9dd1ReXq7Jkyfr1KlTCg4O1oIFCxQWFlbrMkuXLtV7770nf39/TZs2TTExMSopKVFmZqacTqcCAwO1YMEChYeH2xUbAAAAAH7RbDvTt2rVKkVFRenVV1/VnXfeqWXLltVa37Vrl7Zu3aq1a9cqJydHs2bNkiTNnTtX06dPV35+vuLi4vTCCy/YFRkAAAAAfvFsK30ul0v9+/eXJA0YMEBbtmy5YL1fv35yOBy64oorVFNTo9LSUuXk5Cg6OlqSVFNTo6CgILsiAwAAAMAvXrO8vHPt2rVasWJFrX3t2rVTaGioJCk4OFhnz56tte52u3XZZZd5t89f5qqrrpIkbd++Xa+88opWrlxZ532WlJQ05a/QJMrLy30y18WMmfgm5uJ7mInvYSa+h5n4Jubie5hJy2uW0hcfH6/4+Pha+yZMmKCysjJJUllZmdq0aVNrPSQkxLt+/jLnS+Kbb76p3Nxc5eXlXfA+wPPOnw30JSUlJT6Z62LGTHwTc/E9zMT3MBPfw0x8E3PxPczEHi6Xq941217eGRsbq02bNkmSCgsL1bNnzwvWN2/eLI/Ho6NHj8rj8SgsLExvvPGGXnnlFeXn56tz5852xQUAAAAAI9j26Z3JyclKT09XcnKyAgIClJ2dLUlauHChhgwZopiYGPXq1UuJiYnyeDyaMWOGampqNHfuXHXq1ElpaWmSpJtuukkPP/ywXbEBAAAA4BfNttLXunVr/e53v7tg/xNPPOH9OS0tzVvuztu6dWuzZwMAAAAAU/Hl7AAAAABgMEofAAAAABiM0gcAAAAABqP0AQAAAIDBKH0AAAAAYDBKHwAAAAAYjNIHAAAAAAaj9AEAAACAwSh9AAAAAGAwSh8AAAAAGIzSBwAAAAAGo/QBAAAAgMEofQAAAABgMEofAAAAABiM0gcAAAAABqP0AQAAAIDBKH0AAAAAYDBKHwAAAAAYjNIHAAAAAAaj9AEAAACAwSh9AAAAAGAwSh8AAAAAGIzSBwAAAAAGo/QBAAAAgMEofQAAAABgMEofAAAAABiM0gcAAAAABqP0AQAAAIDBKH0AAAAAYDBKHwAAAAAYjNIHAAAAAAaj9AEAAACAwSh9AAAAAGAwSh8AAAAAGIzSBwAAAAAGo/QBAAAAgMEofQAAAABgMEofAAAAABiM0gcAAAAABqP0AQAAAIDBKH0AAAAAYDDbSl95ebnS0tKUkpKiBx54QKWlpRdcZunSpbr77ruVlJSkHTt21Fr705/+pMTERLviAgAAAIARbCt9q1atUlRUlF599VXdeeedWrZsWa31Xbt2aevWrVq7dq1ycnI0a9Ys71pJSYnWrVsny7LsigsAAAAARrCt9LlcLvXv31+SNGDAAG3ZsuWC9X79+snhcOiKK65QTU2NSktL9Y9//EOLFi3StGnT7IoKAAAAAMbwb44bXbt2rVasWFFrX7t27RQaGipJCg4O1tmzZ2utu91uXXbZZd7t4OBgnT592lv4goKCfvA+S0pKmih90ykvL/fJXBczZuKbmIvvYSa+h5n4Hmbim5iL72EmLa9ZSl98fLzi4+Nr7ZswYYLKysokSWVlZWrTpk2t9ZCQEO/6+cu43W4dPHhQGRkZqqio0BdffKG5c+fqySefvOA+o6Ojm+E3aZySkhKfzHUxYya+ibn4Hmbie5iJ72Emvom5+B5mYg+Xy1Xvmm0v74yNjdWmTZskSYWFherZs+cF65s3b5bH49HRo0fl8XgUExOj//u//1N+fr5ycnJ0zTXX1Fn4AAAAAAB1a5YzfXVJTk5Wenq6kpOTFRAQoOzsbEnSwoULNWTIEMXExKhXr15KTEyUx+PRjBkz7IoGAAAAAMayrfS1bt1av/vd7y7Y/8QTT3h/TktLU1paWp3Xj4iIUEFBQbPlAwAAAAAT8eXsAAAAAGAwSh8AAAAAGIzSBwAAAAAGo/QBAAAAgMEofQAAAABgMEofAAAAABiM0gcAAAAABqP0AQAAAIDBKH0AAAAAYDBKHwAAAAAYjNIHAAAAAAaj9AEAAACAwSh9AAAAAGAwSh8AAAAAGIzSBwAAAAAGo/QBAAAAgMEofQAAAABgMEofAAAAABiM0gcAAAAABqP0AQAAAIDBKH0AAAAAYDBKHwAAAAAYjNIHAAAAAAaj9AEAAACAwSh9AAAAAGAwSh8AAAAAGIzSBwAAAAAGo/QBAAAAgMEofQAAAABgMEofAAAAABiM0gcAAAAABqP0AQAAAIDBHJZlWS0dorFcLldLRwAAAACAFtWzZ8869xtR+gAAAAAAdePlnQAAAABgMEofAAAAABiM0gcAAAAABvNv6QAm8ng8ysjI0J49exQYGKg5c+boqquuaulYF4WqqipNmzZNR44cUWVlpcaPH6+OHTtq3LhxuvrqqyVJycnJuv3227V06VK999578vf317Rp0xQTE9Oy4Q135513KjQ0VJIUERGhxMREzZ07V06nU/369dOECRM4dmz02muv6fXXX5ckVVRUqKSkRNnZ2Vq4cKE6deokSUpLS1OvXr2YiQ3+9re/adGiRcrPz9fBgwc1ZcoUORwOXXvttZo5c6b8/PzqfMyq77JovO/OpKSkRJmZmXI6nQoMDNSCBQsUHh6uOXPmaPv27QoODpYkLVu2TFVVVXr88cdVXl6uDh06aN68eWrdunUL/zbm+O5cdu3a9aOf3zlWms93ZzJx4kSdPHlSknTkyBH96le/0uLFizVu3DidPn1aAQEBCgoK0osvvshM7Gahyf3lL3+x0tPTLcuyrE8++cQaN25cCye6eKxbt86aM2eOZVmWVVpaag0cONAqKCiwXnrppVqX27lzp5Wammp5PB7ryJEj1siRI1si7kWjvLzcGj58eK19w4YNsw4ePGh5PB5r7Nix1s6dOzl2WkhGRoa1evVqKycnx3rrrbdqrTGT5peXl2cNHTrUio+PtyzLsh566CHrww8/tCzLsqZPn269/fbb9T5m1XVZNN73ZzJq1Cjr73//u2VZlrVq1SorKyvLsizLSkpKsk6dOlXrupmZmdb69esty7Ks3//+99by5cvtC26478/lpzy/c6w0j+/P5LzTp09bw4YNs77++mvLsizrtttuszweT63LMBN7UaebgcvlUv/+/SVJ3bt3186dO1s40cVjyJAheuSRR7zbTqdTO3fu1HvvvadRo0Zp2rRpcrvdcrlc6tevnxwOh6644grV1NSotLS0BZObbffu3Tp37pzuv/9+3Xvvvdq2bZsqKysVGRkph8Ohfv36acuWLRw7LeDTTz/VF198ocTERO3atUvr169XSkqK5s+fr+rqamZig8jISC1ZssS7vWvXLt18882SpAEDBuiDDz6o9zGrrsui8b4/k5ycHEVHR0uSampqFBQUJI/Ho4MHD2rGjBlKSkrSunXrJNX+fwBm0rS+P5ef8vzOsdI8vj+T85YsWaJ77rlHHTp00MmTJ3XmzBmNGzdOycnJ2rhxo6S6H+vQfHh5ZzNwu90KCQnxbjudTlVXV8vfnz93czv/Ehu3262HH35Yjz76qCorKxUfH68bbrhBubm5eu655xQaGqrLLrus1vXOnj2rsLCwloputFatWmnMmDGKj4/XgQMH9MADD6hNmzbe9eDgYH355ZccOy3g97//vf7rv/5LktS3b1/deuutioiI0MyZM7V69WpmYoPf/OY3Onz4sHfbsiw5HA5J/3pscrvddT5m1XVZNN73Z9KhQwdJ0vbt2/XKK69o5cqV+vbbb3XPPffovvvuU01Nje69917dcMMNcrvd3peyM5Om9f25xMTE/Ojnd46V5vH9mUjSqVOntGXLFk2dOlXSP996c/4ffb/55hslJycrJiaGmdiMM33NICQkRGVlZd5tj8fD/yDZ6NixY7r33ns1fPhw3XHHHYqLixUGulwAAAdISURBVNMNN9wgSYqLi9Pf//73C2ZUVlbmfZJG0+vSpYuGDRsmh8OhLl26KDQ0VKdPn/aul5WVqU2bNhw7Njtz5oz27dun3r17S5Luuusude7cWQ6HQ4MHD67zWGEmze+772mp79g4/5hV12XRPN58803NnDlTeXl5CgsLU+vWrXXvvfeqdevWCgkJUe/evbV79+5as2ImzeunPL9zrNjnrbfe0tChQ+V0OiVJ4eHhSkpKkr+/v9q1a6fo6Gjt37+fmdiM0tcMYmNjVVhYKEkqLi5WVFRUCye6eJw8eVL333+/Jk+erLvvvluSNGbMGO3YsUOStGXLFl1//fWKjY3V5s2b5fF4dPToUXk8Hs7yNaN169Zp/vz5kqSvv/5a586d0yWXXKJDhw7Jsixt3rxZvXr14tix2bZt23TLLbdI+ufZpWHDhumrr76SVPtYYSb2uu666/TRRx9JkgoLC73HRl2PWXVdFk3vjTfe0CuvvKL8/Hx17txZknTgwAGlpKSopqZGVVVV2r59u/eY2bRpk6R/zqRnz54tGd1oP+X5nWPFPlu2bNGAAQO82x988IEeffRRSf8sd59//rm6du3KTGzGP9c2g7i4OBUVFSkpKUmWZSkrK6ulI100nn/+eZ05c0bLli3TsmXLJElTpkxRVlaWAgICFB4erszMTIWEhKhXr15KTEyUx+PRjBkzWji52e6++25NnTpVycnJcjgcysrKkp+fnx5//HHV1NSoX79++tWvfqUbb7yRY8dG+/fvV0REhCTJ4XBozpw5mjBhglq1aqVu3bopISFBTqeTmdgsPT1d06dPV05Ojrp27arf/OY3cjqddT5m1XVZNK2amhrNnTtXnTp1UlpamiTppptu0sMPP6w77rhDCQkJCggI0PDhw3Xttddq/PjxSk9PV0FBgdq2bavs7OwW/g3MlZGRoczMzB/1/M6xYp/9+/d7/3FEkgYOHKjNmzcrISFBfn5+mjRpksLCwpiJzRyWZVktHQIAAAAA0Dx4eScAAAAAGIzSBwAAAAAGo/QBAAAAgMEofQAAAABgMEofAAAAABiM0gcAMMpHH32kPn36KDU1VampqRo5cqQefvhhVVZWNuv9rlq1SkuWLKm1r7CwUGvWrPlR19+7d69SU1ObIxoA4CLH9/QBAIzTu3dvLV682Lv92GOP6d1339WQIUNszfHdLygGAKClUPoAAEarrKzU8ePHdemll0qSsrOztW3bNlmWpdGjR+u2227T3/72N82dO1eWZenyyy/XokWLtG/fPmVmZsrpdCooKEiZmZnyeDwaP368LrvsMg0YMEA9evRQVlaWLr30Uvn5+al79+617vu1117Tvn37lJSUpMcee0wdO3bUl19+qRtvvFGzZs3S8ePH9fjjj8uyLLVv3957va1bt2rx4sVyOp3q3LmzZs+erYKCAm3fvl3Z2dlKT09XTEyMRo0aZevfEgDwy0TpAwAY58MPP1RqaqpOnTolPz8/JSQkqE+fPtq0aZMOHz6s1atXq6KiQgkJCerbt6+mT5+uxYsXq1u3blq5cqX27t2r6dOna+7cuYqOjtaGDRs0f/58PfHEEzpx4oTWr1+vwMBA3XXXXcrOzlaXLl00c+bMH8x04MABvfTSS2rdurVuvfVWnThxQsuXL9fQoUOVkJCgN998U6tWrZJlWZo+fbpeffVVtWvXTs8884xef/11jRo1SkVFRZoyZYqqqqoofACAH4339AEAjNO7d2/l5+dr5cqVCggIUEREhCTps88+065du5SamqqxY8equrpaR48e1alTp9StWzdJ0qhRo3T99dfr+PHjio6OliTddNNN+vzzzyVJERERCgwMlCR9/fXX6tKliyQpNjb2BzNFRkYqJCRETqdT7du3V0VFhT7//HPFxMTUun5paamOHz+uRx99VKmpqSoqKtLRo0clSQ8++KBef/11jRkzpin/XAAAw1H6AADGatu2rZ5++mk99dRTOn78uLp27apf//rXys/P14oVK3TbbbcpIiJCHTp00IEDByRJeXl5+utf/6oOHTpo9+7dkqRt27bp6quvliT5+f3rqbN9+/bau3evJOnTTz/9wSwOh+OCfV27dtUnn3xS6/pt27ZVx44dtWzZMuXn52vcuHH69a9/rcrKSmVlZWn27NnKyMho9g+mAQCYg5d3AgCMds011yg1NVVz5szRs88+q61btyolJUXffvutbr31VoWEhGjWrFmaNm2a/Pz81L59e40ePVpXXnmlMjMzZVmWnE6nsrKyLrjtp59+Wunp6QoODlZwcLD3fYM/1iOPPKKJEyfqzTff9J6N9PPz05NPPqkHH3xQlmUpODhYCxcu1KJFi/Tv//7vSkxM1PHjx5Wdna2pU6c2yd8IAGA2h2VZVkuHAAAAAAA0D17eCQAAAAAGo/QBAAAAgMEofQAAAABgMEofAAAAABiM0gcAAAAABqP0AQAAAIDBKH0AAAAAYLD/Bztx34wFVlxTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(range(0,len(data_length),1),data_length)\n",
    "plt.xlabel(\"Record index\")\n",
    "plt.ylabel(\"String length\")\n",
    "plt.title(\"StrLength\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "from numpy.linalg import svd\n",
    "\n",
    "def PCA_SpaceVisualization(X,title='PCA plot'):\n",
    "    '''\n",
    "    PCA to given array X and creating a plot\n",
    "    Returns PCA components array after fit_transform\n",
    "    '''\n",
    "    \n",
    "    # PCA code\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X)\n",
    "    pcaComponents = pca.fit_transform(X) # pcaComponents is the data that I'll use from PCA\n",
    "    \n",
    "    # Seperating components\n",
    "    first_component = [x[0] for x in pcaComponents]\n",
    "    second_component = [x[1] for x in pcaComponents]\n",
    "    \n",
    "    # Plotting code\n",
    "    fig, ax = plt.subplots(figsize=(25,10))\n",
    "    ax.scatter(first_component, second_component,alpha=0) \n",
    "    fig.suptitle(title,fontsize=40,fontweight='bold')\n",
    "    ax.set_xlabel('X Component',fontsize=30,fontweight='bold')\n",
    "    ax.set_ylabel('Y Component',fontsize=30,fontweight='bold')\n",
    "    \n",
    "    for x0, y0, i in zip(first_component, second_component,range(0,len(first_component),1)):\n",
    "        plt.text(x0,y0,i, ha=\"center\", va=\"center\",fontsize=20,color='b')\n",
    "        \n",
    "    return pcaComponents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarityProbsHeatMap(similarityProb_matrix,clusters,title):\n",
    "    fif,ax = plt.subplots(1,figsize=(20,20))\n",
    "    colors = ['red','green','blue','yellow','orange']\n",
    "    c=0\n",
    "    for cl in clusters:\n",
    "        for i  in range(0,len(cl)):\n",
    "            for j in range(i+1,len(cl)):\n",
    "                ax.add_patch(Rectangle((cl[j],cl[i]), 1, 1, fill=False, edgecolor=colors[c], lw=3))\n",
    "        c+=1\n",
    "#     corr = np.corrcoef(ed_matrix)\n",
    "#     mask = np.zeros_like(corr)\n",
    "#     mask[np.tril_indices_from(mask)] = True\n",
    "    ax.set_title(title,fontsize=20,fontweight='bold')\n",
    "    ax = sns.heatmap(similarityProb_matrix, linewidth=0.5,annot=True,cmap=\"Blues\",fmt='.3g',ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####################################################################\n",
      "#     .~ RankedWTAHash with Vantage embeddings starts training ~.   #\n",
      "#####################################################################\n",
      "\n",
      "###########################################################\n",
      "# > 1. Prototype selection phase                          #\n",
      "###########################################################\n",
      "\n",
      "\n",
      "-> Finding prototypes and representatives of each cluster:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffcda2b9be449d78cb1fa0ea878e65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1879.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-3e01533cf3b4>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n-> Finding prototypes and representatives of each cluster:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mprototypes_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprototypeArray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselected_numOfPrototypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClustering_Prototypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mS_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_numberOf_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_editDistance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairDictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n- Prototypes selected\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddingDim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprototypeArray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-3e01533cf3b4>\u001b[0m in \u001b[0;36mClustering_Prototypes\u001b[1;34m(self, S, k, d, pairDictionary, verbose)\u001b[0m\n\u001b[0;32m    174\u001b[0m                   \u001b[0mClusters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                   \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m               \u001b[1;32melif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEditDistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# case empty second representative\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m                   \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m                                             \u001b[1;31m# and ED of representative 1  smaller than i-th string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                   \u001b[0mC\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-3e01533cf3b4>\u001b[0m in \u001b[0;36mEditDistance\u001b[1;34m(self, str1, str2, verbose)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mdistance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meditdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mS_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mS_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistanceMetric\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'jaccard'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0mdistance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjaccard_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mS_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mS_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Available metrics for space creation: edit, jaccard \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\metrics\\distance.py\u001b[0m in \u001b[0;36mjaccard_distance\u001b[1;34m(label1, label2)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \"\"\"\n\u001b[0;32m    196\u001b[0m     return (len(label1.union(label2)) - len(label1.intersection(label2))) / len(\n\u001b[1;32m--> 197\u001b[1;33m         \u001b[0mlabel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = RankedWTAHash(\n",
    "    max_numberOf_clusters= 300,\n",
    "    max_editDistance= 0.001,\n",
    "    windowSize= 100,\n",
    "    similarityThreshold= 0.65,\n",
    "    metric='kendal',\n",
    "    similarityVectors='ranked',\n",
    "    number_of_permutations = 7,\n",
    "    distanceMetric= 'jaccard',\n",
    "    distanceMetricEmbedding = 'l_inf',\n",
    "    ngramms= 3,\n",
    "    jaccard_withchars = True\n",
    ")\n",
    "model = model.fit(data)\n",
    "evaluate_cora(model.mapping_matrix,true_matrix, False)\n",
    "# similarityProbsHeatMap(model.similarityProb_matrix,clusters,'HeatMap based on similarities after WTA phase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarityProbsHeatMap(model.similarityProb_matrix,[],'HeatMap based on similarities after WTA phase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RankedWTAHash' object has no attribute 'Embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-70f1144da309>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpcaComponents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA_SpaceVisualization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'RankedWTAHash' object has no attribute 'Embeddings'"
     ]
    }
   ],
   "source": [
    "pcaComponents = PCA_SpaceVisualization(model.Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaComponents = PCA_SpaceVisualization(model.rankedVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_numberOf_clusters= [5,10,20]\n",
    "max_editDistance= [50,100,200,300]\n",
    "windowSize= [3, 5, 10, 15]\n",
    "similarityThreshold= [0.6,0.7,0.8]\n",
    "metric= ['kendal', 'customKendal','jaccard','pearson']\n",
    "similarityVectors= ['ranked','initial']\n",
    "distanceMetric= ['edit']\n",
    "distanceMetricEmbedding = ['edit','euclidean']\n",
    "number_of_permutations = [1,3,5,7]\n",
    "\n",
    "\n",
    "# results = GridSearch_cora(data,true_matrix,max_numberOf_clusters,max_editDistance,similarityThreshold,windowSize,metric,similarityVectors,distanceMetricEmbedding,distanceMetric,number_of_permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by=['Accuracy'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# References\n",
    "\n",
    "1.   [The dissimilarity representation for pattern recognition, a tutorial\n",
    "Robert P.W. Duin and Elzbieta Pekalska Delft University of Technology, The Netherlands School of Computer Science, University of Manchester, United Kingdom](http://homepage.tudelft.nl/a9p19/presentations/DisRep_Tutorial_doc.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
