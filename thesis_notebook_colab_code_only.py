# -*- coding: utf-8 -*-
"""Thesis_Notebook_Colab-Code_Only.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12TgM1vJEaQE5sQ1rN-ogExjRoq9fG0T4

<p align="center">
 <img src="http://www.di.uoa.gr/themes/corporate_lite/logo_en.png" title="Department of Informatics and Telecommunications - University of Athens"/> </p>

<br>

---

<h3 align="center" > 
  Bachelor Thesis
</h3>

<h1 align="center" > 
  Entity Resolution in Dissimilarity Spaces <br>
  Implementation notebook
</h1>

---

<h3 align="center"> 
 <b>Konstantinos Nikoletos</b>
</h3>

<h4 align="center"> 
 <b>Supervisor: Dr. Alex Delis</b>,  Professor NKUA
</h4>
<br>
<h4 align="center"> 
Athens
</h4>
<h4 align="center"> 
January 2021 - Ongoing
</h4>


---

|  <font size="5"> Contents</font> |
| :--   |
|**1. [Abstract](#Abstract)** |
|**2. [Introduction](#Introduction)**  |
&nbsp;&nbsp;&nbsp;**2.1. [   Entity resolution](#Entity-resolution)** |
&nbsp;&nbsp;&nbsp;**2.2. [   Dissimilatiry space](#Dissimilatiry-Space)** |
|**3. [ A dissimilarity-based space embedding methodology](#scrollTo=DcAYuFQjY2ni)** <br>
&nbsp;&nbsp;&nbsp;**3.1 [String Clustering and Prototype Selection](#3.1-String-Clustering-and-Prototype-Selection)** <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3.1.1. [Edit distance metric](#Edit-distance-metric)** <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3.1.2. [String clustering algorithm](#String-clustering-algorithm)** <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3.1.3. [Algorithm complexity](#Algorithm-complexity)** <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3.1.4. [Prototype selection](#Prototype-selection)** <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3.1.5. [Algorithm-1: The String Clustering and Prototype Selection Algorithm](#Algorithm-1:-The-String-Clustering-and-Prototype-Selection-Algorithm)** <br>
&nbsp;&nbsp;&nbsp;**3.2 [The Vantage Space Embedding and the Chorus of Prototypes Transform Similarity Coefficient](#3.2-The-Vantage-Space-Embedding-and-the-Chorus-of-Prototypes-Transform-Similarity-Coefficient)&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;**  <br>
&nbsp;&nbsp;&nbsp;**3.3 [A Top-k List Approach for Similarity Searching in the Vantage Space](#3.3-A-Top-k-List-Approach-for-Similarity-Searching-in-the-Vantage-Space)**  |
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3.3.1. [Abstract Algebra definitions](#Abstract-Algebra-definitions)** <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3.3.2. [Hausdorff metric](#Hausdorff-metric)** <br>
&nbsp;&nbsp;&nbsp;**3.4 [Hashing of Partially Ranked Data for Efficient Similarity Search](#3.4-Hashing-of-Partially-Ranked-Data-for-Efficient-Similarity-Search)** |
|**4. [ Evaluation](#Evaluation)** |
|**5. [References](#References)**  |

# __Implementation__

## __0. Libraries__
"""

import pandas as pd
import numpy as np
import collections
from scipy import stats 
import editdistance
import string
import sklearn

!pip install editdistance

"""## __1. Prototype selection algorithm__"""

#####################################################################
# 1. Prototype selection algorithm                                  #
#####################################################################

'''
Clustering_Prototypes(S,k,d,r,C) 
The String Clustering and Prototype Selection Algorithm
is the main clustering method, that takes as input the intial strings S, 
the max number of clusters to be generated in k,
the maximum allowable distance of a string to join a cluster in var d
and returns the prototype for each cluster in array Prototype
'''
def Clustering_Prototypes(S,k,d,pairDictionary,verbose=False):
    
    # ----------------- Initialization phase ----------------- #
    i = 0
    j = 0
    C = np.empty([S.size], dtype=int)
    r = np.empty([2,k],dtype=object)

    Clusters = [ [] for l in range(0,k)]

    while i < S.size:     # String-clustering phase, for all strings
        while j < k :       # iteration through clusters, for all clusters
            if r[0][j] == None:      # case empty first representative for cluster j
                r[0][j] = S[i]   # init cluster representative with string i
                C[i] = j         # store in C that i-string belongs to cluster j
                Clusters[j].append(S[i])
                break
            elif r[1][j] == None and (EditDistance(S[i],r[0][j]) <= d):  # case empty second representative 
                r[1][j] = S[i]                                             # and ED of representative 1  smaller than i-th string 
                C[i] = j
                Clusters[j].append(S[i])
                break
            elif (r[0][j] != None and r[1][j] != None) and (EditDistance(S[i],r[0][j]) + EditDistance(S[i],r[1][j])) <= d:
                C[i] = j
                Clusters[j].append(S[i])
                break
            else:
                j += 1
        i += 1
    
    # ----------------- Prototype selection phase ----------------- #
        
    Projections = np.empty([k],dtype=object)
    Prototypes = np.empty([k],dtype=int)
    sortedProjections = np.empty([k],dtype=object)
    j = 0

    if verbose:
        print("- - - - - - - - -")
        print("Cluster array:")
        print(C)
        print("- - - - - - - - -")
        print("Represantatives array:")
        print(r)
        print("- - - - - - - - -")  
        print("Clusters:")
        print(Clusters)
        print("- - - - - - - - -")  

    # print("\n\n\n****** Prototype selection phase *********") 
    while j < k and len(Clusters[j])>0:
        
        Projections[j] = Approximated_Projection_Distances_ofCluster(r[1][j], r[0][j], j, Clusters[j],pairDictionary)
        
        # print("\n"+str(j)+"-Projections:")
        # print(Projections[j])
        
        sortedProjections[j] = {k: v for k, v in sorted(Projections[j].items(), key=lambda item: item[1])}

        # print(str(j)+"-sortedProjections:")
        # print(sortedProjections[j])
        
        Prototypes[j] = Median(sortedProjections[j])
        
        # print(".............")
        # print(str(j)+"-Prototypes:")        
        # print(Prototypes[j])
        
        j += 1
    # print("\n****** END *********\n")

    return Prototypes


def Approximated_Projection_Distances_ofCluster(right_rep, left_rep, cluster_id, clusterSet, pairDictionary):

    distances_vector = dict()
    rep_distance     = EditDistance(right_rep,left_rep)

    for str_inCluster in range(0,len(clusterSet)): 

      right_rep_distance = EditDistance(right_rep,clusterSet[str_inCluster])
      left_rep_distance  = EditDistance(left_rep,clusterSet[str_inCluster])
      
      distance = (right_rep_distance**2-rep_distance**2-left_rep_distance**2 ) / (2*rep_distance)
      distances_vector[clusterSet[str_inCluster]] = distance

    return distances_vector

def Median(distances):    
    '''
    Returns the median value of a vector
    '''
    keys = list(distances.keys())
    median_position = int(len(keys)/2)
    median_value = keys[median_position]

    return median_value

# pairDictionary = dict()
# input_strings = list(strObjects)
# k = 7 # max_number_of_clusters
# d = 120
# S_set = np.array(input_strings,dtype=object)
# S_index = np.arange(0,len(input_strings),1)
# # S_index_list = list(S_index) 


# # print(S_set)

# print("\n-----------------\nString positions are:")
# print(S_index)
# print("-----------------\n")

# Prototypes = Clustering_Prototypes(S_index,k,d)

# print("\n-----------------\n NumofPrototypes: "+str(k)+" || Prototypes are: ")
# print(Prototypes)
# print("-----------------")

"""## __2. Embeddings based on the Vantage objects__



"""

#####################################################################
#       2. Embeddings based on the Vantage objects                  #
#####################################################################

'''
CreateVantageEmbeddings(S,VantageObjects): Main function for creating the string embeddings based on the Vantage Objects
'''
def CreateVantageEmbeddings(S,VantageObjects, pairDictionary):
    
    # ------- Distance computing ------- #     
    vectors = []
    for s in range(0,S.size):
        string_embedding = []
        for p in range(0,VantageObjects.size): 
            if VantageObjects[p] != None:
                string_embedding.append(DistanceMetric(s,p,S,VantageObjects, pairDictionary))
            
        # --- Ranking representation ---- #
        ranked_string_embedding = stats.rankdata(string_embedding, method='dense')
        
        # ------- Vectors dataset ------- #
        vectors.append(ranked_string_embedding)
    
    return np.array(vectors)
    

'''
DistanceMetric(s,p,S,Prototypes): Implementation of equation (5)
'''
def DistanceMetric(s,p,S,VantageObjects, pairDictionary):
    
    max_distance = None
    
    for pp in range(0,VantageObjects.size):
        if VantageObjects[pp] != None:
            string_distance = EditDistance(S[s],VantageObjects[pp])    # Edit distance String-i -> Vantage Object
            VO_distance     = EditDistance(VantageObjects[p],VantageObjects[pp])    # Edit distance Vantage Object-j -> Vantage Object-i

            abs_diff = abs(string_distance-VO_distance)

            # --- Max distance diff --- #        
            if max_distance == None:
                max_distance = abs_diff
            elif abs_diff > max_distance:
                max_distance = abs_diff
            
    return max_distance

def dropNone(array):
    array = list(filter(None, list(array)))
    return np.array(array)

def topKPrototypes():
    return

"""## __3. Metrics and Similarity functions__"""

#####################################################################
#                 3. Similarity function                            # 
#####################################################################
from scipy.spatial.distance import directed_hausdorff
from scipy.spatial.distance import hamming
from scipy.stats._stats import _kendall_dis

def SimilarityEvaluation(buckets,vectors,threshold, maxOnly=None, metric=None):

  numOfVectors = vectors.shape[0]
  vectorDim    = vectors.shape[1]
  mapping = {}

  for v_index in range(0,numOfVectors,1):
    
    for i_index in range(v_index+1,numOfVectors,1):
      if metric == None or metric == 'kendal': 
        tau, p_value = stats.kendalltau(vectors[v_index], vectors[i_index])
      else:
        numOf_discordant_pairs = _kendall_dis(vectors[v_index], vectors[i_index])
        tau = float((2*numOf_discordant_pairs) / ((vectorDim)*(vectorDim-1)))
      
      # print(tau,numOf_discordant_pairs,vectorDim)
      
      if tau > threshold or maxOnly:
        if not maxOnly:
          if v_index not in mapping.keys():
            mapping[v_index] = []
          mapping[v_index].append(i_index)
        else:
          if v_index not in mapping.keys():  
            mapping[v_index] = (i_index,tau)
          else:
            if mapping[v_index][1] < tau:
              mapping[v_index] = (i_index,tau)
 
  return mapping

"""## __4. Hashing__"""

#####################################################################
#                        4. Hashing                                 # 
#####################################################################

def WTA(vectors,K,inputDim):
  '''
    Winner Take All hash - Yagnik
    .............................

    m: number of permutations
    K: window size
  '''
  newVectors = []
  buckets = dict()

  numOfVectors = vectors.shape[0]
  vectorDim    = vectors.shape[1]

  C = np.zeros([numOfVectors], dtype=int)
  theta = np.random.permutation(inputDim)
  i=0;j=0;

  for v_index in range(0,numOfVectors,1):
    X_new = permuted(vectors[v_index],theta)
    # print( np.array(X_new[:K]))
    newVectors.append(X_new[:K])
    index_max = max(range(len(X_new)), key=X_new.__getitem__)
    c_i = index_max

    j=0
    for j in range(0,K):
      if X_new[j] > X_new[c_i]:
        c_i = j

    C[i] = c_i
    buckets = bucketInsert(buckets,c_i,i)
    i+=1
  
  return C,buckets,np.array(newVectors)

def permuted(vector,permutation):
  permuted_vector = [vector[x] for x in permutation]
  return permuted_vector 

def bucketInsert(buckets,bucket_id,item):
  if bucket_id not in buckets.keys():
    buckets[bucket_id] = []
  buckets[bucket_id].append(item)

  return buckets

"""## __Final model__








"""

class RankedWTAHash:

  def __init__(self, max_numberOf_clusters, max_editDistance, windowSize, metric = 'kendal', similarityThreshold=None, maxOnly=None ):
      '''
        Constructor
      '''
      self.max_numberOf_clusters = max_numberOf_clusters
      self.pairDictionary = dict()
      self.max_editDistance = max_editDistance
      self.windowSize = windowSize
      self.S_set = None 
      self.S_index = None 
      self.similarityThreshold = similarityThreshold
      self.maxOnly = maxOnly
      self.metric = metric
  
  def fit(self, X, y):
    """
      Fit the classifier from the training dataset.
      Parameters
      ----------
      X : Training data.
      y : Target values.
      Returns
      -------
      self : The fitted classifier.
    """
    
    input_strings = list(X)
    print(input_strings)
    self.S_set = np.array(input_strings,dtype=object)
    self.S_index = np.arange(0,len(input_strings),1)
    # S_index_list = list(S_index) 
    # self.X = S_index

    # print(S_set)

    print("\n-----------------\nString positions are:")
    print(self.S_index)
    print("-----------------\n")

    self.prototypeArray = Clustering_Prototypes(self.S_index,self.max_numberOf_clusters, self.max_editDistance, self.pairDictionary)
    self.embeddingDim   = self.prototypeArray.size
    print("\n-----------------\n NumofPrototypes: "+str(self.max_numberOf_clusters)+" || Prototypes are: ")
    print(self.prototypeArray)
    print("-----------------")

    self.Embeddings = CreateVantageEmbeddings(self.S_index,self.prototypeArray, self.pairDictionary)

    print("\n-----------------\nEmbeddings:")
    print(self.Embeddings)
    print("-----------------\n")

    self.HashedClusters,self.buckets,self.rankedVectors = WTA(self.Embeddings,self.windowSize,self.embeddingDim)

    print("\n-----------------\nWTA Buckets:")
    print(self.HashedClusters)
    print("-----------------\n")
    
    print("\n-----------------\nWTA RankedVectors after permutation:")
    print(self.rankedVectors)
    print("-----------------\n")

    print("\n-----------------\nSimilarity checking:")
    self.mapping = SimilarityEvaluation(self.buckets,self.rankedVectors,self.similarityThreshold,maxOnly=self.maxOnly, metric=self.metric)
    print(self.mapping)
    print("-----------------\n")
    
  
  def EditDistance(self, str1,str2,verbose=False):
      if verbose:
          if str1 == None:
              print("1")
          elif str2 == None:
              print("2")
          print("-> "+str(str1))
          print("--> "+str(str2))
          print(str(editdistance.eval(self.S_set[str1],self.S_set[str2])))
      
      
      # NOTE: Duplicates inside the dictionary     

      if ((str1,str2) or (str2,str1))  in self.pairDictionary.keys():
          return self.pairDictionary[(str1,str2)]
      else:
          distance = editdistance.eval(self.S_set[str1],self.S_set[str2])
          self.pairDictionary[(str2,str1)] = self.pairDictionary[(str1,str2)] = distance
          return distance

    # def DBLPACM_classification_report(self, perfectMapping):
    #   print(classification_report())


  # def predict(self, X):
  # """
  #   Predict the class labels for the provided data.
  #   Parameters
  #   ----------
  #   X : Test samples.
  #   Returns
  #   -------
  #   y : Class labels for each data sample.
  # """
  
  
  # def predict_proba(self, X):
  # """
  #   Return probability estimates for the test data X.
  #   Parameters
  #   ----------
  #   X : Test samples.
    
  #   Returns
  #   -------
  #   p : The class probabilities of the input samples.
  # """


  # # def evaluate():

"""---
---

# __Evaluation__
"""

from sklearn.metrics import classification_report

# Opening data file
import io
from google.colab import drive

drive.mount('/content/drive',force_remount=True)

"""## __CoRA__

### Load from Drive
"""

fpcites = r"/content/drive/My Drive/ERinDS/cora_cites.csv"
fppaper = r"/content/drive/My Drive/ERinDS/cora_paper.csv"
fpcontent = r"/content/drive/My Drive/ERinDS/cora_content.csv"

cites = pd.read_csv(fpcites,sep=';')
paper = pd.read_csv(fppaper,sep=';')
content = pd.read_csv(fpcontent,sep=';')

"""### Overview"""

cites

paper

content

"""### Train-Test-Validation datasets


"""

class_labels = np.unique(paper.class_label.to_numpy())
print(class_labels)
print("Number of classes: "+str(len(class_labels)))



"""## __DBLP/ACM__"""

acmfp = r"/content/drive/My Drive/ERinDS/ACM.csv"
dblpfp = r"/content/drive/My Drive/ERinDS/DBLP2.csv"
acm_dblp_mapping_fp = r"/content/drive/My Drive/ERinDS/DBLP-ACM_perfectMapping.csv"

acm = pd.read_csv(acmfp)
dblp = pd.read_csv(dblpfp, encoding='latin-1')
perfect_mapping = pd.read_csv(acm_dblp_mapping_fp)

dblp['year'] = dblp['year'].astype(str)
acm['year'] = acm['year'].astype(str)



"""### Overview"""

acm

dblp

perfect_mapping

acm.loc[acm['id'] == 375678]

dblp.loc[dblp['id'] == 'conf/sigmod/SlivinskasJS01']

"""### Preprocess"""

def preprocess(row):
  # print(row)
  paper_str = " ".join(row)
  paper_str = paper_str.lower()

  return paper_str

"""### Dataset split"""



"""### Model evaluation

Small dataset
"""

text = []
id = []
sameas = []
true_labels = []
data = {'id':[],'text':[],'sameas':[]}
index = 0

for _,row in perfect_mapping.head(10).iterrows():

  # DBLP
  dplp_row = dblp.loc[dblp.id == row['idDBLP'],['title','authors','venue','year']].values.flatten().tolist()
  id.append(row['idDBLP'])
  sameas.append(row['idACM'])
  dplp_row = preprocess(dplp_row)
  text.append(dplp_row)

  # ACM
  acm_row = acm.loc[acm.id == row['idACM'],['title','authors','venue','year']].values.flatten().tolist()
  acm_row = preprocess(acm_row)
  text.append(acm_row)
  id.append(row['idACM'])
  sameas.append(row['idDBLP'])

data['id'] = id
data['text'] = text
data['sameas'] = sameas

dataset=pd.DataFrame(data)
print(dataset)

model = RankedWTAHash(
    max_numberOf_clusters = 10,
    max_editDistance = 140,
    windowSize=5,
    similarityThreshold = 0.8,
    maxOnly=True,
    metric='customkendal'
    )
EditDistance = model.EditDistance
model.fit(dataset['text'],None)

"""---

# References

[1]   [The dissimilarity representation for pattern recognition, a tutorial
Robert P.W. Duin and Elzbieta Pekalska Delft University of Technology, The Netherlands School of Computer Science, University of Manchester, United Kingdom](http://homepage.tudelft.nl/a9p19/presentations/DisRep_Tutorial_doc.pdf)
"""