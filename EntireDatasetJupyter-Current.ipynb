{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    " <img src=\"http://www.di.uoa.gr/themes/corporate_lite/logo_en.png\" title=\"Department of Informatics and Telecommunications - University of Athens\" align=\"center\" /> \n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\"> \n",
    "  <font size=\"4\"><b>Bachelor Thesis</b> </font>\n",
    "</div>\n",
    "<br>\n",
    "<div align=\"center\"> \n",
    "  <font size=\"5\">\n",
    "      <b>Entity Resolution in Dissimilarity Spaces  <br></b> \n",
    "    </font>\n",
    "     <br>\n",
    "     <font size=\"3\">\n",
    "        Implementation notebook     \n",
    "    </font>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\"> \n",
    "    <font size=\"4\">\n",
    "         <b>Konstantinos Nikoletos, BS Student</b>\n",
    "     </font>\n",
    "</div>\n",
    "<br>\n",
    "<div align=\"center\"> \n",
    "    <font size=\"4\">\n",
    "     <b> Dr. Alex Delis</b>,  Professor NKUA <br> \n",
    "     <b> Dr. Vassilis Verikios</b>, Professor Hellenic Open University\n",
    "    </font>\n",
    "</div>\n",
    "<br>\n",
    "<div align=\"center\"> \n",
    "    <font size=\"2\">Athens</font>\n",
    "</div>\n",
    "<div align=\"center\"> \n",
    "    <font size=\"2\">January 2021 - Ongoing</font>\n",
    "</div>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Implementation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Install components__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install editdistance\n",
    "!pip install pandas_read_xml\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Import libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import editdistance\n",
    "import string\n",
    "import sklearn\n",
    "import pandas_read_xml as pdx\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import math\n",
    "import os\n",
    "import scipy.special as special\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from scipy.spatial.distance import directed_hausdorff,hamming\n",
    "from scipy.stats._stats import _kendall_dis\n",
    "from scipy.stats import spearmanr,kendalltau,pearsonr,kruskal,mannwhitneyu\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.metrics.distance import jaro_similarity,jaro_winkler_similarity,jaccard_distance\n",
    "from sklearn.metrics import jaccard_score,accuracy_score,auc,f1_score,recall_score,precision_score,classification_report\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "from scipy import stats \n",
    "from scipy.spatial.distance import euclidean,hamming\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.metrics import ndcg_score\n",
    "from datetime import datetime\n",
    "\n",
    "plt.style.use('seaborn-whitegrid') # plot style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Model__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RankedWTAHash:\n",
    "\n",
    "    def __init__(self, max_numberOf_clusters, max_editDistance, windowSize, number_of_permutations=1, min_numOfNodes = 2,jaccard_withchars =True,distanceMetricEmbedding = 'l_inf', metric = 'kendal', similarityVectors='ranked', distanceMetric = 'edit', prototypesFilterThr=None, ngramms=3, similarityThreshold=None, maxOnly=None,earlyStop=0):\n",
    "        '''\n",
    "          Constructor\n",
    "        '''\n",
    "        self.max_numberOf_clusters = max_numberOf_clusters\n",
    "        self.pairDictionary = dict()\n",
    "        self.max_editDistance = max_editDistance\n",
    "        self.windowSize = windowSize\n",
    "        self.S_set = None\n",
    "        self.S_index = None\n",
    "        self.similarityThreshold = similarityThreshold\n",
    "        self.maxOnly = maxOnly\n",
    "        self.metric = metric\n",
    "        self.min_numOfNodes = min_numOfNodes\n",
    "        self.similarityVectors = similarityVectors\n",
    "        self.number_of_permutations = number_of_permutations\n",
    "        self.distanceMetric = distanceMetric\n",
    "        self.distanceMetricEmbedding = distanceMetricEmbedding\n",
    "        self.ngramms = ngramms\n",
    "        self.jaccard_withchars =  jaccard_withchars\n",
    "        self.prototypesFilterThr = prototypesFilterThr\n",
    "        self.earlyStop = earlyStop\n",
    "        self.selectionVariance = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "          Fit the classifier from the training dataset.\n",
    "          Parameters\n",
    "          ----------\n",
    "          X : Training data.\n",
    "          Returns\n",
    "          -------\n",
    "          self : The fitted classifier.\n",
    "        \"\"\"\n",
    "        print(\"\\n#####################################################################\\n#     .~ RankedWTAHash with Vantage embeddings starts training ~.   #\\n#####################################################################\\n\")\n",
    "\n",
    "        if isinstance(X, list):\n",
    "            input_strings = X\n",
    "        else:\n",
    "            input_strings = list(X)\n",
    "\n",
    "        # print(input_strings)\n",
    "        self.initialS_set = np.array(input_strings,dtype=object)\n",
    "        self.S_set = np.array(input_strings,dtype=object)\n",
    "        if self.distanceMetric == 'jaccard' and self.jaccard_withchars == False:\n",
    "            for i in range(0,len(input_strings)):\n",
    "                self.S_set[i] = set(nltk.ngrams(nltk.word_tokenize(self.S_set[i]), n=self.ngramms))\n",
    "        elif self.distanceMetric == 'jaccard' and self.jaccard_withchars == True:\n",
    "            for i in range(0,len(input_strings)):\n",
    "                self.S_set[i] = set(nltk.ngrams(self.S_set[i], n=self.ngramms))\n",
    "\n",
    "        self.S_index = np.arange(0,len(input_strings),1)\n",
    "\n",
    "        # print(\"\\n\\nString positions are:\")\n",
    "        # print(self.S_index)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        print(\"###########################################################\\n# > 1. Prototype selection phase                          #\\n###########################################################\\n\")\n",
    "        print(\"\\n-> Finding prototypes and representatives of each cluster:\")\n",
    "        prototypes_time = time.time()\n",
    "        self.prototypeArray,self.selected_numOfPrototypes = self.Clustering_Prototypes(self.S_index,self.max_numberOf_clusters, self.max_editDistance, self.pairDictionary)\n",
    "        print(\"\\n- Prototypes selected\")\n",
    "        self.embeddingDim = self.prototypeArray.size\n",
    "        print(self.prototypeArray)\n",
    "        heatmapData = []\n",
    "        for pr in self.prototypeArray:\n",
    "            print(pr,\" -> \",self.initialS_set[pr])\n",
    "            heatmapData.append(self.S_set[pr])\n",
    "            \n",
    "        if self.selected_numOfPrototypes > 2:\n",
    "            self.selectionVariance = myHeatmap(self.prototypeArray,self.metric,self.EditDistance)\n",
    "            print(\"\\n- Mean variance in prototype selection: \",self.selectionVariance )\n",
    "        print(\"\\n- Final number of prototypes: \",self.selected_numOfPrototypes )\n",
    "        prototypes_time = time.time() - prototypes_time\n",
    "        print(\"\\n# Finished in %.6s secs\" % (prototypes_time))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        if self.earlyStop==1:\n",
    "            return self\n",
    "\n",
    "        print(\"###########################################################\\n# > 2. Embeddings based on the Vantage objects            #\\n###########################################################\\n\")\n",
    "        print(\"\\n-> Creating Embeddings:\")\n",
    "        embeddings_time = time.time()\n",
    "        self.Embeddings = self.CreateVantageEmbeddings(self.S_index,self.prototypeArray, self.pairDictionary)\n",
    "        print(\"- Embeddings created\")\n",
    "        print(self.Embeddings)\n",
    "        embeddings_time = time.time() - embeddings_time\n",
    "        print(\"\\n# Finished in %.6s secs\" % (embeddings_time))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        if self.earlyStop==2:\n",
    "            return self\n",
    "\n",
    "        print(\"###########################################################\\n# > 3. WTA Hashing                                        #\\n###########################################################\\n\")\n",
    "        print(\"\\n-> Creating WTA Buckets:\")\n",
    "        wta_time = time.time()\n",
    "        self.HashedClusters,self.buckets,self.rankedVectors = self.WTA(self.Embeddings,self.windowSize, self.number_of_permutations)\n",
    "        print(\"- WTA buckets: \")\n",
    "        for key in self.buckets.keys():\n",
    "            print(key,\" -> \",self.buckets[key])\n",
    "        print(\"\\n- WTA number of buckets: \", len(self.buckets.keys()))\n",
    "        print(\"\\n- WTA RankedVectors after permutation:\")\n",
    "        print(self.rankedVectors)\n",
    "        wta_time = time.time() - wta_time\n",
    "        print(\"\\n# Finished in %.6s secs\" % (wta_time))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "#         for key in self.buckets.keys():\n",
    "#             if len(self.buckets[key]) > 216:\n",
    "#                 self.earlyStop=3\n",
    "                \n",
    "        \n",
    "        if self.earlyStop==3:\n",
    "            return self\n",
    "\n",
    "        print(\"###########################################################\\n# > 4. Similarity checking                                #\\n###########################################################\\n\")\n",
    "        print(\"\\n-> Similarity checking:\")\n",
    "        if len(self.buckets.keys()) < 5:\n",
    "            self.earlyStop=5\n",
    "            return self\n",
    "\n",
    "        similarity_time = time.time()\n",
    "\n",
    "        if self.similarityVectors == 'ranked':\n",
    "            self.mapping,self.mapping_matrix = self.SimilarityEvaluation(self.buckets,self.rankedVectors,self.similarityThreshold,maxOnly=self.maxOnly, metric=self.metric)\n",
    "        elif self.similarityVectors == 'initial':\n",
    "            self.mapping,self.mapping_matrix = self.SimilarityEvaluation(self.buckets,self.Embeddings,self.similarityThreshold,maxOnly=self.maxOnly, metric=self.metric)\n",
    "        else:\n",
    "            warnings.warn(\"similarityVectors: Available options are: ranked,initial\")\n",
    "        #     print(\"- Similarity matrix (all values compared):\")\n",
    "        #     print(self.similarityProb_matrix)\n",
    "        print(\"- Similarity mapping in a matrix\")\n",
    "        print(self.mapping_matrix)\n",
    "        similarity_time = time.time() - similarity_time\n",
    "        print(\"\\n# Finished in %.6s secs\" % (similarity_time))\n",
    "        print(\"\\n#####################################################################\\n#                    .~ End of training ~.                          #\\n#####################################################################\\n\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def EditDistance(self, str1,str2,verbose=False):\n",
    "        if verbose:\n",
    "            if str1 == None:\n",
    "                print(\"1\")\n",
    "            elif str2 == None:\n",
    "                print(\"2\")\n",
    "            print(\"-> \"+str(str1))\n",
    "            print(\"--> \"+str(str2))\n",
    "            print(str(editdistance.eval(self.S_set[str1],self.S_set[str2])))\n",
    "\n",
    "        if ((str1,str2) or (str2,str1))  in self.pairDictionary.keys():\n",
    "            return self.pairDictionary[(str1,str2)]\n",
    "        else:\n",
    "\n",
    "            if self.distanceMetric == 'edit':\n",
    "                distance = editdistance.eval(self.S_set[str1],self.S_set[str2])\n",
    "            elif self.distanceMetric == 'jaccard':\n",
    "                distance = jaccard_distance(self.S_set[str1],self.S_set[str2])\n",
    "                        # jaccard because we want DISSIMILARITY\n",
    "            else:\n",
    "                warnings.warn(\"Available metrics for space creation: edit, jaccard \")\n",
    "            self.pairDictionary[(str2,str1)] = self.pairDictionary[(str1,str2)] = distance\n",
    "            return distance\n",
    "\n",
    "    #####################################################################\n",
    "    # 1. Prototype selection algorithm                                  #\n",
    "    #####################################################################\n",
    "\n",
    "    '''\n",
    "    Clustering_Prototypes(S,k,d,r,C) \n",
    "    The String Clustering and Prototype Selection Algorithm\n",
    "    is the main clustering method, that takes as input the intial strings S, \n",
    "    the max number of clusters to be generated in k,\n",
    "    the maximum allowable distance of a string to join a cluster in var d\n",
    "    and returns the prototype for each cluster in array Prototype\n",
    "    '''\n",
    "    def Clustering_Prototypes(self,S,k,d,pairDictionary,verbose=False):\n",
    "\n",
    "        # ----------------- Initialization phase ----------------- #\n",
    "        i = 0\n",
    "        j = 0\n",
    "        C = np.empty([S.size], dtype=int)\n",
    "        r = np.empty([2,k],dtype=object)\n",
    "\n",
    "        Clusters = [ [] for l in range(0,k)]\n",
    "\n",
    "        for i in tqdm(range(0,S.size,1)):     # String-clustering phase, for all strings\n",
    "            while j < k :       # iteration through clusters, for all clusters\n",
    "                if r[0][j] == None:      # case empty first representative for cluster j\n",
    "                    r[0][j] = S[i]   # init cluster representative with string i\n",
    "                    C[i] = j         # store in C that i-string belongs to cluster j\n",
    "                    Clusters[j].append(S[i])\n",
    "                    break\n",
    "                elif r[1][j] == None and (self.EditDistance(S[i],r[0][j]) <= d):  # case empty second representative\n",
    "                    r[1][j] = S[i]                                             # and ED of representative 1  smaller than i-th string\n",
    "                    C[i] = j\n",
    "                    Clusters[j].append(S[i])\n",
    "                    break\n",
    "                elif (r[0][j] != None and r[1][j] != None) and (self.EditDistance(S[i],r[0][j]) + self.EditDistance(S[i],r[1][j])) <= d:\n",
    "                    C[i] = j\n",
    "                    Clusters[j].append(S[i])\n",
    "                    break\n",
    "                else:\n",
    "                    j += 1\n",
    "            i += 1\n",
    "\n",
    "        # ----------------- Prototype selection phase ----------------- #\n",
    "\n",
    "        Projections = np.empty([k],dtype=object)\n",
    "        Prototypes = np.empty([k],dtype=int)\n",
    "        sortedProjections = np.empty([k],dtype=object)\n",
    "\n",
    "        Projections = []\n",
    "        Prototypes = []\n",
    "        sortedProjections = []\n",
    "\n",
    "        if verbose:\n",
    "            print(\"- - - - - - - - -\")\n",
    "            print(\"Cluster array:\")\n",
    "            print(C)\n",
    "            print(\"- - - - - - - - -\")\n",
    "            print(\"Represantatives array:\")\n",
    "            print(r)\n",
    "            print(\"- - - - - - - - -\")\n",
    "            print(\"Clusters:\")\n",
    "            print(Clusters)\n",
    "            print(\"- - - - - - - - -\")\n",
    "\n",
    "        new_numofClusters = k\n",
    "\n",
    "        # print(\"\\n\\n\\n****** Prototype selection phase *********\")\n",
    "        prototype_index = 0\n",
    "        for j in range(0,k,1):\n",
    "\n",
    "            # IF small cluster\n",
    "            # print(\"Len \",len(Clusters[j]))\n",
    "            if len(Clusters[j]) < self.min_numOfNodes or r[1][j] == None or r[0][j]==None:\n",
    "                new_numofClusters-=1\n",
    "                continue\n",
    "\n",
    "            Projections.append(self.Approximated_Projection_Distances_ofCluster(r[1][j], r[0][j], j, Clusters[j],pairDictionary))\n",
    "            # print(Projections[prototype_index])\n",
    "            sortedProjections.append({new_numofClusters: v for new_numofClusters, v in sorted(Projections[prototype_index].items(), key=lambda item: item[1])})\n",
    "\n",
    "\n",
    "            Prototypes.append(self.Median(sortedProjections[prototype_index]))\n",
    "            # print(Prototypes[prototype_index])\n",
    "\n",
    "            prototype_index += 1\n",
    "\n",
    "        # print(\"\\n****** END *********\\n\")\n",
    "        Prototypes,new_numofClusters = self.optimize_clusterSelection(Prototypes,new_numofClusters)\n",
    "\n",
    "        return np.array(Prototypes),new_numofClusters\n",
    "\n",
    "\n",
    "    def Approximated_Projection_Distances_ofCluster(self, right_rep, left_rep, cluster_id, clusterSet, pairDictionary):\n",
    "\n",
    "        distances_vector = dict()\n",
    "\n",
    "        if len(clusterSet) > 2:\n",
    "            rep_distance     = self.EditDistance(right_rep,left_rep)\n",
    "\n",
    "            for str_inCluster in range(0,len(clusterSet)):\n",
    "                if clusterSet[str_inCluster] != right_rep and clusterSet[str_inCluster] != left_rep:\n",
    "                    # print(clusterSet[str_inCluster],right_rep,left_rep)\n",
    "                    right_rep_distance = self.EditDistance(right_rep,clusterSet[str_inCluster])\n",
    "                    left_rep_distance  = self.EditDistance(left_rep,clusterSet[str_inCluster])\n",
    "\n",
    "                    if rep_distance == 0:\n",
    "                        distances_vector[clusterSet[str_inCluster]] = 0\n",
    "                    else:\n",
    "                        distance = (right_rep_distance**2-rep_distance**2-left_rep_distance**2 ) / (2*rep_distance)\n",
    "                        distances_vector[clusterSet[str_inCluster]] = distance\n",
    "\n",
    "        else:\n",
    "            if left_rep != None and right_rep == None:\n",
    "                distances_vector[left_rep] = left_rep\n",
    "            elif right_rep != None and left_rep == None:\n",
    "                distances_vector[right_rep] = right_rep\n",
    "            elif left_rep == None and right_rep == None:\n",
    "                return None\n",
    "            elif left_rep != None and right_rep != None:\n",
    "                distances_vector[right_rep] = right_rep\n",
    "                distances_vector[left_rep]  = left_rep\n",
    "                \n",
    "        return distances_vector\n",
    "\n",
    "    def Median(self, distances):\n",
    "        '''\n",
    "        Returns the median value of a vector\n",
    "        '''\n",
    "        keys = list(distances.keys())\n",
    "        if keys == 1:\n",
    "            return keys[0]\n",
    "\n",
    "        # print(distances)\n",
    "        keys = list(distances.keys())\n",
    "        # print(keys)\n",
    "        median_position = int(len(keys)/2)\n",
    "        # print(median_position)\n",
    "        median_value = keys[median_position]\n",
    "\n",
    "        return median_value\n",
    "\n",
    "    def optimize_clusterSelection(self,Prototypes,numOfPrototypes):\n",
    "\n",
    "        notwantedPrototypes = []\n",
    "        print(self.max_editDistance/2)\n",
    "        for pr_1 in range(0,numOfPrototypes):\n",
    "            for pr_2 in range(pr_1+1,numOfPrototypes):\n",
    "                if self.EditDistance(Prototypes[pr_1],Prototypes[pr_2]) < self.prototypesFilterThr:\n",
    "                    notwantedPrototypes.append(Prototypes[pr_2])\n",
    "\n",
    "        newPrototypes = list((set(Prototypes)).difference(set(notwantedPrototypes)))\n",
    "\n",
    "        print(\"Prototypes before:\")\n",
    "        print(Prototypes)\n",
    "        print(\"Not wanted:\")\n",
    "        print(set(notwantedPrototypes) )\n",
    "        print(\"Final:\")\n",
    "        print(newPrototypes)\n",
    "        return newPrototypes,len(newPrototypes)\n",
    "\n",
    "\n",
    "    #####################################################################\n",
    "    #       2. Embeddings based on the Vantage objects                  #\n",
    "    #####################################################################\n",
    "\n",
    "    '''\n",
    "    CreateVantageEmbeddings(S,VantageObjects): Main function for creating the string embeddings based on the Vantage Objects\n",
    "    '''\n",
    "    def CreateVantageEmbeddings(self, S, VantageObjects, pairDictionary):\n",
    "\n",
    "        # ------- Distance computing ------- #\n",
    "        vectors = []\n",
    "        for s in tqdm(range(0,S.size)):\n",
    "            string_embedding = []\n",
    "            for p in range(0,VantageObjects.size):\n",
    "                if VantageObjects[p] != None:\n",
    "                    #                   print(\"-\",VantageObjects[p])\n",
    "                    string_embedding.append(self.DistanceMetric(s,p,S,VantageObjects, pairDictionary))\n",
    "\n",
    "            # --- Ranking representation ---- #\n",
    "            ranked_string_embedding = stats.rankdata(string_embedding, method='dense')\n",
    "\n",
    "            # ------- Vectors dataset ------- #\n",
    "            vectors.append(ranked_string_embedding)\n",
    "\n",
    "        return np.array(vectors)\n",
    "\n",
    "\n",
    "    '''\n",
    "    DistanceMetric(s,p,S,Prototypes): Implementation of equation (5)\n",
    "    '''\n",
    "    def DistanceMetric(self, s, p, S, VantageObjects, pairDictionary, distanceMetricEmbedding = 'l_inf'):\n",
    "\n",
    "        if distanceMetricEmbedding == 'l_inf':\n",
    "            max_distance = None\n",
    "\n",
    "            for pp in range(0,VantageObjects.size):\n",
    "                if VantageObjects[pp] != None:\n",
    "                    string_distance = self.EditDistance(S[s],VantageObjects[pp])    # Edit distance String-i -> Vantage Object\n",
    "                    VO_distance     = self.EditDistance(VantageObjects[p],VantageObjects[pp])    # Edit distance Vantage Object-j -> Vantage Object-i\n",
    "\n",
    "                    abs_diff = abs(string_distance-VO_distance)\n",
    "\n",
    "                    # --- Max distance diff --- #\n",
    "                    if max_distance == None:\n",
    "                        max_distance = abs_diff\n",
    "                    elif abs_diff > max_distance:\n",
    "                        max_distance = abs_diff\n",
    "            return max_distance\n",
    "\n",
    "        elif distanceMetricEmbedding == 'edit':\n",
    "            return self.EditDistance(S[s],VantageObjects[p])\n",
    "        elif distanceMetricEmbedding == 'jaccard':\n",
    "            return jaccard_distance(S[s],VantageObjects[p])\n",
    "        elif distanceMetricEmbedding == 'euclid_jaccard':\n",
    "            return sqrt(1-jaccard_distance(S[s],VantageObjects[p]))\n",
    "        elif distanceMetricEmbedding == 'euclidean':\n",
    "            return euclidean(S[s],VantageObjects[p])\n",
    "        else:\n",
    "            warnings.warn(\"Available metrics: edit,jaccard,l_inf,euclidean\")\n",
    "\n",
    "\n",
    "    def dropNone(array):\n",
    "        array = list(filter(None, list(array)))\n",
    "        return np.array(array)\n",
    "\n",
    "\n",
    "    #####################################################################\n",
    "    #                 3. Similarity checking                            #\n",
    "    #####################################################################\n",
    "\n",
    "    def SimilarityEvaluation(self, buckets,vectors,threshold,maxOnly=None,metric=None):\n",
    "\n",
    "        numOfVectors = vectors.shape[0]\n",
    "        vectorDim    = vectors.shape[1]\n",
    "        mapping_matrix = np.zeros([numOfVectors,numOfVectors],dtype=np.int8)\n",
    "        self.similarityProb_matrix = np.empty([numOfVectors,numOfVectors],dtype=np.float)* np.nan\n",
    "        mapping = {}\n",
    "\n",
    "        # Loop for every bucket\n",
    "        for bucketid in tqdm(buckets.keys()):\n",
    "            bucket_vectors = buckets[bucketid]\n",
    "            numOfVectors = len(bucket_vectors)\n",
    "\n",
    "            print(bucket_vectors)\n",
    "            # For every vector inside the bucket\n",
    "            for v_index in range(0,numOfVectors,1):\n",
    "                v_vector_id = bucket_vectors[v_index]\n",
    "                # Loop to all the other\n",
    "                for i_index in range(v_index+1,numOfVectors,1):\n",
    "                    i_vector_id = bucket_vectors[i_index]\n",
    "                    if vectorDim == 1:\n",
    "                        warnings.warn(\"Vector dim equal to 1-Setting metric to kendalltau\")\n",
    "                        metric = 'kendal'\n",
    "\n",
    "                    if metric == None or metric == 'kendal':  # Simple Kendal tau metric\n",
    "                        similarity_prob, p_value = kendalltau(vectors[v_vector_id], vectors[i_vector_id])\n",
    "                    elif metric == 'customKendal':  # Custom Kendal tau\n",
    "                        numOf_discordant_pairs = _kendall_dis(vectors[v_vector_id].astype('intp'), vectors[i_vector_id].astype('intp'))\n",
    "                        similarity_prob = (2*numOf_discordant_pairs) / (vectorDim*(vectorDim-1))\n",
    "                    elif metric == 'jaccard':\n",
    "                        similarity_prob = jaccard_score(vectors[v_vector_id], vectors[i_vector_id], average='micro')\n",
    "                    elif metric == 'cosine':\n",
    "                        similarity_prob = cosine_similarity(np.array(vectors[v_vector_id]).reshape(1, -1), np.array(vectors[i_vector_id]).reshape(1, -1))\n",
    "                    elif metric == 'pearson':\n",
    "                        similarity_prob, _ = pearsonr(vectors[v_vector_id], vectors[i_vector_id])\n",
    "                    elif metric == 'spearman':\n",
    "                        similarity_prob, _ = spearmanr(vectors[v_vector_id], vectors[i_vector_id])\n",
    "                    elif metric == 'spearmanf':\n",
    "                        similarity_prob = 1-spearman_footrule_distance(vectors[v_vector_id], vectors[i_vector_id])\n",
    "                    elif metric == 'hamming':\n",
    "                        similarity_prob, _ = hamming(vectors[v_vector_id].astype('intp'), vectors[i_vector_id].astype('intp'))\n",
    "                    elif metric == 'kruskal':\n",
    "                        if np.array_equal(vectors[v_vector_id],vectors[i_vector_id]):\n",
    "                            similarity_prob=1.0\n",
    "                        else:\n",
    "                            _,similarity_prob = kruskal(vectors[v_vector_id], vectors[i_vector_id])\n",
    "                    elif metric == 'ndcg_score':\n",
    "                        similarity_prob, _ = ndcg_score(vectors[v_vector_id], vectors[i_vector_id])\n",
    "                    elif metric == 'rbo':\n",
    "                        similarity_prob = rbo(vectors[v_vector_id], vectors[i_vector_id])\n",
    "                    elif metric == 'wta':\n",
    "                        similarity_prob = WTA_similarity(vectors[v_vector_id], vectors[i_vector_id])\n",
    "                    elif metric == 'mannwhitneyu':\n",
    "                        if np.array_equal(vectors[v_vector_id],vectors[i_vector_id]):\n",
    "                            similarity_prob=1.0\n",
    "                        else:\n",
    "                            _,similarity_prob = mannwhitneyu(vectors[v_vector_id], vectors[i_vector_id])\n",
    "                    else:\n",
    "                        warnings.warn(\"SimilarityEvaluation: Available similarity metrics: kendal,customKendal,jaccard,ndcg_score,cosine,spearman,pearson\")\n",
    "\n",
    "\n",
    "                    self.similarityProb_matrix[v_vector_id][i_vector_id] = similarity_prob\n",
    "\n",
    "                    if similarity_prob > threshold:\n",
    "                        if v_vector_id not in mapping.keys():\n",
    "                            mapping[v_vector_id] = []\n",
    "                        mapping[v_vector_id].append(i_vector_id)  # insert into mapping\n",
    "                        mapping_matrix[v_vector_id][i_vector_id] = 1  # inform prediction matrix\n",
    "                        mapping_matrix[i_vector_id][v_vector_id] = 1  # inform prediction matrix\n",
    "\n",
    "\n",
    "        return mapping, np.triu(mapping_matrix)\n",
    "\n",
    "    #####################################################################\n",
    "    #                        4. WTA Hashing                             #\n",
    "    #####################################################################\n",
    "\n",
    "    def WTA(self,vectors,K, number_of_permutations):\n",
    "        '''\n",
    "          Winner Take All hash - Yagnik\n",
    "          .............................\n",
    "    \n",
    "          K: window size\n",
    "        '''\n",
    "        newVectors = []\n",
    "        buckets = dict()\n",
    "\n",
    "        numOfVectors = vectors.shape[0]\n",
    "        vectorDim    = vectors.shape[1]\n",
    "\n",
    "        if vectorDim < K:\n",
    "            K = vectorDim\n",
    "            warnings.warn(\"Window size greater than vector dimension\")\n",
    "\n",
    "        C = np.zeros([numOfVectors,number_of_permutations], dtype=int)\n",
    "\n",
    "        permutation_dimension = vectorDim\n",
    "        for permutation_index in range(0,number_of_permutations,1):\n",
    "            theta = np.random.permutation(permutation_dimension)\n",
    "            i=0;j=0;\n",
    "            for v_index in range(0,numOfVectors,1):\n",
    "                if permutation_index == 0:\n",
    "                    X_new = self.permuted(vectors[v_index],theta)\n",
    "                    newVectors.append(X_new)\n",
    "                else:\n",
    "                    X_new = self.permuted(vectors[v_index],theta)\n",
    "                    newVectors[v_index] = X_new\n",
    "\n",
    "                C[i][permutation_index] = max(range(len(X_new[:K])), key=X_new[:K].__getitem__)\n",
    "                i+=1\n",
    "            \n",
    "\n",
    "        for c,i in zip(C,range(0,numOfVectors,1)):\n",
    "            buckets = self.bucketInsert(buckets,str(c),i)\n",
    "\n",
    "        return C,buckets,np.array(newVectors,dtype=np.intp)\n",
    "\n",
    "\n",
    "    def permuted(self,vector,permutation):\n",
    "        permuted_vector = [vector[x] for x in permutation]\n",
    "\n",
    "        return permuted_vector\n",
    "\n",
    "\n",
    "    def bucketInsert(self,buckets,bucket_id,item):\n",
    "        if bucket_id not in buckets.keys():\n",
    "            buckets[bucket_id] = []\n",
    "        buckets[bucket_id].append(item)\n",
    "\n",
    "        return buckets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various similarity metrics to check\n",
    "These functions are for the similarity checking phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spearman footrule distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spearman_footrule_distance(s,t):\n",
    "    \"\"\"\n",
    "    Computes the Spearman footrule distance between two full lists of ranks:\n",
    "        F(s,t) = sum[ |s(i) - t(i)| ]/S,\n",
    "    the normalized sum over all elements in a set of the absolute difference between\n",
    "    the rank according to s and t.  As defined, 0 <= F(s,t) <= 1.\n",
    "    S is a normalizer which is equal to 0.5*len(s)^2 for even length ranklists and\n",
    "    0.5*(len(s)^2 - 1) for odd length ranklists.\n",
    "    If s,t are *not* full, this function should not be used. s,t should be array-like\n",
    "    (lists are OK).\n",
    "    \"\"\"\n",
    "    # check that size of intersection = size of s,t?\n",
    "    assert len(s) == len(t)\n",
    "    sdist = sum(abs(s - t))\n",
    "    # c will be 1 for odd length lists and 0 for even ones\n",
    "    c = len(s) % 2\n",
    "    normalizer = 0.5*(len(s)**2 - c)\n",
    "    \n",
    "    return sdist/normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ravi Kumar generalized Kendall Tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kendall_top_k(a,b,k=None,p=0): #zero is equal 1 is max distance, compare with 1-scipy.stats.kendalltau(a,b)/2+1/2\n",
    "    \"\"\"\n",
    "    kendall_top_k(np.array,np.array,k,p)\n",
    "    This function generalise kendall-tau as defined in [1] Fagin, Ronald, Ravi Kumar, and D. Sivakumar. \"Comparing top k lists.\" SIAM Journal on Discrete Mathematics 17.1 (2003): 134-160.\n",
    "    It returns a distance: 0 for identical (in the sense of top-k) lists and 1 if completely different.\n",
    "    Example:\n",
    "        Simply call it with two same-length arrays of ratings (or also rankings), length of the top elements k (default is the maximum length possible), and p (default is 0, see [1]) as parameters:\n",
    "            $ a = np.array([1,2,3,4,5])\n",
    "            $ b = np.array([5,4,3,2,1])\n",
    "            $ kendall_top_k(a,b,k=4)\n",
    "    \"\"\"\n",
    "\n",
    "    if k is None:\n",
    "        k = a.size\n",
    "    if a.size != b.size:\n",
    "        raise NameError('The two arrays need to have same lengths')\n",
    "    k = min(k,a.size)\n",
    "    a_top_k = np.argpartition(a,-k)[-k:]\n",
    "    b_top_k = np.argpartition(b,-k)[-k:]\n",
    "    common_items = np.intersect1d(a_top_k,b_top_k)\n",
    "    only_in_a = np.setdiff1d(a_top_k, common_items)\n",
    "    only_in_b = np.setdiff1d(b_top_k, common_items)\n",
    "    kendall = (1 - (stats.kendalltau(a[common_items], b[common_items])[0]/2+0.5)) * (common_items.size**2) #case 1\n",
    "    if np.isnan(kendall): # degenerate case with only one item (not defined by Kendall)\n",
    "        kendall = 0\n",
    "    for i in common_items: #case 2\n",
    "        for j in only_in_a:\n",
    "            if a[i] < a[j]:\n",
    "                kendall += 1\n",
    "        for j in only_in_b:\n",
    "            if b[i] < b[j]:\n",
    "                kendall += 1\n",
    "    kendall += 2*p * special.binom(k-common_items.size,2)     #case 4\n",
    "    kendall /= ((only_in_a.size + only_in_b.size + common_items.size)**2 ) #normalization\n",
    "    return kendall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank Biased Overlap (RBO) \n",
    "Article: https://towardsdatascience.com/rbo-v-s-kendall-tau-to-compare-ranked-lists-of-items-8776c5182899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbo(list1, list2, p=0.9):\n",
    "   # tail recursive helper function\n",
    "   def helper(ret, i, d):\n",
    "       l1 = set(list1[:i]) if i < len(list1) else set(list1)\n",
    "       l2 = set(list2[:i]) if i < len(list2) else set(list2)\n",
    "       a_d = len(l1.intersection(l2))/i\n",
    "       term = math.pow(p, i) * a_d\n",
    "       if d == i:\n",
    "           return ret + term\n",
    "       return helper(ret + term, i + 1, d)\n",
    "   k = max(len(list1), len(list2))\n",
    "   x_k = len(set(list1).intersection(set(list2)))\n",
    "   summation = helper(0, 1, k)\n",
    "   return ((float(x_k)/k) * math.pow(p, k)) + ((1-p)/p * summation)\n",
    "\n",
    "# Example usage\n",
    "print(rbo([4,10,20], [1,2,3]))\n",
    "\n",
    "p = 0.9\n",
    "d = 3\n",
    "\n",
    "def sum_series(p, d):\n",
    "   # tail recursive helper function\n",
    "   def helper(ret, p, d, i):\n",
    "       term = math.pow(p, i)/i\n",
    "       if d == i:\n",
    "           return ret + term\n",
    "       return helper(ret + term, p, d, i+1)\n",
    "   return helper(0, p, d, 1)\n",
    "\n",
    "wrbo1_d = 1 - math.pow(p, d-1) + (((1-p)/p) * d *(np.log(1/(1-p)) - sum_series(p, d-1)))\n",
    "\n",
    "print(wrbo1_d) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Winner Takes All proposed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WTA_similarity(vector1,vector2):\n",
    "    \n",
    "    PO=0\n",
    "    for i in range(0,len(vector1),1):\n",
    "        for j in range(0,i,1):\n",
    "            ij_1 = vector1[i] - vector1[j]\n",
    "            ij_2 = vector2[i] - vector2[j]\n",
    "            PO += WTA_Threshold(ij_1*ij_2)\n",
    "    return PO\n",
    "\n",
    "def WTA_Threshold(x):    \n",
    "    if x>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# __Evaluation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from Drive in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Opening data file\n",
    "# import io\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive',force_remount=True)\n",
    "\n",
    "# fpcora = r\"/content/drive/My Drive/ERinDS/CORA.xml\"\n",
    "# fpcora_gold = r\"/content/drive/My Drive/ERinDS/cora_gold.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from disk for Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JedAI Dirty datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORA_groundTruth = os.path.abspath(\"data/coraIdDuplicates.csv\")\n",
    "CORA = os.path.abspath(\"data/coraProfiles.csv\")\n",
    "CORA_groundTruth = pd.read_csv(CORA_groundTruth,sep='|',header=None,names=['id1','id2'])\n",
    "CORA = pd.read_csv(CORA,sep='|')\n",
    "CORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORA_groundTruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CENSUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "CENSUS_groundTruth = os.path.abspath(\"data/censusIdDuplicates.csv\")\n",
    "CENSUS = os.path.abspath(\"data/censusProfiles.csv\")\n",
    "CENSUS_groundTruth = pd.read_csv(CENSUS_groundTruth,sep='|',header=None,names=['id1','id2'])\n",
    "CENSUS = pd.read_csv(CENSUS,sep='|')\n",
    "CENSUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CENSUS_groundTruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CDDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CDDB_groundTruth = os.path.abspath(\"data/cddbIdDuplicates.csv\")\n",
    "CDDB = os.path.abspath(\"data/cddbProfiles.csv\")\n",
    "CDDB_groundTruth = pd.read_csv(CDDB_groundTruth,sep='/00000',engine='python',header=None,names=['id1','id2'])\n",
    "CDDB = pd.read_csv(CDDB,sep='/00000',engine='python')\n",
    "CDDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDDB_groundTruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBLP - ACM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACM = os.path.abspath(\"ACM.csv\")\n",
    "DBLP = os.path.abspath(\"DBLP2.csv\")\n",
    "ACM_DBLP_trueValues = os.path.abspath(\"DBLP-ACM_perfectMapping.csv\")\n",
    "ACM = pd.read_csv(ACM)\n",
    "DBLP = pd.read_csv(DBLP, encoding='latin-1')\n",
    "ACM_DBLP_trueValues = pd.read_csv(ACM_DBLP_trueValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cora - 1st edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpcora = os.path.abspath(\"CORA.xml\")\n",
    "fpcora_gold = os.path.abspath(\"cora_gold.csv\")\n",
    "cora = pdx.read_xml(fpcora,['CORA', 'NEWREFERENCE'],root_is_rows=False)\n",
    "cora_dataframe = cora\n",
    "cora_dataframe['@id'] = pd.to_numeric(cora_dataframe['@id']).subtract(1)\n",
    "cora_gold = pd.read_csv(fpcora_gold,sep=';')\n",
    "true_values = cora_gold\n",
    "cora_gold['id1'] = pd.to_numeric(cora_gold['id1']).subtract(1)\n",
    "cora_gold['id2'] = pd.to_numeric(cora_gold['id2']).subtract(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def myHeatmap(data,metric,distance):\n",
    "    if metric == 'edit':    \n",
    "        matrix  = np.zeros((len(data),len(data)), dtype=np.int)\n",
    "    else:\n",
    "        matrix = np.zeros((len(data),len(data)), dtype=np.float)\n",
    "    \n",
    "    for i in range(0,len(data),1):\n",
    "        for j in range(0,len(data),1):\n",
    "            if i != j:            \n",
    "                matrix[i][j]  = distance(data[i],data[j])\n",
    "\n",
    "                \n",
    "    fif,ax = plt.subplots(1,figsize=(10,10))\n",
    "    corr = np.corrcoef(matrix)\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    ax = sns.heatmap(matrix, linewidth=0.5,annot=False,cmap=\"Blues_r\",mask=mask,fmt='.3g',ax=ax)\n",
    "    plt.show()\n",
    "    return matrix.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __CORA Evaluation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(row):\n",
    "\n",
    "    paper_str = \" \".join(row)\n",
    "\n",
    "    # Lower letters \n",
    "    paper_str = paper_str.lower()\n",
    "    \n",
    "    # Remove unwanted chars \n",
    "    paper_str = paper_str.replace(\"\\n\", \" \").replace(\"/z\", \" \")\n",
    "    \n",
    "    # Remove pancutation     \n",
    "    paper_str = paper_str.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    return str(paper_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df = dataset.sample(frac=1).reset_index(drop=True)\n",
    "# shuffled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting CORA dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cora_createDataset(cora_dataframe, true_values, fields, keepNone = False):\n",
    "\n",
    "    rawStr_col = []\n",
    "    index_to_id_dict = {}\n",
    "    sameEntities_dictionary = {}\n",
    "\n",
    "    i=0\n",
    "    for _, row in tqdm(cora_dataframe.iterrows()):\n",
    "        index_to_id_dict[int(row['@id'])] = i\n",
    "\n",
    "        rawStr = []\n",
    "        for field in fields:    # NAN\n",
    "            \n",
    "            if (isna(row[field]) and keepNone == True) or (keepNone == False and not isna(row[field])):\n",
    "                rawStr.append(str(row[field]))\n",
    "        i+=1\n",
    "        rawStr_col.append(preprocess(rawStr))\n",
    "\n",
    "    num_of_records = len(cora_dataframe)\n",
    "    trueValues_matrix = np.zeros([num_of_records,num_of_records],dtype=np.int8)\n",
    "\n",
    "    cluster_dict = {0:set()}\n",
    "    cluster_dict[0].add(0)\n",
    "    clusters = []\n",
    "    key = 0\n",
    "\n",
    "    for _, row in tqdm(true_values.iterrows()):  \n",
    "        # print(index_to_id_dict[row['id1']],index_to_id_dict[row['id2']])\n",
    "        trueValues_matrix[index_to_id_dict[row['id1']]][index_to_id_dict[row['id2']]] = 1\n",
    "        trueValues_matrix[index_to_id_dict[row['id2']]][index_to_id_dict[row['id1']]] = 1\n",
    "\n",
    "\n",
    "        if index_to_id_dict[row['id1']] in cluster_dict[key] or index_to_id_dict[row['id2']] in cluster_dict[key]:\n",
    "            cluster_dict[key].add(index_to_id_dict[row['id1']])\n",
    "            cluster_dict[key].add(index_to_id_dict[row['id2']])\n",
    "        elif index_to_id_dict[row['id1']] in cluster_dict[key] and index_to_id_dict[row['id2']] not in cluster_dict[key]: \n",
    "            cluster_dict[key].add(index_to_id_dict[row['id2']])\n",
    "        elif index_to_id_dict[row['id2']] in cluster_dict[key] and index_to_id_dict[row['id1']] not in cluster_dict[key]: \n",
    "            cluster_dict[key].add(index_to_id_dict[row['id1']])\n",
    "        elif index_to_id_dict[row['id2']] not in cluster_dict[key] and index_to_id_dict[row['id1']] not in cluster_dict[key]: \n",
    "            key+=1\n",
    "            cluster_dict[key] = set()\n",
    "            cluster_dict[key].add(index_to_id_dict[row['id1']])\n",
    "            cluster_dict[key].add(index_to_id_dict[row['id2']])    \n",
    "        clusters.append(key)\n",
    "\n",
    "        if index_to_id_dict[row['id1']] not in sameEntities_dictionary.keys():\n",
    "             sameEntities_dictionary[index_to_id_dict[row['id1']]] = []\n",
    "        sameEntities_dictionary[ index_to_id_dict[row['id1']]].append( index_to_id_dict[row['id2']])\n",
    "\n",
    "    return rawStr_col,sameEntities_dictionary, np.triu(trueValues_matrix), clusters\n",
    "\n",
    "def isna(value):\n",
    "    if isinstance(value, float) and math.isnan(value):\n",
    "        return True \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "     'address',\n",
    "     'author',\n",
    "     'editor',\n",
    "     'institution',\n",
    "     'month',\n",
    "     'note',\n",
    "     'pages',\n",
    "     'publisher',\n",
    "     'title',\n",
    "     'venue',\n",
    "     'volume',\n",
    "     'year',\n",
    "     'Unnamed: 13'\n",
    "]\n",
    "\n",
    "fields = [\n",
    "     'author',\n",
    "     'title',\n",
    "]\n",
    "\n",
    "# data, true_labels, true_matrix, clusters = cora_createDataset(CORA, CORA_groundTruth, fields)\n",
    "data, true_labels, true_matrix, clusters = cora_createDataset(cora_dataframe, true_values, fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(CORA.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = [ len(x) for x in data ]\n",
    "print(\"Dataset size: \",len(data),\" strings-papers\")\n",
    "print(\"Average length: %d\" % (np.mean(data_length)))\n",
    "print(\"Min length: %d\" % (min(data_length)))\n",
    "print(\"Max length: %d\" % (max(data_length)))\n",
    "print(\"Median length: %d\" % (np.median(data_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(range(0,len(data_length),1),data_length)\n",
    "plt.xlabel(\"Record index\")\n",
    "plt.ylabel(\"String length\")\n",
    "plt.title(\"StrLength\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function\n",
    "\n",
    "Returns:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#                          Evaluation                               # \n",
    "#####################################################################\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "def evaluate_cora(predicted_matrix, true_matrix, with_classification_report=False ):\n",
    "\n",
    "  print(\"#####################################################################\\n#                          Evaluation                               #\\n#####################################################################\\n\")\n",
    "  true_matrix = sparse.triu(true_matrix)\n",
    "  # print(true_matrix)\n",
    "  predicted_matrix =  sparse.triu(predicted_matrix)\n",
    "  # print(predicted_matrix)\n",
    "\n",
    "  acc = 100*accuracy_score(true_matrix, predicted_matrix)\n",
    "  f1 =  100*f1_score(true_matrix, predicted_matrix, average='micro')\n",
    "  recall = 100*recall_score(true_matrix, predicted_matrix, average='micro')\n",
    "  precision = 100*precision_score(true_matrix, predicted_matrix, average='micro')\n",
    "\n",
    "  print(\"Accuracy:  %3.2f %%\" % (acc))\n",
    "  print(\"F1-Score:  %3.2f %%\" % (f1))\n",
    "  print(\"Recall:    %3.2f %%\" % (recall))\n",
    "  print(\"Precision: %3.2f %%\" % (precision))\n",
    "\n",
    "  # results_dataframe = pd.DataFrame(columns=['Accuracy','Precision','Recall','F1'])\n",
    "  # results_dataframe.loc[len(results_dataframe)+1] = [acc,precision,recall,f1]\n",
    "\n",
    "  if with_classification_report:\n",
    "    print(classification_report(true_matrix, predicted_matrix))\n",
    "\n",
    "  print('\\n\\n')\n",
    "  return acc,f1,precision,recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Jaccard VS Edit distance variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "from numpy.linalg import svd\n",
    "\n",
    "def PCA_SpaceVisualization(X,title='PCA plot'):\n",
    "    '''\n",
    "    PCA to given array X and creating a plot\n",
    "    Returns PCA components array after fit_transform\n",
    "    '''\n",
    "    \n",
    "    # PCA code\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X)\n",
    "    pcaComponents = pca.fit_transform(X) # pcaComponents is the data that I'll use from PCA\n",
    "    \n",
    "    # Seperating components\n",
    "    first_component = [x[0] for x in pcaComponents]\n",
    "    second_component = [x[1] for x in pcaComponents]\n",
    "    \n",
    "    # Plotting code\n",
    "    fig, ax = plt.subplots(figsize=(25,10))\n",
    "    ax.scatter(first_component, second_component,alpha=0) \n",
    "    fig.suptitle(title,fontsize=40,fontweight='bold')\n",
    "    ax.set_xlabel('X Component',fontsize=30,fontweight='bold')\n",
    "    ax.set_ylabel('Y Component',fontsize=30,fontweight='bold')\n",
    "    \n",
    "    for x0, y0, i in zip(first_component, second_component,range(0,len(first_component),1)):\n",
    "        plt.text(x0,y0,i, ha=\"center\", va=\"center\",fontsize=20,color='b')\n",
    "        \n",
    "    return pcaComponents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sent = 'This is an example of bigramms and trigramms!'\n",
    "print(set(nltk.ngrams(sent, n=3)))\n",
    "print(set(nltk.ngrams(nltk.word_tokenize(sent), n=3)))\n",
    "print(nltk.jaccard_distance(set(nltk.ngrams(sent, n=3)),set(nltk.ngrams(sent, n=3))))\n",
    "print(nltk.jaccard_distance(set(nltk.ngrams(nltk.word_tokenize(sent), n=3)),set(nltk.ngrams(nltk.word_tokenize(sent), n=3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype selection variance HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarityProbsHeatMap(similarityProb_matrix,clusters,title):\n",
    "    fif,ax = plt.subplots(1,figsize=(20,20))\n",
    "    colors = ['red','green','blue','yellow','orange']\n",
    "    c=0\n",
    "    for cl in clusters:\n",
    "        for i  in range(0,len(cl)):\n",
    "            for j in range(i+1,len(cl)):\n",
    "                ax.add_patch(Rectangle((cl[j],cl[i]), 1, 1, fill=False, edgecolor=colors[c], lw=3))\n",
    "        c+=1\n",
    "#     corr = np.corrcoef(ed_matrix)\n",
    "#     mask = np.zeros_like(corr)\n",
    "#     mask[np.tril_indices_from(mask)] = True\n",
    "    ax.set_title(title,fontsize=20,fontweight='bold')\n",
    "    ax = sns.heatmap(similarityProb_matrix, linewidth=0.5,annot=True,cmap=\"Blues\",fmt='.3g',ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"m. ahlskog  j. paloheimo  h. stubb  p. dyreklev  m. fahlman  o. inganas and m.r.   andersson  nan j appl. phys.\"\n",
    "s2 = \"m. ahlskog  j. paloheimo  h. stubb  p. dyreklev  m. fahlman  o. inganas and m.r. andersson  j appl. phys. \"\n",
    "\n",
    "jc3 = jaccard_distance(set(nltk.ngrams(s1, n=3)),set(nltk.ngrams(s2, n=3)))\n",
    "js3 = jaccard_distance(set(nltk.ngrams(nltk.word_tokenize(s1), n=3)),set(nltk.ngrams(nltk.word_tokenize(s2), n=3)))\n",
    "jc2 = jaccard_distance(set(nltk.ngrams(s1, n=2)),set(nltk.ngrams(s2, n=2)))\n",
    "js2 = jaccard_distance(set(nltk.ngrams(nltk.word_tokenize(s1), n=2)),set(nltk.ngrams(nltk.word_tokenize(s2), n=2)))\n",
    "\n",
    "print(jc3,js3,jc2,js2)\n",
    "print(editdistance.eval(s1,s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dataframe = pd.DataFrame(columns=['max_numberOf_clusters','max_editDistance','similarityThreshold','windowSize','metric','similarityVectors',\"distanceMetricEmbedding\",\"distanceMetric\",\"number_of_permutations\",\"ngramms\",\"jaccard_with_chars\",'Accuracy','Precision','Recall','F1','Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORA: Best Jaccard execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ngramms= 3                                  # If jaccard used, n-gramms are used\n",
    "jaccard_withchars = True                    # n-gramms either of chars and either of words\n",
    "\n",
    "# Prototype selection\n",
    "max_numberOf_clusters= 500                  # Νumber of loops for finding representatives, it is an upper bound of clusters.\n",
    "max_editDistance= 0.5                       # The threshold for the triangle inequality\n",
    "distanceMetric= 'jaccard'                   # Distance metric between the vectors when creating the space\n",
    "prototypesFilterThr = 0.3                 # Prototypes must differ more that threshold\n",
    "\n",
    "# Embedding phase\n",
    "distanceMetricEmbedding = 'euclid_jaccard'  # Embedding metric\n",
    "\n",
    "# WTA algorithm\n",
    "windowSize= 70                             # Vector size for WTA algo\n",
    "number_of_permutations = 1                 # WTA number of permutations\n",
    "\n",
    "\n",
    "# Similarity evaluation\n",
    "similarityVectors='initial'                  # which vectors will be passed to WTA step\n",
    "similarityThreshold= 0.7                    # Similarity threshold for the final step\n",
    "metric='kendal'                             # Similarity metric between vectors\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "model = RankedWTAHash(\n",
    "    max_numberOf_clusters= max_numberOf_clusters,    \n",
    "    max_editDistance= max_editDistance,    \n",
    "    windowSize= windowSize,    \n",
    "    similarityThreshold= similarityThreshold,    \n",
    "    metric=metric,    \n",
    "    similarityVectors=similarityVectors,    \n",
    "    number_of_permutations = number_of_permutations,\n",
    "    distanceMetric= distanceMetric,\n",
    "    distanceMetricEmbedding = distanceMetricEmbedding,\n",
    "    ngramms= ngramms,\n",
    "    jaccard_withchars = jaccard_withchars,\n",
    "    prototypesFilterThr = prototypesFilterThr\n",
    ")\n",
    "model = model.fit(data)\n",
    "acc,f1,precision,recall = evaluate_cora(model.mapping_matrix,true_matrix, False)\n",
    "exec_time = time.time() - start\n",
    "results_dataframe.loc[len(results_dataframe)+1] = [max_numberOf_clusters,max_editDistance,similarityThreshold,windowSize,metric,similarityVectors,distanceMetricEmbedding,distanceMetric,number_of_permutations,ngramms,jaccard_withchars,acc,precision,recall,f1,exec_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORA: Best Edit distance execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ngramms= 3                                  # If jaccard used, n-gramms are used\n",
    "jaccard_withchars = False                    # n-gramms either of chars and either of words\n",
    "\n",
    "# Prototype selection\n",
    "max_numberOf_clusters= 1000                  # Νumber of loops for finding representatives, it is an upper bound of clusters.\n",
    "max_editDistance= 120                       # The threshold for the triangle inequality\n",
    "distanceMetric= 'edit'                   # Distance metric between the vectors when creating the space\n",
    "prototypesFilterThr = 50                # Prototypes must differ more that threshold\n",
    "\n",
    "# Embedding phase\n",
    "distanceMetricEmbedding = 'edit'  # Embedding metric\n",
    "\n",
    "# WTA algorithm\n",
    "windowSize= 37                             # Vector size for WTA algo\n",
    "number_of_permutations = 10                 # WTA number of permutations\n",
    "\n",
    "\n",
    "# Similarity evaluation\n",
    "similarityVectors='initial'                  # which vectors will be passed to WTA step\n",
    "similarityThreshold= 0.7                    # Similarity threshold for the final step\n",
    "metric='kendal'                             # Similarity metric between vectors\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "model = RankedWTAHash(\n",
    "    max_numberOf_clusters= max_numberOf_clusters,    \n",
    "    max_editDistance= max_editDistance,    \n",
    "    windowSize= windowSize,    \n",
    "    similarityThreshold= similarityThreshold,    \n",
    "    metric=metric,    \n",
    "    similarityVectors=similarityVectors,    \n",
    "    number_of_permutations = number_of_permutations,\n",
    "    distanceMetric= distanceMetric,\n",
    "    distanceMetricEmbedding = distanceMetricEmbedding,\n",
    "    ngramms= ngramms,\n",
    "    jaccard_withchars = jaccard_withchars,\n",
    "    prototypesFilterThr = prototypesFilterThr\n",
    ")\n",
    "model = model.fit(data)\n",
    "acc,f1,precision,recall = evaluate_cora(model.mapping_matrix,true_matrix, False)\n",
    "exec_time = time.time() - start\n",
    "results_dataframe.loc[len(results_dataframe)+1] = [max_numberOf_clusters,max_editDistance,similarityThreshold,windowSize,metric,similarityVectors,distanceMetricEmbedding,distanceMetric,number_of_permutations,ngramms,jaccard_withchars,acc,precision,recall,f1,exec_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search each section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch_cora(data,true_matrix,max_numberOf_clusters,max_editDistance,similarityThreshold,windowSize,metric,similarityVectors,distanceMetricEmbedding,distanceMetric,number_of_permutations,ngramms,withchars,prototypeFilter,earlyStop):\n",
    "    results_dataframe = pd.DataFrame(columns=['max_numberOf_clusters','max_editDistance','similarityThreshold','windowSize','metric','similarityVectors',\"distanceMetricEmbedding\",\"distanceMetric\",\"number_of_permutations\",'prototypesFilterThr',\"protSelectionVariance\",'numOfPrototypes','numOfBuckets','averageBucketSize','Accuracy','Precision','Recall','F1','Time'])\n",
    "    i=1\n",
    "    for n1 in tqdm(max_numberOf_clusters):\n",
    "        for n2 in (max_editDistance):\n",
    "            for n3 in (similarityThreshold):\n",
    "                for n4 in (windowSize):\n",
    "                    for n5 in (metric):\n",
    "                        for n6 in (similarityVectors):\n",
    "                            for n7 in (distanceMetricEmbedding):\n",
    "                                for n8 in (distanceMetric):\n",
    "                                    for n9 in (number_of_permutations):\n",
    "                                        for n10 in (withchars):\n",
    "                                            for n11 in (withchars):\n",
    "                                                for n12 in (prototypeFilter):\n",
    "                                                    print(\"+ ------------  \",i,\"   ------------- +\")\n",
    "                                                    print('max_numberOf_clusters: ',n1)\n",
    "                                                    print('max_editDistancez: ',n2)\n",
    "                                                    print('similarityThreshold: ',n3)\n",
    "                                                    print('windowSize: ',n4)\n",
    "                                                    print('metric: ',n5)\n",
    "                                                    print('similarityVectors: ',n6)\n",
    "                                                    print('distanceMetricEmbedding: ',n7)\n",
    "                                                    print('distanceMetric: ',n8)\n",
    "                                                    print('number_of_permutations: ',n9)\n",
    "                                                    print('withchars: ',n10)\n",
    "                                                    print('ngramms: ',n11)\n",
    "                                                    print('prototypeFilter: ',n12)\n",
    "                                                    print(\"+ ----------------------------------- +\")\n",
    "                                                    start = time.time()\n",
    "                                                    model = RankedWTAHash(\n",
    "                                                      earlyStop = earlyStop,\n",
    "                                                      max_numberOf_clusters= n1,\n",
    "                                                      max_editDistance= n2,\n",
    "                                                      windowSize= n4,\n",
    "                                                      similarityThreshold= n3,\n",
    "                                                      maxOnly= False,\n",
    "                                                      metric=n5,\n",
    "                                                      similarityVectors=n6,\n",
    "                                                      number_of_permutations = n9,\n",
    "                                                      distanceMetric= n8,\n",
    "                                                      distanceMetricEmbedding = n7,\n",
    "                                                      jaccard_withchars = n10,\n",
    "                                                      ngramms= n11,                                                      \n",
    "                                                      prototypesFilterThr = n12\n",
    "                                                    )\n",
    "                                                    model = model.fit(data)\n",
    "                                                    exec_time = time.time() - start\n",
    "                                                    if model.earlyStop==0:                                            \n",
    "                                                        acc,f1,precision,recall = evaluate_cora(model.mapping_matrix,true_matrix)\n",
    "                                                        for key in model.buckets.keys():\n",
    "                                                            tempListmodel.buckets[key]\n",
    "                                                        averageBucketSize = np.mean([len(model.buckets[x]) for x in model.buckets.keys() ])\n",
    "                                                        numOfBuckets=len(model.buckets.keys())\n",
    "                                                    else:\n",
    "                                                        if model.earlyStop == 3:\n",
    "                                                            acc = f1 = precision = recall = 'Not counted'\n",
    "                                                            averageBucketSize = np.mean([len(model.buckets[x]) for x in model.buckets.keys() ])\n",
    "                                                            numOfBuckets=len(model.buckets.keys())\n",
    "                                                        else:\n",
    "                                                            numOfBuckets = averageBucketSize = acc = f1 = precision = recall = 'Not counted'\n",
    "                                                    i+=1\n",
    "                                                    results_dataframe.loc[len(results_dataframe)+1] = [n1,n2,n3,n4,n5,n6,n7,n8,n9,n12,model.selectionVariance,model.selected_numOfPrototypes,numOfBuckets,averageBucketSize,acc,precision,recall,f1,exec_time]\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H_%M_%S\")\n",
    "    results_dataframe.to_pickle(str(current_time)+\".pkl\")\n",
    "    \n",
    "    return results_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __[i]__ Fine tunning  __Prototype selection__\n",
    "\n",
    "\n",
    "Goals:\n",
    "\n",
    "- __Variance__: We want prototypes to be as much different as possible. \n",
    "- __Number__: We need a large amount of prototypes.\n",
    "\n",
    "Parameters to fine tunne:\n",
    "\n",
    "- ```max_numberOf_clusters```: Νumber of loops for finding representatives, it is an upper bound of clusters.\n",
    "- ```max_editDistance```:  The threshold for the triangle inequality\n",
    "- ```distanceMetric```:  Distance metric between the vectors when creating the space\n",
    "- ```prototypesFilterThr```: Prototypes must differ more that threshold\n",
    "\n",
    "All these parameters will be tested both for edit distance and jaccard with 3-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_numberOf_clusters= [500,1000,2000,5000]\n",
    "max_editDistance= [50,100,200,300]\n",
    "prototypesFilterThr = [10,20,40,70,100]\n",
    "\n",
    "distanceMetric= ['edit']\n",
    "\n",
    "# ---------------- #\n",
    "\n",
    "ngramms= [3]  \n",
    "jaccard_withchars = [False] \n",
    "\n",
    "distanceMetricEmbedding = ['euclidean']\n",
    "\n",
    "windowSize= [50]\n",
    "number_of_permutations = [5]\n",
    "\n",
    "similarityThreshold= [0.7]\n",
    "similarityVectors= ['ranked']\n",
    "metric= ['kendal']\n",
    "\n",
    "\n",
    "\n",
    "results_section1_edit = GridSearch_cora(\n",
    "    data,true_matrix,\n",
    "    max_numberOf_clusters,\n",
    "    max_editDistance,\n",
    "    similarityThreshold,\n",
    "    windowSize,\n",
    "    metric,\n",
    "    similarityVectors,\n",
    "    distanceMetricEmbedding,\n",
    "    distanceMetric,\n",
    "    number_of_permutations,\n",
    "    ngramms,\n",
    "    jaccard_withchars,\n",
    "    prototypesFilterThr,\n",
    "    earlyStop=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_section1_edit[[\"max_numberOf_clusters\",\"max_editDistance\",\"prototypesFilterThr\",\"numOfPrototypes\",\"protSelectionVariance\"]].sort_values(by=['numOfPrototypes'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_section1_edit[[\"max_numberOf_clusters\",\"max_editDistance\",\"prototypesFilterThr\",\"numOfPrototypes\",\"protSelectionVariance\"]].sort_values(by=['protSelectionVariance'], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_numberOf_clusters= [10,50,100,500,1000]\n",
    "max_editDistance= [0.3,0.6,0.7,0.8]\n",
    "prototypesFilterThr = [0.2,0.3,0.4,0.7,0.8]\n",
    "\n",
    "distanceMetric= ['jaccard']\n",
    "\n",
    "\n",
    "ngramms= [2,3]  \n",
    "jaccard_withchars = [True] \n",
    "\n",
    "# ---------------- #\n",
    "\n",
    "distanceMetricEmbedding = ['euclidean']\n",
    "\n",
    "windowSize= [50]\n",
    "number_of_permutations = [5]\n",
    "\n",
    "similarityThreshold= [0.7]\n",
    "similarityVectors= ['ranked']\n",
    "metric= ['kendal']\n",
    "\n",
    "\n",
    "results_section1_jac = GridSearch_cora(\n",
    "    data,true_matrix,\n",
    "    max_numberOf_clusters,\n",
    "    max_editDistance,\n",
    "    similarityThreshold,\n",
    "    windowSize,\n",
    "    metric,\n",
    "    similarityVectors,\n",
    "    distanceMetricEmbedding,\n",
    "    distanceMetric,\n",
    "    number_of_permutations,\n",
    "    ngramms,\n",
    "    jaccard_withchars,\n",
    "    prototypesFilterThr,\n",
    "    earlyStop=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_section1_jac[[\"max_numberOf_clusters\",\"max_editDistance\",\"prototypesFilterThr\",\"numOfPrototypes\",\"protSelectionVariance\"]].sort_values(by=['numOfPrototypes'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_section1_jac[[\"max_numberOf_clusters\",\"max_editDistance\",\"prototypesFilterThr\",\"numOfPrototypes\",\"protSelectionVariance\"]].sort_values(by=['protSelectionVariance'], ascending=True).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Remarks\n",
    "\n",
    "Two main factors:\n",
    "\n",
    "- __numOfPrototypes__ and\n",
    "- __protSelectionVariance__\n",
    "\n",
    "The first one is the number of prototypes selected, which is very important as more prototypes will enhance model when creating the embeddings.\n",
    "\n",
    "The second one is the average distance between all the prototypes selected. As this factor increases, the prototypes selected differ the most.\n",
    "\n",
    "\n",
    "According to the above, best parameters so far:\n",
    "\n",
    "- __Edit distance__\n",
    "\n",
    "\n",
    "- __Jaccard with 3-grams__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ii] Fine tunning  __Embedding phase__\n",
    "\n",
    "At the end, when the model will be fine tunned alltogether"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [iii] Fine tunning  __WTA algorithm__\n",
    "\n",
    "\n",
    "Goals:\n",
    "\n",
    "- __Ranked vectors dimension__\n",
    "- __Permutations__: A number of permutations will be forced in order to better split data into buckets.\n",
    "\n",
    " \n",
    "Parameters to fine tunne:\n",
    "\n",
    "- ```windowSize```: Vector size after WTA\n",
    "- ```number_of_permutations```: How many times vectors will be permuted and hashed\n",
    "\n",
    "All these parameters will be tested both for edit distance and jaccard with 3-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [iv] Model final fine tunning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_numberOf_clusters= [100,500,1000]\n",
    "max_editDistance= [50,100]\n",
    "prototypesFilterThr = [20,40,70]\n",
    "\n",
    "distanceMetric= ['edit']\n",
    "\n",
    "ngramms= [3]  \n",
    "jaccard_withchars = [False] \n",
    "\n",
    "distanceMetricEmbedding = ['l_inf','euclidean']\n",
    "\n",
    "windowSize= [16,32,64,128]\n",
    "number_of_permutations = [1,4,8]\n",
    "\n",
    "similarityThreshold= [0.7,0.75,0.8]\n",
    "similarityVectors= ['initial','ranked']\n",
    "metric= ['kendal','customKendal']\n",
    "\n",
    "\n",
    "\n",
    "results_section1_edit = GridSearch_cora(\n",
    "    data,true_matrix,\n",
    "    max_numberOf_clusters,\n",
    "    max_editDistance,\n",
    "    similarityThreshold,\n",
    "    windowSize,\n",
    "    metric,\n",
    "    similarityVectors,\n",
    "    distanceMetricEmbedding,\n",
    "    distanceMetric,\n",
    "    number_of_permutations,\n",
    "    ngramms,\n",
    "    jaccard_withchars,\n",
    "    prototypesFilterThr,\n",
    "    earlyStop=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_section1_edit[[\"max_numberOf_clusters\",\"max_editDistance\",\"prototypesFilterThr\",\"numOfPrototypes\",\"protSelectionVariance\"]].sort_values(by=['numOfPrototypes'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_section1_edit[[\"max_numberOf_clusters\",\"max_editDistance\",\"prototypesFilterThr\",\"numOfPrototypes\",\"protSelectionVariance\"]].sort_values(by=['protSelectionVariance'], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_numberOf_clusters= [100,500,1000]\n",
    "max_jacDistance= [0.3,0.6,0.7,0.8]\n",
    "prototypesFilterThr = [0.2,0.3,0.4,0.7,0.8]\n",
    "\n",
    "distanceMetric= ['jaccard']\n",
    "\n",
    "\n",
    "ngramms= [3]  \n",
    "jaccard_withchars = [True] \n",
    "\n",
    "distanceMetricEmbedding = ['euclidean','euclid_jaccard']\n",
    "\n",
    "windowSize= [50]\n",
    "number_of_permutations = [5]\n",
    "\n",
    "similarityThreshold= [0.7]\n",
    "similarityVectors= ['ranked']\n",
    "metric= ['kendal']\n",
    "\n",
    "\n",
    "results_section1_jac = GridSearch_cora(\n",
    "    data,true_matrix,\n",
    "    max_numberOf_clusters,\n",
    "    max_jacDistance,\n",
    "    similarityThreshold,\n",
    "    windowSize,\n",
    "    metric,\n",
    "    similarityVectors,\n",
    "    distanceMetricEmbedding,\n",
    "    distanceMetric,\n",
    "    number_of_permutations,\n",
    "    ngramms,\n",
    "    jaccard_withchars,\n",
    "    prototypesFilterThr,\n",
    "    earlyStop=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_section1_jac[[\"max_numberOf_clusters\",\"max_editDistance\",\"prototypesFilterThr\",\"numOfPrototypes\",\"protSelectionVariance\"]].sort_values(by=['numOfPrototypes'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_section1_jac[[\"max_numberOf_clusters\",\"max_editDistance\",\"prototypesFilterThr\",\"numOfPrototypes\",\"protSelectionVariance\"]].sort_values(by=['protSelectionVariance'], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaComponents = PCA_SpaceVisualization(model.Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaComponents = PCA_SpaceVisualization(model.rankedVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1.   [The dissimilarity representation for pattern recognition, a tutorial\n",
    "Robert P.W. Duin and Elzbieta Pekalska Delft University of Technology, The Netherlands School of Computer Science, University of Manchester, United Kingdom](http://homepage.tudelft.nl/a9p19/presentations/DisRep_Tutorial_doc.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
